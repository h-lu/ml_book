<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>分类与逻辑回归、KNN – 机器学习：从理论到Python实践</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./05-svm.html" rel="next">
<link href="./03-regression.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03-regression.html">第二部分：监督学习</a></li><li class="breadcrumb-item"><a href="./04-classification-logreg-knn.html"><span class="chapter-title">分类与逻辑回归、KNN</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">机器学习：从理论到Python实践</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">欢迎学习《机器学习：从理论到Python实践》</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">第一部分：机器学习基石与Python生态</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">机器学习导论</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-environment.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Python机器学习环境与核心库</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">第二部分：监督学习</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">回归与线性模型</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-classification-logreg-knn.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">分类与逻辑回归、KNN</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-svm.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">支持向量机 (SVM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-decision-trees-ensemble-learning.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">决策树与集成学习</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">第三部分：无监督学习</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-clustering.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">聚类分析</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-dimensionality-reduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">降维</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">第四部分：模型评估、优化与特征工程</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-model-evaluation-feature-engineering.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">模型评估、优化与特征工程</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">第五部分：深度学习初探</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-deep-learning-basics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">深度学习基础</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-cnn.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">卷积神经网络 (CNN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-rnn.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">循环神经网络 (RNN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-deep-learning-advanced.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">深度学习进阶</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">第六部分：强化学习入门</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-reinforcement-learning.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">强化学习基础与应用</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">第七部分：综合项目与展望</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-ml-project-workflow-summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">机器学习项目实战流程与总结</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-summary-outlook.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">课程总结与展望</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendices.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">附录</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#学习目标" id="toc-学习目标" class="nav-link active" data-scroll-target="#学习目标">学习目标</a></li>
  <li><a href="#什么是分类问题" id="toc-什么是分类问题" class="nav-link" data-scroll-target="#什么是分类问题">4.1 什么是分类问题？</a>
  <ul class="collapse">
  <li><a href="#分类与回归的关键区别" id="toc-分类与回归的关键区别" class="nav-link" data-scroll-target="#分类与回归的关键区别">4.1.1 分类与回归的关键区别</a></li>
  <li><a href="#常见的分类类型" id="toc-常见的分类类型" class="nav-link" data-scroll-target="#常见的分类类型">4.1.2 常见的分类类型</a></li>
  <li><a href="#分类模型的评估指标" id="toc-分类模型的评估指标" class="nav-link" data-scroll-target="#分类模型的评估指标">4.1.3 分类模型的评估指标</a></li>
  </ul></li>
  <li><a href="#逻辑回归-logistic-regression" id="toc-逻辑回归-logistic-regression" class="nav-link" data-scroll-target="#逻辑回归-logistic-regression">4.2 逻辑回归 (Logistic Regression)</a>
  <ul class="collapse">
  <li><a href="#为什么不用线性回归做分类" id="toc-为什么不用线性回归做分类" class="nav-link" data-scroll-target="#为什么不用线性回归做分类">4.2.1 为什么不用线性回归做分类？</a></li>
  <li><a href="#sigmoid-logistic-函数" id="toc-sigmoid-logistic-函数" class="nav-link" data-scroll-target="#sigmoid-logistic-函数">4.2.2 Sigmoid (Logistic) 函数</a></li>
  <li><a href="#模型表示与决策边界" id="toc-模型表示与决策边界" class="nav-link" data-scroll-target="#模型表示与决策边界">4.2.3 模型表示与决策边界</a></li>
  <li><a href="#代价函数-cost-function" id="toc-代价函数-cost-function" class="nav-link" data-scroll-target="#代价函数-cost-function">4.2.4 代价函数 (Cost Function)</a></li>
  <li><a href="#优化梯度下降" id="toc-优化梯度下降" class="nav-link" data-scroll-target="#优化梯度下降">4.2.5 优化：梯度下降</a></li>
  <li><a href="#多分类逻辑回归-softmax-regression" id="toc-多分类逻辑回归-softmax-regression" class="nav-link" data-scroll-target="#多分类逻辑回归-softmax-regression">4.2.6 多分类逻辑回归 (Softmax Regression)</a></li>
  <li><a href="#逻辑回归的优缺点" id="toc-逻辑回归的优缺点" class="nav-link" data-scroll-target="#逻辑回归的优缺点">4.2.7 逻辑回归的优缺点</a></li>
  </ul></li>
  <li><a href="#k-近邻算法-k-nearest-neighbors-knn" id="toc-k-近邻算法-k-nearest-neighbors-knn" class="nav-link" data-scroll-target="#k-近邻算法-k-nearest-neighbors-knn">4.3 K-近邻算法 (K-Nearest Neighbors, KNN)</a>
  <ul class="collapse">
  <li><a href="#knn算法步骤" id="toc-knn算法步骤" class="nav-link" data-scroll-target="#knn算法步骤">4.3.1 KNN算法步骤</a></li>
  <li><a href="#距离度量-distance-metrics" id="toc-距离度量-distance-metrics" class="nav-link" data-scroll-target="#距离度量-distance-metrics">4.3.2 距离度量 (Distance Metrics)</a></li>
  <li><a href="#k值的选择" id="toc-k值的选择" class="nav-link" data-scroll-target="#k值的选择">4.3.3 K值的选择</a></li>
  <li><a href="#特征缩放-feature-scaling" id="toc-特征缩放-feature-scaling" class="nav-link" data-scroll-target="#特征缩放-feature-scaling">4.3.4 特征缩放 (Feature Scaling)</a></li>
  <li><a href="#knn的优缺点" id="toc-knn的优缺点" class="nav-link" data-scroll-target="#knn的优缺点">4.3.5 KNN的优缺点</a></li>
  </ul></li>
  <li><a href="#使用-scikit-learn-实现分类模型" id="toc-使用-scikit-learn-实现分类模型" class="nav-link" data-scroll-target="#使用-scikit-learn-实现分类模型">4.4 使用 Scikit-learn 实现分类模型</a>
  <ul class="collapse">
  <li><a href="#逻辑回归-scikit-learn" id="toc-逻辑回归-scikit-learn" class="nav-link" data-scroll-target="#逻辑回归-scikit-learn">4.4.1 逻辑回归 (Scikit-learn)</a></li>
  <li><a href="#k-近邻-knn-scikit-learn" id="toc-k-近邻-knn-scikit-learn" class="nav-link" data-scroll-target="#k-近邻-knn-scikit-learn">4.4.2 K-近邻 (KNN) (Scikit-learn)</a></li>
  <li><a href="#比较逻辑回归和knn" id="toc-比较逻辑回归和knn" class="nav-link" data-scroll-target="#比较逻辑回归和knn">4.4.3 比较逻辑回归和KNN</a></li>
  </ul></li>
  <li><a href="#本章小结" id="toc-本章小结" class="nav-link" data-scroll-target="#本章小结">4.5 本章小结</a></li>
  <li><a href="#思考与练习" id="toc-思考与练习" class="nav-link" data-scroll-target="#思考与练习">4.6 思考与练习</a>
  <ul class="collapse">
  <li><a href="#基础练习" id="toc-基础练习" class="nav-link" data-scroll-target="#基础练习">4.6.1 基础练习</a></li>
  <li><a href="#编码与实践-可使用iris数据集" id="toc-编码与实践-可使用iris数据集" class="nav-link" data-scroll-target="#编码与实践-可使用iris数据集">4.6.2 编码与实践 (可使用Iris数据集)</a></li>
  <li><a href="#推荐阅读" id="toc-推荐阅读" class="nav-link" data-scroll-target="#推荐阅读">4.6.3 推荐阅读</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03-regression.html">第二部分：监督学习</a></li><li class="breadcrumb-item"><a href="./04-classification-logreg-knn.html"><span class="chapter-title">分类与逻辑回归、KNN</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">分类与逻辑回归、KNN</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="学习目标" class="level2">
<h2 class="anchored" data-anchor-id="学习目标">学习目标</h2>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>学习目标：</strong></p>
<ul>
<li>理解分类问题的定义及其与回归问题的区别。</li>
<li>掌握逻辑回归的原理，包括Sigmoid函数、决策边界和代价函数。</li>
<li>理解K近邻（KNN）算法的核心思想、距离度量方法和K值选择的重要性。</li>
<li>了解特征缩放对于距离敏感算法（如KNN）的必要性。</li>
<li>能够使用Scikit-learn库实现、训练和评估逻辑回归和KNN分类模型。</li>
<li>能够对逻辑回归和KNN模型进行基本的参数调整和结果比较。</li>
</ul>
</div>
</div>
</div>
</section>
<section id="什么是分类问题" class="level2">
<h2 class="anchored" data-anchor-id="什么是分类问题">4.1 什么是分类问题？</h2>
<p>在机器学习中，<strong>分类 (Classification)</strong> 是一种监督学习任务，其目标是预测一个<strong>离散的类别标签 (discrete class label)</strong>。换句话说，我们希望将输入数据点分配到一个预定义的类别中。</p>
<p>例如：</p>
<ul>
<li>判断一封邮件是否为垃圾邮件（类别：“垃圾邮件” 或 “非垃圾邮件”）。</li>
<li>识别一张图片中的动物是猫还是狗（类别：“猫” 或 “狗”）。</li>
<li>根据客户的特征判断其信用等级（类别：“高信用”, “中等信用”, “低信用”）。</li>
<li>识别手写数字（类别：“0”, “1”, “2”, …, “9”）。</li>
</ul>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>分类的核心</strong>是找到<strong>输入特征</strong>（自变量）与<strong>类别标签</strong>（因变量）之间的映射关系。我们希望建立一个模型，当给定新的输入特征时，该模型能够给出正确的类别预测。</p>
</div>
</div>
</div>
<section id="分类与回归的关键区别" class="level3">
<h3 class="anchored" data-anchor-id="分类与回归的关键区别">4.1.1 分类与回归的关键区别</h3>
<p>我们在上一章学习了回归问题，其目标是预测连续值。分类与回归的主要区别总结如下：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 37%">
<col style="width: 51%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">特征</th>
<th style="text-align: left;">回归 (Regression)</th>
<th style="text-align: left;">分类 (Classification)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>输出类型</strong></td>
<td style="text-align: left;">连续数值 (e.g., 1.23, 100, -5.7)</td>
<td style="text-align: left;">离散类别 (e.g., “A”, “B”, “C”, True/False, 0, 1)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>目标</strong></td>
<td style="text-align: left;">预测一个具体的量</td>
<td style="text-align: left;">将输入划分到预定义的类别中</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>示例</strong></td>
<td style="text-align: left;">房价预测, 气温预测, 股票价格预测</td>
<td style="text-align: left;">邮件分类, 图像识别, 疾病诊断</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>评估指标</strong></td>
<td style="text-align: left;">均方误差 (MSE), R², 平均绝对误差 (MAE)</td>
<td style="text-align: left;">准确率, 精确率, 召回率, F1分数, ROC曲线, AUC</td>
</tr>
</tbody>
</table>
</section>
<section id="常见的分类类型" class="level3">
<h3 class="anchored" data-anchor-id="常见的分类类型">4.1.2 常见的分类类型</h3>
<ul>
<li><strong>二分类 (Binary Classification):</strong> 只有两个可能的输出类别。例如：是/否，垃圾/非垃圾，通过/未通过。</li>
<li><strong>多分类 (Multiclass Classification):</strong> 有三个或更多可能的输出类别，且每个样本只能属于一个类别。例如：手写数字识别（0-9），新闻主题分类（体育、财经、娱乐）。</li>
<li><strong>多标签分类 (Multilabel Classification):</strong> 每个样本可以同时属于多个类别。例如：一部电影可以同时被标记为”动作片”、“科幻片”和”冒险片”。</li>
</ul>
<p>本章将主要关注二分类和多分类问题。</p>
</section>
<section id="分类模型的评估指标" class="level3">
<h3 class="anchored" data-anchor-id="分类模型的评估指标">4.1.3 分类模型的评估指标</h3>
<p>一旦我们训练好一个分类模型，就需要评估其性能。与回归模型使用MSE、R²等指标不同，分类模型有其特定的一套评估指标。这些指标通常基于<strong>混淆矩阵 (Confusion Matrix)</strong> 计算得出。</p>
<section id="混淆矩阵-confusion-matrix" class="level4">
<h4 class="anchored" data-anchor-id="混淆矩阵-confusion-matrix">4.1.3.1 混淆矩阵 (Confusion Matrix)</h4>
<p>对于一个二分类问题（假设类别为正类1和负类0），混淆矩阵是一个2x2的表格，总结了模型预测结果与真实标签之间的关系：</p>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 36%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">预测为正类 (Predicted: 1)</th>
<th style="text-align: left;">预测为负类 (Predicted: 0)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>实际为正类 (Actual: 1)</strong></td>
<td style="text-align: left;">真正例 (True Positive, TP)</td>
<td style="text-align: left;">假反例 (False Negative, FN)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>实际为负类 (Actual: 0)</strong></td>
<td style="text-align: left;">假正例 (False Positive, FP)</td>
<td style="text-align: left;">真反例 (True Negative, TN)</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>真正例 (TP):</strong> 实际为正类，模型也预测为正类。</li>
<li><strong>真反例 (TN):</strong> 实际为负类，模型也预测为负类。</li>
<li><strong>假正例 (FP):</strong> 实际为负类，但模型错误地预测为正类 (也称为 Type I error)。</li>
<li><strong>假反例 (FN):</strong> 实际为正类，但模型错误地预测为负类 (也称为 Type II error)。</li>
</ul>
<p>对于多分类问题，混淆矩阵会扩展为 NxN 的表格，其中N是类别的数量。</p>
</section>
<section id="准确率-accuracy" class="level4">
<h4 class="anchored" data-anchor-id="准确率-accuracy">4.1.3.2 准确率 (Accuracy)</h4>
<p>准确率是最直观的评估指标，表示模型正确预测的样本占总样本的比例。</p>
<p><span class="math display">\[ \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{\text{正确预测的样本数}}{\text{总样本数}} \]</span></p>
<p><strong>优点：</strong> 简单易懂。</p>
<p><strong>缺点：</strong> 在类别不平衡的数据集上具有误导性。例如，如果95%的样本属于负类，一个总是预测负类的模型也能达到95%的准确率，但这并不是一个好模型。</p>
</section>
<section id="精准率-precision" class="level4">
<h4 class="anchored" data-anchor-id="精准率-precision">4.1.3.3 精准率 (Precision)</h4>
<p>精准率（也称查准率）衡量的是在所有被模型预测为正类的样本中，有多少是真正的正类。</p>
<p><span class="math display">\[ \text{Precision} = \frac{TP}{TP + FP} = \frac{\text{真正例}}{\text{所有预测为正例的样本数}} \]</span></p>
<p><strong>关注点：</strong> 预测为正的样本中，有多少是“名副其实”的。常用于希望避免“误报”的场景，例如垃圾邮件检测（不希望将正常邮件误判为垃圾邮件）。</p>
</section>
<section id="召回率-recall" class="level4">
<h4 class="anchored" data-anchor-id="召回率-recall">4.1.3.4 召回率 (Recall)</h4>
<p>召回率（也称查全率、敏感度 (Sensitivity)）衡量的是在所有实际为正类的样本中，有多少被模型成功预测出来。</p>
<p><span class="math display">\[ \text{Recall} = \frac{TP}{TP + FN} = \frac{\text{真正例}}{\text{所有实际为正例的样本数}} \]</span></p>
<p><strong>关注点：</strong> 真正为正的样本中，有多少被“找回来”了。常用于希望避免“漏报”的场景，例如疾病诊断（不希望漏掉任何一个真正的病人）。</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>精准率与召回率的权衡 (Precision-Recall Trade-off):</strong> 通常情况下，精准率和召回率是相互制约的。提高一个往往会导致另一个下降。例如，如果我们希望召回所有病人（提高召回率），可能会将一些健康人也误诊为病人（降低精准率）。选择合适的阈值来平衡两者非常重要。</p>
</div>
</div>
</div>
</section>
<section id="f1-分数-f1-score" class="level4">
<h4 class="anchored" data-anchor-id="f1-分数-f1-score">4.1.3.5 F1 分数 (F1-Score)</h4>
<p>F1分数是精准率和召回率的调和平均值，它综合考虑了这两个指标。</p>
<p><span class="math display">\[ F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \]</span></p>
<p><strong>特点：</strong> 当精准率和召回率都较高时，F1分数也会较高。它比简单平均更偏向于较小的值，因此只有当两者都较好时，F1才会高。</p>
</section>
<section id="roc-曲线与-auc" class="level4">
<h4 class="anchored" data-anchor-id="roc-曲线与-auc">4.1.3.6 ROC 曲线与 AUC</h4>
<p><strong>ROC (Receiver Operating Characteristic) 曲线</strong>是另一种评估二分类模型性能的强大工具，尤其适用于类别不平衡的情况。ROC曲线通过绘制不同分类阈值下的<strong>真正例率 (True Positive Rate, TPR)</strong> 与 <strong>假正例率 (False Positive Rate, FPR)</strong> 的关系来展示模型的性能。</p>
<ul>
<li><strong>真正例率 (TPR):</strong> 就是召回率。 <span class="math display">\[ TPR = \text{Recall} = \frac{TP}{TP + FN} \]</span></li>
<li><strong>假正例率 (FPR):</strong> 实际为负类的样本中，被错误预测为正类的比例。 <span class="math display">\[ FPR = \frac{FP}{FP + TN} \]</span></li>
</ul>
<p>ROC曲线的横轴是FPR，纵轴是TPR。</p>
<ul>
<li>一个完美的分类器，其ROC曲线会经过左上角 (FPR=0, TPR=1)。</li>
<li>随机猜测的分类器，其ROC曲线是一条从 (0,0) 到 (1,1) 的对角线。</li>
<li>曲线越靠近左上角，模型的性能越好。</li>
</ul>
<p><strong>AUC (Area Under the ROC Curve):</strong> ROC曲线下的面积。AUC值介于0到1之间。</p>
<ul>
<li>AUC = 1：完美分类器。</li>
<li>AUC = 0.5：随机猜测。</li>
<li>AUC &lt; 0.5：比随机猜测还差（通常意味着标签可能反了）。</li>
</ul>
<p>AUC提供了一个单一的数值来总结模型在所有可能阈值下的性能。</p>
<p><img src="./images/04-classification/roc_curve_example.svg" class="img-fluid"> <em>(图 4.1: ROC曲线示例。曲线越靠近左上角，模型性能越好。对角线代表随机猜测。AUC即曲线下面积。)</em></p>
<p>选择哪种评估指标取决于具体的应用场景和我们更关注哪方面的性能（例如，是最小化假正例还是最小化假反例）。通常我们会综合考虑多个指标。</p>
</section>
</section>
</section>
<section id="逻辑回归-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="逻辑回归-logistic-regression">4.2 逻辑回归 (Logistic Regression)</h2>
<p>虽然名字中带有”回归”，但逻辑回归实际上是一种广泛用于解决<strong>二分类问题</strong>的线性模型。它通过一个特殊的函数（Sigmoid或Logistic函数）将线性回归的输出映射到 (0, 1) 区间，从而得到样本属于某个类别的概率。</p>
<section id="为什么不用线性回归做分类" class="level3">
<h3 class="anchored" data-anchor-id="为什么不用线性回归做分类">4.2.1 为什么不用线性回归做分类？</h3>
<p>直接使用线性回归进行分类存在一些问题：</p>
<ol type="1">
<li><strong>输出范围不匹配：</strong> 线性回归的输出是连续的，可以超出 [0, 1] 的范围，而概率值必须在 [0, 1] 之内。</li>
<li><strong>对异常值敏感：</strong> 如下图所示，如果使用线性回归拟合0/1的分类标签，少数几个离群点就可能显著改变决策边界。</li>
</ol>
<p><img src="./images/04-classification/linear_reg_for_classification_issue.svg" class="img-fluid"> <em>(图 4.2: 线性回归用于分类的问题。左图：一个看似合理的阈值可以将两类分开。右图：增加一个离群点后，线性回归的拟合线发生显著改变，导致原阈值不再适用。)</em></p>
<p>为了解决这些问题，逻辑回归引入了Sigmoid函数。</p>
</section>
<section id="sigmoid-logistic-函数" class="level3">
<h3 class="anchored" data-anchor-id="sigmoid-logistic-函数">4.2.2 Sigmoid (Logistic) 函数</h3>
<p>Sigmoid函数，也称为Logistic函数，其数学表达式为： <span class="math display">\[ \sigma(z) = \frac{1}{1 + e^{-z}} \]</span> 其中 <span class="math inline">\(z\)</span> 是线性组合，即 <span class="math inline">\(z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = \mathbf{\beta}^T \mathbf{x}\)</span> (这里 <span class="math inline">\(\mathbf{x}\)</span> 包含了 <span class="math inline">\(x_0=1\)</span> 的截距项)。</p>
<p>Sigmoid函数的特点：</p>
<ul>
<li>输出值域为 (0, 1)，正好可以解释为概率。</li>
<li>当 <span class="math inline">\(z \to \infty\)</span> 时，<span class="math inline">\(\sigma(z) \to 1\)</span>。</li>
<li>当 <span class="math inline">\(z \to -\infty\)</span> 时，<span class="math inline">\(\sigma(z) \to 0\)</span>。</li>
<li>当 <span class="math inline">\(z = 0\)</span> 时，<span class="math inline">\(\sigma(z) = 0.5\)</span>。</li>
<li>它是一个单调递增的S型曲线。</li>
</ul>
<p><img src="./images/04-classification/sigmoid_function.svg" class="img-fluid"> <em>(图 4.3: Sigmoid (Logistic) 函数图像。它将任意实数输入映射到 (0, 1) 区间。)</em></p>
</section>
<section id="模型表示与决策边界" class="level3">
<h3 class="anchored" data-anchor-id="模型表示与决策边界">4.2.3 模型表示与决策边界</h3>
<p>逻辑回归模型预测样本属于正类（通常记为类别1）的概率为： <span class="math display">\[ P(y=1 | \mathbf{x}; \mathbf{\beta}) = h_{\mathbf{\beta}}(\mathbf{x}) = \sigma(\mathbf{\beta}^T \mathbf{x}) = \frac{1}{1 + e^{-\mathbf{\beta}^T \mathbf{x}}} \]</span> 相应地，样本属于负类（通常记为类别0）的概率为： <span class="math display">\[ P(y=0 | \mathbf{x}; \mathbf{\beta}) = 1 - h_{\mathbf{\beta}}(\mathbf{x}) \]</span></p>
<p><strong>决策边界 (Decision Boundary):</strong> 为了做出明确的分类预测，我们需要设定一个阈值，通常为0.5。</p>
<ul>
<li>如果 <span class="math inline">\(h_{\mathbf{\beta}}(\mathbf{x}) \ge 0.5\)</span>，则预测 <span class="math inline">\(y=1\)</span>。</li>
<li>如果 <span class="math inline">\(h_{\mathbf{\beta}}(\mathbf{x}) &lt; 0.5\)</span>，则预测 <span class="math inline">\(y=0\)</span>。</li>
</ul>
<p>由于Sigmoid函数在 <span class="math inline">\(z=0\)</span> 时取值为0.5，所以决策边界对应于 <span class="math inline">\(\mathbf{\beta}^T \mathbf{x} = 0\)</span>。 对于线性组合 <span class="math inline">\(\mathbf{\beta}^T \mathbf{x} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...\)</span>，决策边界是一个<strong>线性</strong>边界。 例如，在二维空间 (两个特征 <span class="math inline">\(x_1, x_2\)</span>)，决策边界是一条直线：<span class="math inline">\(\beta_0 + \beta_1 x_1 + \beta_2 x_2 = 0\)</span>。 在三维空间，它是一个平面；更高维则是一个超平面。</p>
<p><img src="./images/04-classification/logistic_decision_boundary.svg" class="img-fluid"> <em>(图 4.4: 逻辑回归的线性决策边界示例 (二维特征空间)。橙色三角和蓝色圆点代表两个类别，黑线是学习到的决策边界。)</em></p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>非线性决策边界：</strong> 虽然逻辑回归本身是线性分类器，但可以通过引入多项式特征（类似于多项式回归）或使用核技巧（不在此处详述）来创建非线性的决策边界。</p>
</div>
</div>
</div>
</section>
<section id="代价函数-cost-function" class="level3">
<h3 class="anchored" data-anchor-id="代价函数-cost-function">4.2.4 代价函数 (Cost Function)</h3>
<p>对于线性回归，我们使用均方误差 (MSE) 作为代价函数。但如果将MSE直接用于逻辑回归（即 <span class="math inline">\(J(\mathbf{\beta}) = \frac{1}{m} \sum (h_{\mathbf{\beta}}(\mathbf{x}^{(i)}) - y^{(i)})^2\)</span>），由于 <span class="math inline">\(h_{\mathbf{\beta}}(\mathbf{x})\)</span> 是非线性的Sigmoid函数，代价函数会变成一个<strong>非凸函数 (non-convex function)</strong>，有很多局部最优解，梯度下降法难以找到全局最优。</p>
<p>因此，逻辑回归使用一个不同的代价函数，称为<strong>对数损失 (Log Loss)</strong> 或<strong>二元交叉熵 (Binary Cross-Entropy)</strong>。 对于单个训练样本 <span class="math inline">\((\mathbf{x}, y)\)</span>，其代价为： <span class="math display">\[ Cost(h_{\mathbf{\beta}}(\mathbf{x}), y) = -y \log(h_{\mathbf{\beta}}(\mathbf{x})) - (1-y) \log(1 - h_{\mathbf{\beta}}(\mathbf{x})) \]</span></p>
<p>这个代价函数的特点是：</p>
<ul>
<li>如果 <span class="math inline">\(y=1\)</span> 且 <span class="math inline">\(h_{\mathbf{\beta}}(\mathbf{x}) \to 1\)</span> (预测正确)，则 <span class="math inline">\(Cost \to 0\)</span>。</li>
<li>如果 <span class="math inline">\(y=1\)</span> 且 <span class="math inline">\(h_{\mathbf{\beta}}(\mathbf{x}) \to 0\)</span> (预测错误)，则 <span class="math inline">\(Cost \to \infty\)</span> (给予大的惩罚)。</li>
<li>如果 <span class="math inline">\(y=0\)</span> 且 <span class="math inline">\(h_{\mathbf{\beta}}(\mathbf{x}) \to 0\)</span> (预测正确)，则 <span class="math inline">\(Cost \to 0\)</span>。</li>
<li>如果 <span class="math inline">\(y=0\)</span> 且 <span class="math inline">\(h_{\mathbf{\beta}}(\mathbf{x}) \to 1\)</span> (预测错误)，则 <span class="math inline">\(Cost \to \infty\)</span> (给予大的惩罚)。</li>
</ul>
<p>对于整个训练集（<span class="math inline">\(m\)</span> 个样本），总的代价函数为： <span class="math display">\[ J(\mathbf{\beta}) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(h_{\mathbf{\beta}}(\mathbf{x}^{(i)})) + (1-y^{(i)}) \log(1 - h_{\mathbf{\beta}}(\mathbf{x}^{(i)}))] \]</span> 这个代价函数是凸函数，可以使用梯度下降等优化算法找到全局最优解。</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>为什么MSE直接作用是非凸的，而对数损失是凸的？</strong></p>
<ol type="1">
<li><strong>MSE的非凸性</strong>：
<ul>
<li>逻辑回归的预测函数是Sigmoid函数 <span class="math inline">\(h_{\beta}(x) = \frac{1}{1+e^{-\beta^T x}}\)</span>，这是一个非线性变换</li>
<li>当MSE作用在这个非线性输出上时，代价函数 <span class="math inline">\(J(\beta) = \frac{1}{m}\sum (h_{\beta}(x^{(i)})-y^{(i)})^2\)</span> 会形成多个局部极小值</li>
<li>这种非凸性使得梯度下降容易陷入局部最优而无法找到全局最优解</li>
</ul></li>
<li><strong>对数损失的凸性</strong>：
<ul>
<li>对数损失函数 <span class="math inline">\(J(\beta) = -\frac{1}{m}\sum [y^{(i)}\log(h_{\beta}(x^{(i)})) + (1-y^{(i)})\log(1-h_{\beta}(x^{(i)}))]\)</span> 经过精心设计</li>
<li>可以证明这个函数关于参数 <span class="math inline">\(\beta\)</span> 是凸的（二阶导数始终非负）</li>
<li>凸函数保证梯度下降一定能收敛到全局最小值</li>
<li>这个设计还符合最大似然估计的原理，使预测概率与真实标签的差异最小化</li>
</ul></li>
</ol>
<p><strong>数学证明参考</strong>：</p>
<ul>
<li><a href="https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf">Convexity of Logistic Regression Cost Function (Stanford CS229 Notes)</a> (Section 5)</li>
<li><a href="https://stats.stackexchange.com/questions/326350/why-is-mse-convex-but-logistic-regression-with-mse-is-not-convex">Why is MSE non-convex for logistic regression? (Cross Validated)</a></li>
<li><a href="https://web.stanford.edu/~boyd/cvxbook/">Convex Optimization in Machine Learning (Boyd &amp; Vandenberghe)</a> (Chapter 4)</li>
</ul>
</div>
</div>
</div>
</section>
<section id="优化梯度下降" class="level3">
<h3 class="anchored" data-anchor-id="优化梯度下降">4.2.5 优化：梯度下降</h3>
<p>逻辑回归的参数 <span class="math inline">\(\mathbf{\beta}\)</span> 可以通过梯度下降法来优化。梯度下降的更新规则与线性回归类似： <span class="math display">\[ \beta_j := \beta_j - \alpha \frac{\partial J(\mathbf{\beta})}{\partial \beta_j} \]</span> 对于逻辑回归的代价函数，其偏导数为： <span class="math display">\[ \frac{\partial J(\mathbf{\beta})}{\partial \beta_j} = \frac{1}{m} \sum_{i=1}^{m} (h_{\mathbf{\beta}}(\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \]</span> 这个偏导数的形式与线性回归的梯度在形式上是相同的（尽管 <span class="math inline">\(h_{\mathbf{\beta}}(\mathbf{x})\)</span> 的定义不同）。</p>
<p>与线性回归一样，逻辑回归也可以添加L1或L2正则化项来防止过拟合。Scikit-learn中的 <code>LogisticRegression</code> 类默认就包含了L2正则化。</p>
</section>
<section id="多分类逻辑回归-softmax-regression" class="level3">
<h3 class="anchored" data-anchor-id="多分类逻辑回归-softmax-regression">4.2.6 多分类逻辑回归 (Softmax Regression)</h3>
<p>对于多分类问题（K个类别，K &gt; 2），逻辑回归可以推广为<strong>Softmax Regression</strong> (或称 Multinomial Logistic Regression)。 Softmax函数将一个K维向量 <span class="math inline">\(z\)</span> 转换为一个K维的概率分布向量 <span class="math inline">\(p\)</span>，其中每个元素 <span class="math inline">\(p_k\)</span> 表示样本属于类别 <span class="math inline">\(k\)</span> 的概率，且所有概率之和为1。 <span class="math display">\[ p_k = \text{softmax}(z)_k = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}} \]</span> 其中 <span class="math inline">\(z_k = \mathbf{\beta}_k^T \mathbf{x}\)</span> 是类别 <span class="math inline">\(k\)</span> 的线性得分。每个类别都有一组自己的参数 <span class="math inline">\(\mathbf{\beta}_k\)</span>。 代价函数也相应地推广为交叉熵损失。</p>
<p>另一种处理多分类的方法是<strong>One-vs-Rest (OvR)</strong> 或 <strong>One-vs-All (OvA)</strong>。对于K个类别，我们训练K个独立的二分类逻辑回归模型。第 <span class="math inline">\(k\)</span> 个模型将类别 <span class="math inline">\(k\)</span> 视为正类，所有其他类别视为负类。预测时，选择概率最高的那个模型对应的类别。Scikit-learn中的 <code>LogisticRegression</code> 支持OvR和Softmax (multinomial) 策略。</p>
</section>
<section id="逻辑回归的优缺点" class="level3">
<h3 class="anchored" data-anchor-id="逻辑回归的优缺点">4.2.7 逻辑回归的优缺点</h3>
<p><strong>优点：</strong></p>
<ul>
<li>实现简单，计算代价不高，训练速度快。</li>
<li>输出结果易于理解，可以解释为概率。</li>
<li>对数据中小噪声的鲁棒性好。</li>
<li>由于是线性模型，可解释性较强。</li>
<li>易于通过正则化来防止过拟合。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>容易欠拟合，因为它假设数据是线性可分的（或通过特征工程变为线性可分）。</li>
<li>对于非线性关系复杂的数据集，表现可能不佳。</li>
<li>对特征空间较大时，性能可能下降。</li>
</ul>
</section>
</section>
<section id="k-近邻算法-k-nearest-neighbors-knn" class="level2">
<h2 class="anchored" data-anchor-id="k-近邻算法-k-nearest-neighbors-knn">4.3 K-近邻算法 (K-Nearest Neighbors, KNN)</h2>
<p>K-近邻算法是一种简单且直观的<strong>非参数 (non-parametric)</strong> 分类（和回归）算法。它不做任何关于数据分布的假设。</p>
<p><strong>核心思想：</strong> “物以类聚，人以群分”。一个样本的类别由其在特征空间中最邻近的 K 个样本的类别来决定。</p>
<section id="knn算法步骤" class="level3">
<h3 class="anchored" data-anchor-id="knn算法步骤">4.3.1 KNN算法步骤</h3>
<ol type="1">
<li><strong>选择参数 K：</strong> 确定邻居的数量 K，K通常是一个小的正整数。</li>
<li><strong>计算距离：</strong> 对于一个新的未知样本，计算它与训练集中所有样本之间的距离。</li>
<li><strong>找到K个最近邻：</strong> 根据计算出的距离，找出训练集中距离新样本最近的 K 个样本。</li>
<li><strong>进行预测：</strong>
<ul>
<li><strong>分类任务：</strong> K 个最近邻中出现次数最多的类别（多数表决）即为新样本的预测类别。可以对不同距离的邻居赋予不同的权重（例如，距离越近权重越大）。</li>
<li><strong>回归任务：</strong> K 个最近邻的目标值的平均值（或加权平均值）即为新样本的预测值。</li>
</ul></li>
</ol>
<p><img src="./images/04-classification/knn_illustration.svg" class="img-fluid"> <em>(图 4.5: KNN算法原理图示。绿色圆圈为未知样本。如果K=3，其最近的3个邻居（虚线连接）决定其类别。图中K=3时，最近的邻居中有2个红色三角，1个蓝色方块，因此新样本会被预测为红色三角。如果K=5，则会考虑5个最近邻居，预测结果可能不同。)</em></p>
</section>
<section id="距离度量-distance-metrics" class="level3">
<h3 class="anchored" data-anchor-id="距离度量-distance-metrics">4.3.2 距离度量 (Distance Metrics)</h3>
<p>选择合适的距离度量方法对KNN的性能至关重要。常用的距离度量包括：</p>
<ol type="1">
<li><strong>欧几里得距离 (Euclidean Distance):</strong> 最常用的距离度量。对于两个 <span class="math inline">\(n\)</span> 维向量 <span class="math inline">\(\mathbf{a}=(a_1, ..., a_n)\)</span> 和 <span class="math inline">\(\mathbf{b}=(b_1, ..., b_n)\)</span>，其欧氏距离为： <span class="math display">\[ d(\mathbf{a}, \mathbf{b}) = \sqrt{\sum_{i=1}^{n} (a_i - b_i)^2} \]</span></li>
<li><strong>曼哈顿距离 (Manhattan Distance):</strong> 也称为城市街区距离。 <span class="math display">\[ d(\mathbf{a}, \mathbf{b}) = \sum_{i=1}^{n} |a_i - b_i| \]</span></li>
<li><strong>闵可夫斯基距离 (Minkowski Distance):</strong> 是欧氏距离和曼哈顿距离的推广。参数 <span class="math inline">\(p \ge 1\)</span>。 <span class="math display">\[ d(\mathbf{a}, \mathbf{b}) = (\sum_{i=1}^{n} |a_i - b_i|^p)^{1/p} \]</span>
<ul>
<li>当 <span class="math inline">\(p=1\)</span> 时，为曼哈顿距离。</li>
<li>当 <span class="math inline">\(p=2\)</span> 时，为欧几里得距离。</li>
</ul></li>
<li><strong>切比雪夫距离 (Chebyshev Distance):</strong> <span class="math display">\[ d(\mathbf{a}, \mathbf{b}) = \max_i (|a_i - b_i|) \]</span></li>
<li><strong>余弦相似度 (Cosine Similarity) / 余弦距离 (Cosine Distance):</strong> 常用于文本数据。衡量两个向量方向的相似性。余弦距离 = 1 - 余弦相似度。 <span class="math display">\[ \text{similarity}(\mathbf{a}, \mathbf{b}) = \frac{\mathbf{a} \cdot \mathbf{b}}{||\mathbf{a}|| \cdot ||\mathbf{b}||} \]</span></li>
<li><strong>汉明距离 (Hamming Distance):</strong> 常用于二元或类别特征。表示两个等长字符串之间不同位置的字符个数。</li>
</ol>
<p>Scikit-learn中的 <code>KNeighborsClassifier</code> 默认使用闵可夫斯基距离且 <span class="math inline">\(p=2\)</span>（即欧氏距离）。</p>
</section>
<section id="k值的选择" class="level3">
<h3 class="anchored" data-anchor-id="k值的选择">4.3.3 K值的选择</h3>
<p>K值的选择对KNN模型的性能有很大影响：</p>
<ul>
<li><strong>较小的K值：</strong>
<ul>
<li>模型对噪声数据更敏感，容易受到异常点的影响。</li>
<li>决策边界会变得更复杂，波动更大。</li>
<li>可能导致过拟合 (low bias, high variance)。</li>
</ul></li>
<li><strong>较大的K值：</strong>
<ul>
<li>模型对噪声数据更不敏感，具有平滑效果。</li>
<li>决策边界会变得更平滑。</li>
<li>可能导致欠拟合 (high bias, low variance)，因为会包含较远的不太相似的点。</li>
</ul></li>
</ul>
<p><strong>如何选择K值？</strong></p>
<ul>
<li>通常K取奇数，以避免投票时出现平局（主要针对二分类）。</li>
<li><strong>交叉验证 (Cross-Validation):</strong> 最常用的方法。尝试不同的K值，选择在验证集上表现最好的那个K。</li>
<li><strong>经验法则：</strong> <span class="math inline">\(K \approx \sqrt{N}\)</span>，其中N是训练样本数量（这只是一个非常粗略的启发式方法）。</li>
<li><strong>可视化错误率 vs K值曲线：</strong> 绘制不同K值下的模型错误率（或准确率），寻找”肘部”或者错误率最低的点。</li>
</ul>
<p><img src="./images/04-classification/knn_k_impact_on_boundary.svg" class="img-fluid"> <em>(图 4.6: K值对KNN决策边界的影响。较小的K值（如K=1）产生复杂的边界，容易过拟合。较大的K值（如K=20）产生较平滑的边界，可能欠拟合。)</em></p>
</section>
<section id="特征缩放-feature-scaling" class="level3">
<h3 class="anchored" data-anchor-id="特征缩放-feature-scaling">4.3.4 特征缩放 (Feature Scaling)</h3>
<p>由于KNN是基于距离的算法，如果不同特征的取值范围（尺度）差异很大，那么尺度较大的特征会在距离计算中占据主导地位，而尺度较小的特征可能几乎不起作用。</p>
<p>例如，一个特征的范围是 0-1，另一个特征的范围是 0-1000。在计算欧氏距离时，第二个特征的差异会被放大。</p>
<p><strong>因此，在使用KNN之前，对特征进行标准化 (Standardization) 或归一化 (Normalization) 通常是至关重要的步骤。</strong></p>
<ul>
<li><strong>标准化 (Z-score normalization):</strong> 将特征缩放到均值为0，标准差为1。 <span class="math display">\[ x' = \frac{x - \mu}{\sigma} \]</span></li>
<li><strong>归一化 (Min-Max scaling):</strong> 将特征缩放到一个特定的范围，通常是 [0, 1] 或 [-1, 1]。 <span class="math display">\[ x' = \frac{x - \min(x)}{\max(x) - \min(x)} \]</span></li>
</ul>
</section>
<section id="knn的优缺点" class="level3">
<h3 class="anchored" data-anchor-id="knn的优缺点">4.3.5 KNN的优缺点</h3>
<p><strong>优点：</strong></p>
<ul>
<li><strong>简单直观：</strong> 算法原理容易理解和实现。</li>
<li><strong>无需训练：</strong> KNN是一种”懒惰学习 (lazy learning)“算法，它不构建显式的模型，训练阶段仅仅是存储训练数据。预测阶段才进行计算。</li>
<li><strong>对数据分布没有假设：</strong> 作为非参数模型，它不要求数据符合特定分布。</li>
<li><strong>天然支持多分类：</strong> 直接通过多数表决即可。</li>
<li><strong>对异常点不敏感（当K较大时）：</strong> 少数异常点不太可能影响多数投票的结果。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>计算成本高：</strong> 预测阶段需要计算新样本与所有训练样本的距离，当训练集很大时，非常耗时。</li>
<li><strong>对K值敏感：</strong> K的选择直接影响模型性能。</li>
<li><strong>对特征尺度敏感：</strong> 需要进行特征缩放。</li>
<li><strong>“维度灾难”：</strong> 在高维空间中，所有点之间的距离趋向于变得一样远，使得”近邻”的概念变得不那么有意义。KNN在高维数据上性能通常会下降。</li>
<li><strong>样本不平衡问题：</strong> 如果某些类别的样本数量远多于其他类别，多数表决机制会偏向于这些多数类。</li>
<li><strong>需要存储所有训练数据：</strong> 内存开销大。</li>
</ul>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>直观理解维度灾难</strong></p>
<p>维度灾难 (Curse of Dimensionality) 是指在高维空间中出现的各种反直觉现象，这些现象会导致许多机器学习算法（特别是基于距离的算法如KNN）性能下降。我们可以通过一个简单的例子来理解：</p>
<ol type="1">
<li><p><strong>高维空间中的距离趋同现象</strong><br>
随着维度增加，所有样本点之间的距离会趋向于相同。这意味着”近邻”的概念在高维空间中变得模糊，KNN算法难以找到真正有意义的邻居。</p></li>
<li><p><strong>空间稀疏性</strong><br>
高维空间中数据点分布极其稀疏。例如，在10维单位超立方体中，即使有100万个数据点，每个维度上的平均间隔仍然很大。</p></li>
<li><p><strong>体积集中在边缘</strong><br>
高维空间中，大部分体积集中在超立方体的边缘附近。例如，在10维单位球中，99.9%的体积都在距离中心0.9半径以外的区域。</p></li>
<li><p><strong>可视化对比</strong></p>
<ul>
<li>2D空间：数据点可以均匀分布在平面上</li>
<li>100D空间：数据点几乎都位于”壳层”上，中心区域几乎是空的</li>
</ul></li>
</ol>
<p><strong>对KNN的影响</strong>：随着维度增加，分类准确率通常会先提高后下降，因为最初增加特征可以提供更多信息，但超过某个点后，噪声和距离趋同效应会主导结果。</p>
</div>
</div>
</div>
</section>
</section>
<section id="使用-scikit-learn-实现分类模型" class="level2">
<h2 class="anchored" data-anchor-id="使用-scikit-learn-实现分类模型">4.4 使用 Scikit-learn 实现分类模型</h2>
<p>下面我们将使用Scikit-learn库来演示逻辑回归和KNN的实现。我们将使用一个常见的分类数据集，例如鸢尾花 (Iris) 数据集或者一个合成的二分类数据集。为了更好地说明，我们先用一个简单的合成二分类数据集。</p>
<div id="a506788e" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 导入必要的库</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, cross_val_score, GridSearchCV</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report, roc_curve, auc</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification, load_iris</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> ListedColormap</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 设置绘图风格</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'seaborn-v0_8-whitegrid'</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>sns.set_palette(<span class="st">"husl"</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 生成一个简单的二分类合成数据集</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>X_cls, y_cls <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">200</span>, n_features<span class="op">=</span><span class="dv">2</span>, n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>                                   n_informative<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>, n_clusters_per_class<span class="op">=</span><span class="dv">1</span>, flip_y<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 可视化数据</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_cls[:, <span class="dv">0</span>], X_cls[:, <span class="dv">1</span>], c<span class="op">=</span>y_cls, cmap<span class="op">=</span>ListedColormap([<span class="st">'#FF0000'</span>, <span class="st">'#0000FF'</span>]), edgecolors<span class="op">=</span><span class="st">'k'</span>, s<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Synthetic Binary Classification Dataset'</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Feature 1'</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Feature 2'</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 划分训练集和测试集</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>X_cls_train, X_cls_test, y_cls_train, y_cls_test <span class="op">=</span> train_test_split(X_cls, y_cls, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>, stratify<span class="op">=</span>y_cls)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"训练集大小: X_train=</span><span class="sc">{</span>X_cls_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, y_train=</span><span class="sc">{</span>y_cls_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"测试集大小: X_test=</span><span class="sc">{</span>X_cls_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, y_test=</span><span class="sc">{</span>y_cls_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="co"># 特征缩放 (对于KNN尤其重要，逻辑回归有时也能从中受益)</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>X_cls_train_scaled <span class="op">=</span> scaler.fit_transform(X_cls_train)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>X_cls_test_scaled <span class="op">=</span> scaler.transform(X_cls_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04-classification-logreg-knn_files/figure-html/cell-8-output-1.png" width="652" height="516" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>训练集大小: X_train=(140, 2), y_train=(140,)
测试集大小: X_test=(60, 2), y_test=(60,)</code></pre>
</div>
</div>
<section id="逻辑回归-scikit-learn" class="level3">
<h3 class="anchored" data-anchor-id="逻辑回归-scikit-learn">4.4.1 逻辑回归 (Scikit-learn)</h3>
<div id="4a4a533b" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建逻辑回归模型实例</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># C是正则化强度的倒数，较小的C表示更强的正则化</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>log_reg_model <span class="op">=</span> LogisticRegression(C<span class="op">=</span><span class="fl">1.0</span>, solver<span class="op">=</span><span class="st">'liblinear'</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练模型 (可以使用原始数据或缩放后的数据，对于liblinear solver，缩放影响不大，但对于其他solver如lbfgs可能重要)</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>log_reg_model.fit(X_cls_train_scaled, y_cls_train) <span class="co"># 使用缩放数据</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 查看模型参数</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">逻辑回归模型:"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"截距 (beta_0): </span><span class="sc">{</span>log_reg_model<span class="sc">.</span>intercept_<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"系数 (beta_j): </span><span class="sc">{</span>log_reg_model<span class="sc">.</span>coef_<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 在测试集上进行预测</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>y_pred_log_reg <span class="op">=</span> log_reg_model.predict(X_cls_test_scaled)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>y_proba_log_reg <span class="op">=</span> log_reg_model.predict_proba(X_cls_test_scaled)[:, <span class="dv">1</span>] <span class="co"># 获取正类的概率</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 评估模型</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>accuracy_log_reg <span class="op">=</span> accuracy_score(y_cls_test, y_pred_log_reg)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"准确率 (Accuracy): </span><span class="sc">{</span>accuracy_log_reg<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">混淆矩阵:"</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_cls_test, y_pred_log_reg))</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">分类报告:"</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_cls_test, y_pred_log_reg))</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 绘制决策边界 (辅助函数)</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_decision_boundary(X, y, model, scaler, title, ax):</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    X_scaled <span class="op">=</span> scaler.transform(X) <span class="co"># 确保传入的是原始X</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="fl">0.5</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="fl">0.5</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="fl">0.5</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="fl">0.5</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, <span class="fl">0.02</span>),</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>                         np.arange(y_min, y_max, <span class="fl">0.02</span>))</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 对网格点进行缩放后预测</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>    mesh_points_scaled <span class="op">=</span> scaler.transform(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> model.predict(mesh_points_scaled)</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>    cmap_light <span class="op">=</span> ListedColormap([<span class="st">'#FFAAAA'</span>, <span class="st">'#AAAAFF'</span>])</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>    cmap_bold <span class="op">=</span> ListedColormap([<span class="st">'#FF0000'</span>, <span class="st">'#0000FF'</span>])</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>    ax.contourf(xx, yy, Z, cmap<span class="op">=</span>cmap_light, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>    ax.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span>cmap_bold, edgecolor<span class="op">=</span><span class="st">'k'</span>, s<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim(xx.<span class="bu">min</span>(), xx.<span class="bu">max</span>())</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim(yy.<span class="bu">min</span>(), yy.<span class="bu">max</span>())</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>    ax.set_title(title)</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Feature 1 (original scale)'</span>)</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Feature 2 (original scale)'</span>)</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(X_cls_test, y_cls_test, log_reg_model, scaler, <span class="st">'Logistic Regression Decision Boundary (Test Set)'</span>, ax)</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
逻辑回归模型:
截距 (beta_0): [0.03182521]
系数 (beta_j): [[-0.1118213   1.25736399]]
准确率 (Accuracy): 0.9000

混淆矩阵:
[[26  4]
 [ 2 28]]

分类报告:
              precision    recall  f1-score   support

           0       0.93      0.87      0.90        30
           1       0.88      0.93      0.90        30

    accuracy                           0.90        60
   macro avg       0.90      0.90      0.90        60
weighted avg       0.90      0.90      0.90        60
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04-classification-logreg-knn_files/figure-html/cell-9-output-2.png" width="652" height="516" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="k-近邻-knn-scikit-learn" class="level3">
<h3 class="anchored" data-anchor-id="k-近邻-knn-scikit-learn">4.4.2 K-近邻 (KNN) (Scikit-learn)</h3>
<div id="99a4e8a5" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建KNN分类器实例</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 先尝试一个常见的K值，例如 K=5</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>knn_model <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">5</span>, weights<span class="op">=</span><span class="st">'uniform'</span>, metric<span class="op">=</span><span class="st">'minkowski'</span>, p<span class="op">=</span><span class="dv">2</span>) <span class="co"># p=2 即欧氏距离</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练模型 (KNN的"训练"只是存储数据)</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># **务必使用缩放后的数据**</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>knn_model.fit(X_cls_train_scaled, y_cls_train)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 在测试集上进行预测</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>y_pred_knn <span class="op">=</span> knn_model.predict(X_cls_test_scaled)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>y_proba_knn <span class="op">=</span> knn_model.predict_proba(X_cls_test_scaled)[:, <span class="dv">1</span>] <span class="co"># 获取正类的概率</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 评估模型</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>accuracy_knn <span class="op">=</span> accuracy_score(y_cls_test, y_pred_knn)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">KNN模型 (K=5):"</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"准确率 (Accuracy): </span><span class="sc">{</span>accuracy_knn<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">混淆矩阵:"</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_cls_test, y_pred_knn))</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">分类报告:"</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_cls_test, y_pred_knn))</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(X_cls_test, y_cls_test, knn_model, scaler, <span class="st">'KNN (K=5) Decision Boundary (Test Set)'</span>, ax)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用交叉验证为KNN选择最优K值</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>k_range <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">31</span>)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>k_scores <span class="op">=</span> []</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k_val <span class="kw">in</span> k_range:</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    knn_cv <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span>k_val)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 使用5折交叉验证，评估指标为准确率</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> cross_val_score(knn_cv, X_cls_train_scaled, y_cls_train, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">'accuracy'</span>)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    k_scores.append(scores.mean())</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="co"># 绘制K值与准确率的关系图</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>plt.plot(k_range, k_scores, marker<span class="op">=</span><span class="st">'o'</span>, linestyle<span class="op">=</span><span class="st">'dashed'</span>)</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'KNN: K Value vs Cross-Validation Accuracy'</span>)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'K Value (n_neighbors)'</span>)</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cross-Validation Accuracy'</span>)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>plt.xticks(k_range)</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>best_k <span class="op">=</span> k_range[np.argmax(k_scores)]</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"通过交叉验证找到的最佳K值: </span><span class="sc">{</span>best_k<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a><span class="co"># 使用最佳K值重新训练和评估KNN</span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>knn_best_model <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span>best_k)</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>knn_best_model.fit(X_cls_train_scaled, y_cls_train)</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>y_pred_knn_best <span class="op">=</span> knn_best_model.predict(X_cls_test_scaled)</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>accuracy_knn_best <span class="op">=</span> accuracy_score(y_cls_test, y_pred_knn_best)</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">KNN模型 (最佳K=</span><span class="sc">{</span>best_k<span class="sc">}</span><span class="ss">):"</span>)</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"准确率 (Accuracy): </span><span class="sc">{</span>accuracy_knn_best<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_cls_test, y_pred_knn_best))</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(X_cls_test, y_cls_test, knn_best_model, scaler, <span class="ss">f'KNN (Best K=</span><span class="sc">{</span>best_k<span class="sc">}</span><span class="ss">) Decision Boundary (Test Set)'</span>, ax)</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
KNN模型 (K=5):
准确率 (Accuracy): 0.8833

混淆矩阵:
[[25  5]
 [ 2 28]]

分类报告:
              precision    recall  f1-score   support

           0       0.93      0.83      0.88        30
           1       0.85      0.93      0.89        30

    accuracy                           0.88        60
   macro avg       0.89      0.88      0.88        60
weighted avg       0.89      0.88      0.88        60
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04-classification-logreg-knn_files/figure-html/cell-10-output-2.png" width="652" height="516" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04-classification-logreg-knn_files/figure-html/cell-10-output-3.png" width="811" height="516" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>通过交叉验证找到的最佳K值: 21

KNN模型 (最佳K=21):
准确率 (Accuracy): 0.8500
              precision    recall  f1-score   support

           0       0.92      0.77      0.84        30
           1       0.80      0.93      0.86        30

    accuracy                           0.85        60
   macro avg       0.86      0.85      0.85        60
weighted avg       0.86      0.85      0.85        60
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="04-classification-logreg-knn_files/figure-html/cell-10-output-5.png" width="652" height="516" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="比较逻辑回归和knn" class="level3">
<h3 class="anchored" data-anchor-id="比较逻辑回归和knn">4.4.3 比较逻辑回归和KNN</h3>
<p>在我们的简单合成数据集上，两种模型可能表现相似。在实际应用中：</p>
<ul>
<li><strong>逻辑回归：</strong>
<ul>
<li>优点：速度快，可解释性好，输出概率。</li>
<li>缺点：线性模型，对非线性问题能力有限（除非特征工程）。</li>
<li>适用场景：需要快速基线模型，数据线性可分或特征关系简单，需要概率输出或模型解释。</li>
</ul></li>
<li><strong>KNN：</strong>
<ul>
<li>优点：简单，非参数，能捕捉非线性关系。</li>
<li>缺点：计算成本高，对K和特征尺度敏感，维度灾难。</li>
<li>适用场景：数据集不大，特征数量不多，数据分布未知或复杂，不需要显式模型。</li>
</ul></li>
</ul>
<p>通常，在实际项目中，我们会尝试多种模型，并通过交叉验证来选择最适合特定任务和数据的模型。</p>
</section>
</section>
<section id="本章小结" class="level2">
<h2 class="anchored" data-anchor-id="本章小结">4.5 本章小结</h2>
<p>本章我们进入了监督学习的另一大领域——分类。我们首先明确了分类问题的定义及其与回归的区别。随后，详细学习了两种基础且重要的分类算法：</p>
<ol type="1">
<li><strong>逻辑回归：</strong> 尽管名为回归，但它是一种强大的线性分类器。我们探讨了其核心组件Sigmoid函数、线性决策边界的形成、对数损失（交叉熵）代价函数，以及如何通过梯度下降进行优化。我们还简要提及了其在多分类场景下的扩展（如Softmax回归和OvR策略）。</li>
<li><strong>K-近邻 (KNN)：</strong> 一种简单直观的非参数”懒惰学习”算法。我们学习了其基于”近朱者赤”思想的分类原理、常用的距离度量方法、K值选择的关键性以及特征缩放的必要性。</li>
</ol>
<p>通过Scikit-learn的实践，我们掌握了这两种模型在Python中的实现、训练、评估和基本调优（如KNN的K值选择）。理解这两种算法的特性、优缺点及其适用场景，对于后续学习更复杂的分类模型和解决实际问题至关重要。</p>
</section>
<section id="思考与练习" class="level2">
<h2 class="anchored" data-anchor-id="思考与练习">4.6 思考与练习</h2>
<section id="基础练习" class="level3">
<h3 class="anchored" data-anchor-id="基础练习">4.6.1 基础练习</h3>
<ol type="1">
<li><strong>概念辨析：</strong>
<ul>
<li>逻辑回归的输出 <span class="math inline">\(h_{\mathbf{\beta}}(\mathbf{x})\)</span> 代表什么？它如何用于分类决策？</li>
<li>为什么逻辑回归不直接使用均方误差作为代价函数？它使用的代价函数是什么，有何优点？</li>
<li>KNN算法中的”K”代表什么？K值过大或过小分别可能导致什么问题？</li>
<li>为什么说特征缩放对KNN算法通常是必要的，而对逻辑回归（某些情况下）不是那么关键？</li>
</ul></li>
<li><strong>决策边界：</strong>
<ul>
<li>逻辑回归产生的决策边界一定是线性的吗？如果不是，如何实现非线性决策边界？</li>
<li>KNN算法产生的决策边界是怎样的？它与K值有何关系？</li>
</ul></li>
<li><strong>算法对比：</strong>
<ul>
<li>从训练速度、预测速度、模型可解释性、对数据分布的假设等方面比较逻辑回归和KNN。</li>
<li>“懒惰学习”是什么意思？KNN属于懒惰学习吗？逻辑回归呢？</li>
</ul></li>
</ol>
</section>
<section id="编码与实践-可使用iris数据集" class="level3">
<h3 class="anchored" data-anchor-id="编码与实践-可使用iris数据集">4.6.2 编码与实践 (可使用Iris数据集)</h3>
<p>Scikit-learn内置了鸢尾花 (Iris) 数据集，它是一个经典的多分类（3个类别）数据集。</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> load_iris()</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>X_iris <span class="op">=</span> iris.data[:, :<span class="dv">2</span>] <span class="co"># 为了方便可视化，仅使用前两个特征（萼片长度和宽度）</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>y_iris <span class="op">=</span> iris.target</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 接下来可以进行数据划分、缩放、模型训练和评估</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol type="1">
<li><strong>逻辑回归实践 (Iris 数据集)：</strong>
<ul>
<li>使用上述Iris数据（或完整的4个特征），将其划分为训练集和测试集。</li>
<li>训练一个逻辑回归模型（Scikit-learn的 <code>LogisticRegression</code> 默认可以处理多分类问题，通常使用OvR策略）。</li>
<li>评估模型的准确率，并打印混淆矩阵和分类报告。</li>
<li>如果只使用前两个特征，尝试绘制决策边界。</li>
</ul></li>
<li><strong>KNN实践 (Iris 数据集)：</strong>
<ul>
<li>对Iris数据进行特征缩放。</li>
<li>使用交叉验证为KNN模型在Iris数据集上寻找一个合适的K值。</li>
<li>使用找到的最佳K值训练KNN模型，并在测试集上评估其性能（准确率、混淆矩阵、分类报告）。</li>
<li>如果只使用前两个特征，尝试绘制不同K值下的决策边界，观察其变化。</li>
</ul></li>
<li><strong>距离度量影响 (KNN)：</strong>
<ul>
<li>在KNN模型中，尝试使用不同的距离度量（<code>metric</code> 参数，如 <code>'euclidean'</code>, <code>'manhattan'</code>）。观察它们对模型在Iris数据集上（或你选择的其他数据集）的性能是否有影响。</li>
</ul></li>
</ol>
</section>
<section id="推荐阅读" class="level3">
<h3 class="anchored" data-anchor-id="推荐阅读">4.6.3 推荐阅读</h3>
<ul>
<li><strong>《An Introduction to Statistical Learning (with Applications in R or Python)》 - Chapter 4: Classification:</strong> (ISLR/ISLP) 详细介绍了逻辑回归、LDA、QDA和KNN等分类方法。(<a href="https://www.statlearning.com/" class="uri">https://www.statlearning.com/</a>)</li>
<li><strong>《动手学深度学习》 - Softmax回归章节:</strong> 虽然侧重深度学习，但其对Softmax回归的从零实现有助于理解多分类逻辑回归的底层。 (<a href="https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html" class="uri">https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html</a>)</li>
<li><strong>Scikit-learn官方文档 - Logistic Regression:</strong> (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a>)</li>
<li><strong>Scikit-learn官方文档 - KNeighborsClassifier:</strong> (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html</a>)</li>
<li><strong>StatQuest with Josh Starmer - Logistic Regression / KNN:</strong> (YouTube频道) Josh用非常直观和易懂的方式解释了这些概念。</li>
</ul>
<p></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./03-regression.html" class="pagination-link" aria-label="回归与线性模型">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">回归与线性模型</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./05-svm.html" class="pagination-link" aria-label="支持向量机 (SVM)">
        <span class="nav-page-text"><span class="chapter-title">支持向量机 (SVM)</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>