<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>循环神经网络 (RNN) – 机器学习：从理论到Python实践</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./13-deep-learning-advanced.html" rel="next">
<link href="./11-cnn.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-deep-learning-basics.html">第五部分：深度学习初探</a></li><li class="breadcrumb-item"><a href="./12-rnn.html"><span class="chapter-title">循环神经网络 (RNN)</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">机器学习：从理论到Python实践</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">欢迎学习《机器学习：从理论到Python实践》</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">第一部分：机器学习基石与Python生态</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">机器学习导论</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-environment.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Python机器学习环境与核心库</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">第二部分：监督学习</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">回归与线性模型</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-classification-logreg-knn.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">分类与逻辑回归、KNN</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-svm.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">支持向量机 (SVM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-decision-trees-ensemble-learning.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">决策树与集成学习</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">第三部分：无监督学习</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-clustering.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">聚类分析</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-dimensionality-reduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">降维</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">第四部分：模型评估、优化与特征工程</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-model-evaluation-feature-engineering.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">模型评估、优化与特征工程</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">第五部分：深度学习初探</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-deep-learning-basics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">深度学习基础</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-cnn.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">卷积神经网络 (CNN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-rnn.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">循环神经网络 (RNN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-deep-learning-advanced.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">深度学习进阶</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">第六部分：强化学习入门</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-reinforcement-learning.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">强化学习基础与应用</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">第七部分：综合项目与展望</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-ml-project-workflow-summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">机器学习项目实战流程与总结</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-summary-outlook.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">课程总结与展望</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendices.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">附录</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#学习目标" id="toc-学习目标" class="nav-link active" data-scroll-target="#学习目标">学习目标</a></li>
  <li><a href="#序列数据与rnn简介" id="toc-序列数据与rnn简介" class="nav-link" data-scroll-target="#序列数据与rnn简介">12.1 序列数据与RNN简介</a></li>
  <li><a href="#rnn的结构与工作原理" id="toc-rnn的结构与工作原理" class="nav-link" data-scroll-target="#rnn的结构与工作原理">12.2 RNN的结构与工作原理</a></li>
  <li><a href="#rnn的变种与改进lstm和gru" id="toc-rnn的变种与改进lstm和gru" class="nav-link" data-scroll-target="#rnn的变种与改进lstm和gru">12.3 RNN的变种与改进：LSTM和GRU</a>
  <ul class="collapse">
  <li><a href="#长短期记忆网络-long-short-term-memory-lstm" id="toc-长短期记忆网络-long-short-term-memory-lstm" class="nav-link" data-scroll-target="#长短期记忆网络-long-short-term-memory-lstm">12.3.1 长短期记忆网络 (Long Short-Term Memory, LSTM)</a></li>
  <li><a href="#门控循环单元-gated-recurrent-unit-gru" id="toc-门控循环单元-gated-recurrent-unit-gru" class="nav-link" data-scroll-target="#门控循环单元-gated-recurrent-unit-gru">12.3.2 门控循环单元 (Gated Recurrent Unit, GRU)</a></li>
  <li><a href="#双向rnn-bidirectional-rnn" id="toc-双向rnn-bidirectional-rnn" class="nav-link" data-scroll-target="#双向rnn-bidirectional-rnn">12.3.3 双向RNN (Bidirectional RNN)</a></li>
  <li><a href="#深层rnn-deep-rnn-stacked-rnn" id="toc-深层rnn-deep-rnn-stacked-rnn" class="nav-link" data-scroll-target="#深层rnn-deep-rnn-stacked-rnn">12.3.4 深层RNN (Deep RNN / Stacked RNN)</a></li>
  </ul></li>
  <li><a href="#使用keras构建简单rnnlstm-文本情感分析示例" id="toc-使用keras构建简单rnnlstm-文本情感分析示例" class="nav-link" data-scroll-target="#使用keras构建简单rnnlstm-文本情感分析示例">12.4 使用Keras构建简单RNN/LSTM (文本情感分析示例)</a>
  <ul class="collapse">
  <li><a href="#数据准备-imdb-数据集" id="toc-数据准备-imdb-数据集" class="nav-link" data-scroll-target="#数据准备-imdb-数据集">12.4.1 数据准备 (IMDb 数据集)</a></li>
  <li><a href="#模型定义-使用lstm" id="toc-模型定义-使用lstm" class="nav-link" data-scroll-target="#模型定义-使用lstm">12.4.2 模型定义 (使用LSTM)</a></li>
  <li><a href="#模型编译" id="toc-模型编译" class="nav-link" data-scroll-target="#模型编译">12.4.3 模型编译</a></li>
  <li><a href="#模型训练" id="toc-模型训练" class="nav-link" data-scroll-target="#模型训练">12.4.4 模型训练</a></li>
  <li><a href="#模型评估" id="toc-模型评估" class="nav-link" data-scroll-target="#模型评估">12.4.5 模型评估</a></li>
  </ul></li>
  <li><a href="#rnn的应用领域" id="toc-rnn的应用领域" class="nav-link" data-scroll-target="#rnn的应用领域">12.5 RNN的应用领域</a></li>
  <li><a href="#rnn的挑战" id="toc-rnn的挑战" class="nav-link" data-scroll-target="#rnn的挑战">12.6 RNN的挑战</a></li>
  <li><a href="#本章总结" id="toc-本章总结" class="nav-link" data-scroll-target="#本章总结">12.7 本章总结</a></li>
  <li><a href="#思考与练习" id="toc-思考与练习" class="nav-link" data-scroll-target="#思考与练习">12.8 思考与练习</a>
  <ul class="collapse">
  <li><a href="#基础概念回顾" id="toc-基础概念回顾" class="nav-link" data-scroll-target="#基础概念回顾">12.8.1 基础概念回顾</a></li>
  <li><a href="#keras实践与探索" id="toc-keras实践与探索" class="nav-link" data-scroll-target="#keras实践与探索">12.8.2 Keras实践与探索</a></li>
  <li><a href="#深入思考与挑战" id="toc-深入思考与挑战" class="nav-link" data-scroll-target="#深入思考与挑战">12.8.3 深入思考与挑战</a></li>
  <li><a href="#推荐阅读" id="toc-推荐阅读" class="nav-link" data-scroll-target="#推荐阅读">12.8.4 推荐阅读</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-deep-learning-basics.html">第五部分：深度学习初探</a></li><li class="breadcrumb-item"><a href="./12-rnn.html"><span class="chapter-title">循环神经网络 (RNN)</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">循环神经网络 (RNN)</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="学习目标" class="level2">
<h2 class="anchored" data-anchor-id="学习目标">学习目标</h2>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>学习目标：</strong></p>
<ul>
<li>理解序列数据的特点以及为什么需要专门的网络结构来处理它们。</li>
<li>掌握循环神经网络 (RNN) 的基本原理，包括隐藏状态和时间上的循环连接。</li>
<li>了解标准RNN面临的梯度消失/爆炸问题。</li>
<li>理解长短期记忆网络 (LSTM) 的核心思想和门控机制（输入门、遗忘门、输出门、细胞状态）。</li>
<li>理解门控循环单元 (GRU) 作为LSTM的一种简化变体的结构和工作原理。</li>
<li>能够使用Keras构建、编译、训练和评估简单的RNN、LSTM或GRU模型处理序列数据（如文本分类或简单时间序列）。</li>
<li>了解RNN在自然语言处理、时间序列分析等领域的常见应用。</li>
<li>对双向RNN和深层RNN有初步认识。</li>
</ul>
</div>
</div>
</div>
</section>
<section id="序列数据与rnn简介" class="level2">
<h2 class="anchored" data-anchor-id="序列数据与rnn简介">12.1 序列数据与RNN简介</h2>
<p>在前面的章节中，我们学习了全连接神经网络 (MLP) 和卷积神经网络 (CNN)。MLP适用于处理扁平化的向量数据，而CNN则特别擅长处理具有网格结构的数据（如图像）。然而，现实世界中还有一类非常重要的数据类型——<strong>序列数据 (Sequential Data)</strong>。</p>
<p><strong>什么是序列数据？</strong></p>
<p>序列数据是指其元素具有特定顺序的数据。改变元素的顺序通常会改变数据的含义。例子包括：</p>
<ul>
<li><strong>文本 (Text)：</strong> 单词或字符的序列。</li>
<li><strong>语音 (Speech)：</strong> 音频帧的序列。</li>
<li><strong>时间序列 (Time Series)：</strong> 股票价格、天气数据、传感器读数等按时间排序的观测值。</li>
<li><strong>视频 (Video)：</strong> 图像帧的序列。</li>
<li><strong>DNA序列：</strong> 核苷酸的序列。</li>
</ul>
<p><strong>为什么传统网络不适合处理序列数据？</strong></p>
<ol type="1">
<li><strong>输入/输出长度不固定：</strong> 传统的前馈网络通常要求固定大小的输入和输出。然而，序列数据的长度往往是可变的（例如，不同长度的句子）。</li>
<li><strong>无法共享跨时间步的特征：</strong> 传统网络独立处理每个时间步的输入（如果强行切分成固定长度片段的话），无法有效地学习和利用序列中跨越不同时间步的依赖关系和模式。</li>
<li><strong>没有记忆能力：</strong> 它们不具备”记忆”先前信息以影响后续处理的能力。</li>
</ol>
<p><strong>循环神经网络 (Recurrent Neural Network, RNN)</strong> 是一类专门设计用来处理序列数据的神经网络。RNN的核心思想是引入<strong>循环 (Recurrence)</strong>，使得网络在处理序列中的当前元素时，能够利用先前元素的信息。</p>
<div id="fig-rnn-unfold" class="quarto-float quarto-figure quarto-figure-center anchored" alt="RNN Unfolded">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rnn-unfold-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/12-rnn/Recurrent_neural_network_unfold.svg.png" class="img-fluid figure-img" alt="RNN Unfolded" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rnn-unfold-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.1: 简单RNN示意图 (来源: Wikipedia)
</figcaption>
</figure>
</div>
<p>上图左边是RNN的折叠表示，其中循环箭头表示信息会从当前时间步的输出反馈回下一个时间步的输入。右边是RNN按时间展开的表示，更清晰地显示了信息如何在序列中传递。</p>
<p>RNN通过一个内部的<strong>隐藏状态 (Hidden State)</strong> 或<strong>记忆 (Memory)</strong> 来捕捉序列中的历史信息。在每个时间步，RNN接收当前输入和前一个时间步的隐藏状态，然后计算新的隐藏状态和当前时间步的输出。</p>
</section>
<section id="rnn的结构与工作原理" class="level2">
<h2 class="anchored" data-anchor-id="rnn的结构与工作原理">12.2 RNN的结构与工作原理</h2>
<p>一个简单的RNN单元（有时称为Elman Network）在时间步 <span class="math inline">\(t\)</span> 的操作可以描述如下：</p>
<ol type="1">
<li><strong>输入：</strong>
<ul>
<li>当前时间步的输入向量 <span class="math inline">\(\mathbf{x}_t\)</span>。</li>
<li>前一个时间步的隐藏状态 <span class="math inline">\(\mathbf{h}_{t-1}\)</span> (对于第一个时间步 <span class="math inline">\(t=0\)</span>，<span class="math inline">\(\mathbf{h}_{-1}\)</span> 通常初始化为零向量)。</li>
</ul></li>
<li><strong>计算新的隐藏状态 <span class="math inline">\(\mathbf{h}_t\)</span>：</strong> <span class="math display">\[ \mathbf{h}_t = f(\mathbf{W}_{hh} \mathbf{h}_{t-1} + \mathbf{W}_{xh} \mathbf{x}_t + \mathbf{b}_h) \]</span> 其中：
<ul>
<li><span class="math inline">\(\mathbf{W}_{hh}\)</span> 是隐藏状态到隐藏状态的权重矩阵 (recurrent weights)。</li>
<li><span class="math inline">\(\mathbf{W}_{xh}\)</span> 是输入到隐藏状态的权重矩阵。</li>
<li><span class="math inline">\(\mathbf{b}_h\)</span> 是隐藏层的偏置向量。</li>
<li><span class="math inline">\(f\)</span> 是一个非线性激活函数，通常是 tanh 或 ReLU。</li>
</ul></li>
<li><strong>计算当前时间步的输出 <span class="math inline">\(\mathbf{y}_t\)</span> (可选)：</strong> <span class="math display">\[ \mathbf{y}_t = g(\mathbf{W}_{hy} \mathbf{h}_t + \mathbf{b}_y) \]</span> 其中：
<ul>
<li><span class="math inline">\(\mathbf{W}_{hy}\)</span> 是隐藏状态到输出的权重矩阵。</li>
<li><span class="math inline">\(\mathbf{b}_y\)</span> 是输出层的偏置向量。</li>
<li><span class="math inline">\(g\)</span> 是输出层的激活函数（例如，对于分类任务是Softmax，对于回归任务是线性函数）。</li>
</ul></li>
</ol>
<p><strong>关键特性：</strong></p>
<ul>
<li><strong>参数共享：</strong> 权重矩阵 <span class="math inline">\(\mathbf{W}_{hh}, \mathbf{W}_{xh}, \mathbf{W}_{hy}\)</span> 和偏置 <span class="math inline">\(\mathbf{b}_h, \mathbf{b}_y\)</span> 在所有时间步都是共享的。这使得RNN能够处理不同长度的序列，并且模型参数数量不随序列长度增加而增加。</li>
<li><strong>记忆：</strong> 隐藏状态 <span class="math inline">\(\mathbf{h}_t\)</span> 充当了网络的记忆，它编码了直到时间步 <span class="math inline">\(t\)</span> 的所有相关历史信息。</li>
</ul>
<p><strong>沿时间反向传播 (Backpropagation Through Time, BPTT)</strong></p>
<p>RNN的训练通常使用BPTT算法。由于参数在所有时间步共享，损失函数对某个参数的梯度是该参数在所有时间步对损失贡献的梯度之和。BPTT将RNN按时间展开，然后应用标准的反向传播算法来计算这些梯度。</p>
<p><strong>标准RNN的局限性：梯度消失/爆炸问题</strong></p>
<p>虽然理论上RNN可以捕捉长距离依赖关系，但在实践中，训练标准RNN（SimpleRNN）处理长序列时，经常会遇到<strong>梯度消失 (Vanishing Gradients)</strong> 或<strong>梯度爆炸 (Exploding Gradients)</strong> 的问题：</p>
<ul>
<li><strong>梯度消失：</strong> 当序列很长时，通过BPTT计算的梯度在向早期时间步传播时，可能会指数级衰减变小，接近于零。这使得网络难以学习到序列中早期信息对后期输出的影响，即难以捕捉长期依赖。</li>
<li><strong>梯度爆炸：</strong> 相反，梯度也可能指数级增长变得非常大，导致训练不稳定。梯度裁剪 (Gradient Clipping) 通常用来缓解这个问题。</li>
</ul>
<div class="callout callout-style-simple callout-warning">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>梯度消失问题：标准RNN的主要瓶颈</strong></p>
<p>标准RNN在处理长序列时面临的主要挑战是梯度消失问题。当序列较长时，通过反向传播计算的梯度会随着时间步的增加而指数级衰减，导致早期时间步的参数几乎无法得到有效更新，从而难以学习长期依赖关系。</p>
</div>
</div>
</div>
</section>
<section id="rnn的变种与改进lstm和gru" class="level2">
<h2 class="anchored" data-anchor-id="rnn的变种与改进lstm和gru">12.3 RNN的变种与改进：LSTM和GRU</h2>
<p>为了解决标准RNN的梯度消失问题并更好地捕捉长期依赖，研究人员提出了更复杂的循环单元，其中最著名的是长短期记忆网络 (LSTM) 和门控循环单元 (GRU)。</p>
<section id="长短期记忆网络-long-short-term-memory-lstm" class="level3">
<h3 class="anchored" data-anchor-id="长短期记忆网络-long-short-term-memory-lstm">12.3.1 长短期记忆网络 (Long Short-Term Memory, LSTM)</h3>
<p>LSTM由Sepp Hochreiter和Jürgen Schmidhuber在1997年提出，是一种特殊的RNN架构，通过引入精巧的<strong>门控机制 (Gating Mechanism)</strong> 来控制信息的流动和记忆的更新。</p>
<p>一个LSTM单元的核心组成部分：</p>
<ol type="1">
<li><strong>细胞状态 (Cell State, <span class="math inline">\(\mathbf{c}_t\)</span>)：</strong>
<ul>
<li>这是LSTM的关键，它像一条传送带一样在整个时间链中运行，信息可以很容易地保持不变地流过它。</li>
<li>细胞状态允许LSTM有效地存储和传递长期信息。</li>
</ul></li>
<li><strong>门 (Gates)：</strong>
<ul>
<li>LSTM有三个主要的门，它们是学习到的函数，用于控制信息是否应该被添加或移除出细胞状态。这些门由Sigmoid激活函数（输出0到1之间的值，表示允许多少信息通过）和一个逐点乘法操作组成。</li>
<li><strong>遗忘门 (Forget Gate, <span class="math inline">\(\mathbf{f}_t\)</span>)：</strong> 决定从细胞状态中丢弃哪些信息。 <span class="math display">\[ \mathbf{f}_t = \sigma(\mathbf{W}_f [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f) \]</span></li>
<li><strong>输入门 (Input Gate, <span class="math inline">\(\mathbf{i}_t\)</span>)：</strong> 决定哪些新的信息要存储到细胞状态中。 <span class="math display">\[ \mathbf{i}_t = \sigma(\mathbf{W}_i [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i) \]</span> 同时，一个tanh层创建一个候选值向量 <span class="math inline">\(\tilde{\mathbf{c}}_t\)</span>，可以被添加到状态中： <span class="math display">\[ \tilde{\mathbf{c}}_t = \tanh(\mathbf{W}_c [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_c) \]</span></li>
<li><strong>更新细胞状态：</strong> <span class="math display">\[ \mathbf{c}_t = \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t \]</span> (其中 <span class="math inline">\(\odot\)</span> 表示逐元素乘法)</li>
<li><strong>输出门 (Output Gate, <span class="math inline">\(\mathbf{o}_t\)</span>)：</strong> 决定输出什么作为隐藏状态 <span class="math inline">\(\mathbf{h}_t\)</span>。 <span class="math display">\[ \mathbf{o}_t = \sigma(\mathbf{W}_o [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o) \]</span> <span class="math display">\[ \mathbf{h}_t = \mathbf{o}_t \odot \tanh(\mathbf{c}_t) \]</span></li>
</ul></li>
</ol>
<div id="fig-lstm-unit" class="quarto-float quarto-figure quarto-figure-center anchored" alt="LSTM Unit">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lstm-unit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" class="img-fluid figure-img" alt="LSTM Unit" width="700">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lstm-unit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.2: LSTM单元结构图 (来源: Chris Olah’s blog)
</figcaption>
</figure>
</div>
<p>通过这些门控机制，LSTM可以学习在序列中何时应该忘记旧信息、何时应该接纳新信息，以及何时应该输出信息，从而有效地捕捉长期依赖关系。</p>
</section>
<section id="门控循环单元-gated-recurrent-unit-gru" class="level3">
<h3 class="anchored" data-anchor-id="门控循环单元-gated-recurrent-unit-gru">12.3.2 门控循环单元 (Gated Recurrent Unit, GRU)</h3>
<p>GRU由Kyunghyun Cho等人在2014年提出，是LSTM的一种流行的变体，它简化了LSTM的结构，同时保持了相似的性能。</p>
<p>GRU的主要特点：</p>
<ul>
<li>将LSTM中的遗忘门和输入门合并为一个<strong>更新门 (Update Gate, <span class="math inline">\(\mathbf{z}_t\)</span>)</strong>。</li>
<li>引入一个<strong>重置门 (Reset Gate, <span class="math inline">\(\mathbf{r}_t\)</span>)</strong>。</li>
<li>没有单独的细胞状态 <span class="math inline">\(\mathbf{c}_t\)</span>，隐藏状态 <span class="math inline">\(\mathbf{h}_t\)</span> 直接承载记忆信息。</li>
</ul>
<p>GRU的计算过程：</p>
<ol type="1">
<li><strong>重置门 <span class="math inline">\(\mathbf{r}_t\)</span>：</strong> 控制前一个隐藏状态 <span class="math inline">\(\mathbf{h}_{t-1}\)</span> 有多少信息被遗忘。 <span class="math display">\[ \mathbf{r}_t = \sigma(\mathbf{W}_r [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_r) \]</span></li>
<li><strong>更新门 <span class="math inline">\(\mathbf{z}_t\)</span>：</strong> 控制新的候选隐藏状态与前一个隐藏状态如何结合。 <span class="math display">\[ \mathbf{z}_t = \sigma(\mathbf{W}_z [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_z) \]</span></li>
<li><strong>候选隐藏状态 <span class="math inline">\(\tilde{\mathbf{h}}_t\)</span>：</strong> <span class="math display">\[ \tilde{\mathbf{h}}_t = \tanh(\mathbf{W}_h [\mathbf{r}_t \odot \mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_h) \]</span></li>
<li><strong>新的隐藏状态 <span class="math inline">\(\mathbf{h}_t\)</span>：</strong> <span class="math display">\[ \mathbf{h}_t = (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \tilde{\mathbf{h}}_t \]</span></li>
</ol>
<div id="fig-gru-unit" class="quarto-float quarto-figure quarto-figure-center anchored" alt="GRU Unit">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gru-unit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png" class="img-fluid figure-img" alt="GRU Unit" width="500">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gru-unit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.3: GRU单元结构图 (来源: Chris Olah’s blog)
</figcaption>
</figure>
</div>
<p>GRU的参数比LSTM少，计算上通常更高效一些，并且在许多任务上表现与LSTM相当。选择LSTM还是GRU通常取决于具体的任务和经验性的尝试。</p>
</section>
<section id="双向rnn-bidirectional-rnn" class="level3">
<h3 class="anchored" data-anchor-id="双向rnn-bidirectional-rnn">12.3.3 双向RNN (Bidirectional RNN)</h3>
<p>在某些任务中（例如，自然语言理解），当前时间步的输出可能不仅依赖于过去的输入，还依赖于未来的输入。例如，要理解一个句子中某个词的含义，我们通常需要看它前面的词和后面的词。</p>
<p><strong>双向RNN (Bidirectional RNN, BiRNN)</strong> 通过使用两个独立的RNN层来处理这个问题：</p>
<ul>
<li>一个RNN按正向顺序处理输入序列（从 <span class="math inline">\(t=1\)</span> 到 <span class="math inline">\(t=T\)</span>）。</li>
<li>另一个RNN按反向顺序处理输入序列（从 <span class="math inline">\(t=T\)</span> 到 <span class="math inline">\(t=1\)</span>）。</li>
</ul>
<p>在每个时间步 <span class="math inline">\(t\)</span>，BiRNN的输出通常是将正向RNN的隐藏状态 <span class="math inline">\(\overrightarrow{\mathbf{h}_t}\)</span> 和反向RNN的隐藏状态 <span class="math inline">\(\overleftarrow{\mathbf{h}_t}\)</span> 拼接起来：<span class="math inline">\(\mathbf{h}_t = [\overrightarrow{\mathbf{h}_t}, \overleftarrow{\mathbf{h}_t}]\)</span>。</p>
<p>BiRNN能够同时捕捉序列中的过去和未来上下文信息，在许多NLP任务中表现优于单向RNN。</p>
</section>
<section id="深层rnn-deep-rnn-stacked-rnn" class="level3">
<h3 class="anchored" data-anchor-id="深层rnn-deep-rnn-stacked-rnn">12.3.4 深层RNN (Deep RNN / Stacked RNN)</h3>
<p>与CNN类似，我们也可以通过堆叠多个RNN层来构建<strong>深层RNN (Deep RNN)</strong> 或 <strong>堆叠RNN (Stacked RNN)</strong>。</p>
<p>在深层RNN中，一个RNN层的输出序列作为下一个RNN层的输入序列。这使得网络能够学习数据中更复杂、更层次化的时间模式。</p>
</section>
</section>
<section id="使用keras构建简单rnnlstm-文本情感分析示例" class="level2">
<h2 class="anchored" data-anchor-id="使用keras构建简单rnnlstm-文本情感分析示例">12.4 使用Keras构建简单RNN/LSTM (文本情感分析示例)</h2>
<p>我们将使用Keras构建一个LSTM网络对IMDb电影评论数据集进行情感分析（二分类问题：正面/负面评论）。</p>
<section id="数据准备-imdb-数据集" class="level3">
<h3 class="anchored" data-anchor-id="数据准备-imdb-数据集">12.4.1 数据准备 (IMDb 数据集)</h3>
<p>IMDb数据集包含50,000条电影评论，已经预处理并编码为整数序列（每个整数代表字典中的一个特定单词）。</p>
<div id="code-rnn-imdb-data-prep" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.datasets <span class="im">import</span> imdb</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.sequence <span class="im">import</span> pad_sequences</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 参数设置</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>max_features <span class="op">=</span> <span class="dv">10000</span>  <span class="co"># 只考虑数据集中最常见的10000个词</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>maxlen <span class="op">=</span> <span class="dv">200</span>         <span class="co"># 每条评论只考虑最后200个词 (如果评论太长则截断，太短则填充)</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载数据</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>(X_train, y_train), (X_test, y_test) <span class="op">=</span> imdb.load_data(num_words<span class="op">=</span>max_features)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"原始训练样本数: </span><span class="sc">{</span><span class="bu">len</span>(X_train)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"原始测试样本数: </span><span class="sc">{</span><span class="bu">len</span>(X_test)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 查看一条评论 (编码后的整数序列)</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">第一条训练评论 (编码后): </span><span class="sc">{</span>X_train[<span class="dv">0</span>][:<span class="dv">20</span>]<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"第一条训练评论的标签: </span><span class="sc">{</span>y_train[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 将整数序列填充或截断到相同的长度 (maxlen)</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># padding='pre' 表示在序列前面填充0, truncating='pre' 表示从前面截断</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>X_train_padded <span class="op">=</span> pad_sequences(X_train, maxlen<span class="op">=</span>maxlen, padding<span class="op">=</span><span class="st">'pre'</span>, truncating<span class="op">=</span><span class="st">'pre'</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>X_test_padded <span class="op">=</span> pad_sequences(X_test, maxlen<span class="op">=</span>maxlen, padding<span class="op">=</span><span class="st">'pre'</span>, truncating<span class="op">=</span><span class="st">'pre'</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">填充/截断后的训练集形状: </span><span class="sc">{</span>X_train_padded<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>) <span class="co"># (25000, 200)</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"填充/截断后的测试集形状: </span><span class="sc">{</span>X_test_padded<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)   <span class="co"># (25000, 200)</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"填充后的第一条训练评论: </span><span class="ch">\n</span><span class="sc">{</span>X_train_padded[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>原始训练样本数: 25000
原始测试样本数: 25000

第一条训练评论 (编码后): [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25]...
第一条训练评论的标签: 1

填充/截断后的训练集形状: (25000, 200)
填充/截断后的测试集形状: (25000, 200)
填充后的第一条训练评论: 
[   5   25  100   43  838  112   50  670    2    9   35  480  284    5
  150    4  172  112  167    2  336  385   39    4  172 4536 1111   17
  546   38   13  447    4  192   50   16    6  147 2025   19   14   22
    4 1920 4613  469    4   22   71   87   12   16   43  530   38   76
   15   13 1247    4   22   17  515   17   12   16  626   18    2    5
   62  386   12    8  316    8  106    5    4 2223 5244   16  480   66
 3785   33    4  130   12   16   38  619    5   25  124   51   36  135
   48   25 1415   33    6   22   12  215   28   77   52    5   14  407
   16   82    2    8    4  107  117 5952   15  256    4    2    7 3766
    5  723   36   71   43  530  476   26  400  317   46    7    4    2
 1029   13  104   88    4  381   15  297   98   32 2071   56   26  141
    6  194 7486   18    4  226   22   21  134  476   26  480    5  144
   30 5535   18   51   36   28  224   92   25  104    4  226   65   16
   38 1334   88   12   16  283    5   16 4472  113  103   32   15   16
 5345   19  178   32]</code></pre>
</div>
</div>
</section>
<section id="模型定义-使用lstm" class="level3">
<h3 class="anchored" data-anchor-id="模型定义-使用lstm">12.4.2 模型定义 (使用LSTM)</h3>
<p>我们将构建一个包含嵌入层 (Embedding Layer) 和LSTM层的模型。</p>
<div id="code-rnn-imdb-model-define" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Sequential</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Embedding, LSTM, Dense, Dropout</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 设置随机种子</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">32</span> <span class="co"># 词向量的维度</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential([</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. 嵌入层 (Embedding Layer)</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    将整数编码的单词转换为密集向量表示 (词向量)</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    input_dim: 词汇表大小 (max_features)</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    output_dim: 嵌入向量的维度 (embedding_dim)</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    Embedding(input_dim<span class="op">=</span>max_features, output_dim<span class="op">=</span>embedding_dim),</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. LSTM层</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    units: LSTM单元的数量 (输出空间的维度)</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    LSTM(units<span class="op">=</span><span class="dv">32</span>),</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Dropout层 (可选，用于正则化)</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    Dropout(<span class="fl">0.5</span>),</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. 输出层</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    二分类问题，使用Sigmoid激活函数</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    Dense(units<span class="op">=</span><span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="code-rnn-imdb-model-define-1" class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential"</span>
</pre>
</div>
<div id="code-rnn-imdb-model-define-2" class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ embedding (<span style="color: #0087ff; text-decoration-color: #0087ff">Embedding</span>)           │ ?                      │   <span style="color: #00af00; text-decoration-color: #00af00">0</span> (unbuilt) │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>)                     │ ?                      │   <span style="color: #00af00; text-decoration-color: #00af00">0</span> (unbuilt) │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)               │ ?                      │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                   │ ?                      │   <span style="color: #00af00; text-decoration-color: #00af00">0</span> (unbuilt) │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div>
<div id="code-rnn-imdb-model-define-3" class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
<div id="code-rnn-imdb-model-define-4" class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
<div id="code-rnn-imdb-model-define-5" class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
<p><strong>解释：</strong></p>
<ul>
<li><strong><code>Embedding</code> 层：</strong>
<ul>
<li>这是处理文本数据时常用的第一层。它将每个整数索引（代表一个词）映射到一个固定大小的密集向量（词嵌入）。</li>
<li>词嵌入可以学习到单词之间的语义关系（例如，相似的词会有相似的嵌入向量）。</li>
<li><code>input_dim</code>: 词汇表的大小。</li>
<li><code>output_dim</code>: 嵌入向量的维度。</li>
</ul></li>
<li><strong><code>LSTM</code> 层：</strong>
<ul>
<li><code>units</code>: LSTM单元中神经元的数量，也即LSTM层输出的维度。</li>
</ul></li>
<li><strong><code>Dropout</code> 层：</strong> 用于防止过拟合。</li>
<li><strong><code>Dense</code> 输出层：</strong> 对于二分类问题，一个神经元和Sigmoid激活函数。</li>
</ul>
</section>
<section id="模型编译" class="level3">
<h3 class="anchored" data-anchor-id="模型编译">12.4.3 模型编译</h3>
<div id="code-rnn-imdb-model-compile" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span><span class="st">'rmsprop'</span>, <span class="co"># RMSprop通常是RNN的一个不错的选择</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">'accuracy'</span>]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="模型训练" class="level3">
<h3 class="anchored" data-anchor-id="模型训练">12.4.4 模型训练</h3>
<div id="cell-code-rnn-imdb-model-train" class="cell" data-results="hold" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练模型</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    X_train_padded, y_train,</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">20</span>,        <span class="co"># 为了快速演示，只训练20轮</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    validation_split<span class="op">=</span><span class="fl">0.2</span>, <span class="co"># 从训练数据中分出20%作为验证集</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span> <span class="co"># 不显示训练过程</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 绘制学习曲线</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(history.history).plot(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>plt.gca().set_ylim(<span class="dv">0</span>, <span class="dv">1</span>) <span class="co"># 设置y轴范围</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"LSTM Model Training History (IMDb)"</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Epoch"</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.savefig("images/12-rnn/imdb_lstm_training_history.svg")</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="12-rnn_files/figure-html/code-rnn-imdb-model-train-output-1.png" id="code-rnn-imdb-model-train" width="645" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="模型评估" class="level3">
<h3 class="anchored" data-anchor-id="模型评估">12.4.5 模型评估</h3>
<div id="code-rnn-imdb-model-evaluate" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>loss, accuracy <span class="op">=</span> model.evaluate(X_test_padded, y_test, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">测试集损失 (Test Loss): </span><span class="sc">{</span>loss<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"测试集准确率 (Test Accuracy): </span><span class="sc">{</span>accuracy<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
测试集损失 (Test Loss): 0.4386
测试集准确率 (Test Accuracy): 0.8646</code></pre>
</div>
</div>
</section>
</section>
<section id="rnn的应用领域" class="level2">
<h2 class="anchored" data-anchor-id="rnn的应用领域">12.5 RNN的应用领域</h2>
<p>RNN及其变种 (LSTM, GRU) 由于其处理序列数据的强大能力，在许多领域都有广泛应用：</p>
<ul>
<li><strong>自然语言处理 (NLP)：</strong>
<ul>
<li><strong>机器翻译 (Machine Translation)</strong></li>
<li><strong>文本生成 (Text Generation)</strong> (例如，写诗、生成代码)</li>
<li><strong>情感分析 (Sentiment Analysis)</strong></li>
<li><strong>问答系统 (Question Answering)</strong></li>
<li><strong>命名实体识别 (Named Entity Recognition)</strong></li>
<li><strong>语音识别 (Speech Recognition)</strong> (将语音波形转换为文本)</li>
<li><strong>文本摘要 (Text Summarization)</strong></li>
</ul></li>
<li><strong>时间序列分析：</strong>
<ul>
<li>股票价格预测</li>
<li>天气预报</li>
<li>传感器数据分析 (例如，物联网设备数据)</li>
<li>医疗数据分析 (例如，心电图ECG信号)</li>
</ul></li>
<li><strong>其他：</strong>
<ul>
<li>音乐生成</li>
<li>视频分析 (结合CNN)</li>
<li>机器人控制</li>
</ul></li>
</ul>
</section>
<section id="rnn的挑战" class="level2">
<h2 class="anchored" data-anchor-id="rnn的挑战">12.6 RNN的挑战</h2>
<p>尽管LSTM和GRU在很大程度上缓解了标准RNN的梯度消失问题，但RNN的训练仍然存在一些挑战：</p>
<ul>
<li><strong>梯度消失/爆炸：</strong> 虽然有所改善，但在非常长的序列上，这些问题仍可能出现。</li>
<li><strong>计算成本：</strong> RNN的计算本质上是顺序的（每个时间步的计算依赖于前一个时间步），这使得它们难以像CNN那样进行大规模并行化，导致训练时间可能较长。</li>
<li><strong>对超参数敏感：</strong> RNN的性能有时对网络结构、优化器选择、学习率等超参数比较敏感。</li>
</ul>
<p>近年来，像Transformer这样的基于注意力机制的架构在许多NLP任务上表现优于RNN，因为它们能更好地捕捉长距离依赖，并且更容易并行化。然而，RNN及其变种在某些类型的序列数据和资源受限的环境中仍然是重要且有效的工具。</p>
</section>
<section id="本章总结" class="level2">
<h2 class="anchored" data-anchor-id="本章总结">12.7 本章总结</h2>
<p>本章我们学习了循环神经网络 (RNN)，一类专门用于处理序列数据的强大模型：</p>
<ul>
<li><strong>序列数据特点：</strong> 理解了文本、时间序列等数据的顺序依赖性。</li>
<li><strong>RNN核心思想：</strong> 通过隐藏状态在时间步之间传递信息，实现对序列模式的记忆和学习。</li>
<li><strong>标准RNN的局限：</strong> 讨论了梯度消失/爆炸问题，这限制了其捕捉长期依赖的能力。</li>
<li><strong>LSTM与GRU：</strong> 学习了这两种通过门控机制改进RNN的先进架构，它们能更有效地学习长期依赖。
<ul>
<li>LSTM：遗忘门、输入门、输出门、细胞状态。</li>
<li>GRU：更新门、重置门，结构更简化。</li>
</ul></li>
<li><strong>其他RNN变种：</strong> 简要了解了双向RNN和深层RNN的概念。</li>
<li><strong>Keras实践：</strong> 我们使用Keras构建了一个包含嵌入层和LSTM层的模型，用于IMDb电影评论的情感分析。</li>
<li><strong>应用与挑战：</strong> 探讨了RNN在NLP、时间序列等领域的广泛应用，以及它们仍然面临的一些挑战。</li>
</ul>
<p>RNN为处理动态变化的数据提供了一个强大的框架。虽然新的架构如Transformer正在崛起，但理解RNN的原理对于深入学习序列建模仍然至关重要。</p>
</section>
<section id="思考与练习" class="level2">
<h2 class="anchored" data-anchor-id="思考与练习">12.8 思考与练习</h2>
<section id="基础概念回顾" class="level3">
<h3 class="anchored" data-anchor-id="基础概念回顾">12.8.1 基础概念回顾</h3>
<ol type="1">
<li><strong>序列数据与RNN：</strong>
<ul>
<li>什么是序列数据？请举例说明。</li>
<li>为什么说传统的前馈神经网络不适合直接处理可变长度的序列数据？</li>
<li>RNN的核心思想是什么？隐藏状态在RNN中扮演什么角色？</li>
</ul></li>
<li><strong>RNN的挑战与改进：</strong>
<ul>
<li>标准RNN（SimpleRNN）在处理长序列时主要面临什么问题？简述梯度消失和梯度爆炸。</li>
<li>LSTM是如何尝试解决梯度消失问题的？简述其三个主要门（遗忘门、输入门、输出门）和细胞状态的作用。</li>
<li>GRU与LSTM相比有什么主要区别和相似之处？</li>
</ul></li>
<li><strong>RNN架构与应用：</strong>
<ul>
<li>解释词嵌入 (Word Embedding) 在处理文本数据时的作用。</li>
<li>双向RNN相比单向RNN有什么优势？在什么类型的任务中它可能更有效？</li>
<li>列举至少三个RNN（或其变种）的典型应用场景。</li>
</ul></li>
</ol>
</section>
<section id="keras实践与探索" class="level3">
<h3 class="anchored" data-anchor-id="keras实践与探索">12.8.2 Keras实践与探索</h3>
<p><strong>项目目标：</strong> 进一步实践RNN、LSTM、GRU的构建，探索不同参数和模型对序列数据处理任务的影响。</p>
<p><strong>任务步骤：</strong></p>
<ol type="1">
<li><strong>IMDb情感分析实验扩展：</strong>
<ul>
<li><strong>尝试SimpleRNN和GRU：</strong> 在本章的IMDb示例代码基础上，将 <code>LSTM</code> 层替换为 <code>SimpleRNN</code> 层和 <code>GRU</code> 层。比较它们的训练速度、最终验证集/测试集准确率。观察 <code>SimpleRNN</code> 是否更容易出现性能瓶颈。</li>
<li><strong>调整LSTM/GRU单元数量：</strong> 改变 <code>LSTM</code> 或 <code>GRU</code> 层中的 <code>units</code> 参数（例如，尝试16, 64, 128），观察对模型性能和训练时间的影响。</li>
<li><strong>使用双向LSTM/GRU：</strong> 将 <code>LSTM</code> 或 <code>GRU</code> 层用 <code>Bidirectional</code> 包装器包裹起来（例如，<code>Bidirectional(LSTM(32))</code>）。比较其与单向版本的性能差异。</li>
<li><strong>堆叠RNN层：</strong> 尝试构建一个包含多个LSTM或GRU层的深层RNN（例如，两个堆叠的LSTM层）。注意，当一个RNN层返回完整的序列（而不仅仅是最后一个时间步的输出）给下一个RNN层时，需要设置 <code>return_sequences=True</code>（除了最后一层RNN）。 <code>python     # 示例：堆叠LSTM     # model.add(LSTM(32, return_sequences=True))     # model.add(LSTM(32))</code></li>
<li><strong>改变嵌入维度：</strong> 调整 <code>Embedding</code> 层中的 <code>output_dim</code> (词向量维度)，观察其影响。</li>
</ul></li>
<li><strong>(选做) 简单时间序列预测：</strong>
<ul>
<li><strong>生成数据：</strong> 生成一个简单的正弦波或带有趋势和季节性的时间序列数据。 <code>python     # 示例：生成正弦波数据     # import numpy as np     # series = np.sin(0.1 * np.arange(1000)) + np.random.randn(1000) * 0.1</code></li>
<li><strong>数据预处理：</strong> 将时间序列数据转换为监督学习问题。例如，使用前 <code>n</code> 个时间步的数据作为输入，预测第 <code>n+1</code> 个时间步的值。你需要创建输入序列 (X) 和目标序列 (y)。</li>
<li><strong>构建模型：</strong> 构建一个简单的RNN、LSTM或GRU模型进行预测。你可能需要一个 <code>Dense</code> 输出层，其激活函数为线性（或不使用激活函数）。</li>
<li><strong>训练与评估：</strong> 训练模型并评估其在预测未见过的时间序列数据上的性能（例如，使用均方误差MSE作为损失函数和评估指标）。可视化预测结果与真实值的对比。</li>
</ul></li>
</ol>
</section>
<section id="深入思考与挑战" class="level3">
<h3 class="anchored" data-anchor-id="深入思考与挑战">12.8.3 深入思考与挑战</h3>
<ol type="1">
<li><strong>BPTT的计算：</strong> 为什么沿时间反向传播 (BPTT) 对于RNN来说计算成本可能较高，尤其是对于长序列？</li>
<li><strong>门控机制的直观理解：</strong> 尝试用自己的话更直观地解释LSTM中的遗忘门、输入门和输出门是如何协同工作以控制信息流的。</li>
<li><strong>注意力机制 (Attention Mechanism) 初探：</strong> RNN（尤其是结合了注意力机制的RNN）在机器翻译等任务中取得了巨大成功。简单了解一下什么是注意力机制，它是如何帮助RNN更好地处理长序列和对齐输入输出的？（这可以作为后续学习Transformer架构的铺垫）</li>
<li><strong>何时选择RNN vs CNN vs MLP？</strong> 对于一个给定的机器学习问题，你将如何判断应该优先考虑使用RNN、CNN还是MLP？考虑数据的类型和结构。</li>
</ol>
</section>
<section id="推荐阅读" class="level3">
<h3 class="anchored" data-anchor-id="推荐阅读">12.8.4 推荐阅读</h3>
<ol type="1">
<li><strong>Chollet, F. (2021). <em>Deep Learning with Python</em> (2nd ed.). Manning Publications. (Chapter 9: Working with sequence data)</strong> - 详细介绍了使用Keras处理序列数据，包括RNN、LSTM、GRU的实践。</li>
<li><strong>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <em>Deep Learning</em>. MIT Press. (Chapter 10: Sequence Modeling: Recurrent and Recursive Nets)</strong> - 对RNN及其理论的深入讨论。</li>
<li><strong>Olah, C. (2015). <em>Understanding LSTM Networks</em>. Colah’s Blog.</strong> - 一篇非常经典且直观易懂的解释LSTM的文章：<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li>
<li><strong>Karpathy, A. (2015). <em>The Unreasonable Effectiveness of Recurrent Neural Networks</em>. Andrej Karpathy blog.</strong> - 展示了RNN在字符级语言模型上的惊人能力：<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></li>
<li><strong>TensorFlow官方教程 - 文本处理与序列模型：</strong>
<ul>
<li>Text classification with an RNN: <a href="https://www.tensorflow.org/text/tutorials/text_classification_rnn">https://www.tensorflow.org/text/tutorials/text_classification_rnn</a></li>
<li>Time series forecasting: <a href="https://www.tensorflow.org/tutorials/structured_data/time_series">https://www.tensorflow.org/tutorials/structured_data/time_series</a></li>
</ul></li>
</ol>
<hr>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./11-cnn.html" class="pagination-link" aria-label="卷积神经网络 (CNN)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">卷积神经网络 (CNN)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./13-deep-learning-advanced.html" class="pagination-link" aria-label="深度学习进阶">
        <span class="nav-page-text"><span class="chapter-title">深度学习进阶</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>