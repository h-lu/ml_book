<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>决策树与集成学习 – 机器学习：从理论到Python实践</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./07-clustering.html" rel="next">
<link href="./05-svm.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03-regression.html">第二部分：监督学习</a></li><li class="breadcrumb-item"><a href="./06-decision-trees-ensemble-learning.html"><span class="chapter-title">决策树与集成学习</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">机器学习：从理论到Python实践</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">欢迎学习《机器学习：从理论到Python实践》</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">第一部分：机器学习基石与Python生态</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">机器学习导论</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-environment.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Python机器学习环境与核心库</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">第二部分：监督学习</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">回归与线性模型</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-classification-logreg-knn.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">分类与逻辑回归、KNN</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-svm.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">支持向量机 (SVM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-decision-trees-ensemble-learning.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">决策树与集成学习</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">第三部分：无监督学习</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-clustering.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">聚类分析</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-dimensionality-reduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">降维</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">第四部分：模型评估、优化与特征工程</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-model-evaluation-feature-engineering.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">模型评估、优化与特征工程</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">第五部分：深度学习初探</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-deep-learning-basics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">深度学习基础</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-cnn.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">卷积神经网络 (CNN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-rnn.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">循环神经网络 (RNN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-deep-learning-advanced.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">深度学习进阶</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">第六部分：强化学习入门</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-reinforcement-learning.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">强化学习基础与应用</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">第七部分：综合项目与展望</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-ml-project-workflow-summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">机器学习项目实战流程与总结</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-summary-outlook.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">课程总结与展望</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendices.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">附录</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#学习目标" id="toc-学习目标" class="nav-link active" data-scroll-target="#学习目标">学习目标</a></li>
  <li><a href="#决策树简介" id="toc-决策树简介" class="nav-link" data-scroll-target="#决策树简介">6.1 决策树简介</a></li>
  <li><a href="#决策树的构建与分裂标准" id="toc-决策树的构建与分裂标准" class="nav-link" data-scroll-target="#决策树的构建与分裂标准">6.2 决策树的构建与分裂标准</a>
  <ul class="collapse">
  <li><a href="#衡量标准的思想纯度提升" id="toc-衡量标准的思想纯度提升" class="nav-link" data-scroll-target="#衡量标准的思想纯度提升">6.2.1 衡量标准的思想：纯度提升</a></li>
  <li><a href="#信息熵-entropy-与信息增益-information-gain" id="toc-信息熵-entropy-与信息增益-information-gain" class="nav-link" data-scroll-target="#信息熵-entropy-与信息增益-information-gain">6.2.2 信息熵 (Entropy) 与信息增益 (Information Gain)</a></li>
  <li><a href="#基尼不纯度-gini-impurity-与基尼指数" id="toc-基尼不纯度-gini-impurity-与基尼指数" class="nav-link" data-scroll-target="#基尼不纯度-gini-impurity-与基尼指数">6.2.3 基尼不纯度 (Gini Impurity) 与基尼指数</a></li>
  <li><a href="#处理连续值特征" id="toc-处理连续值特征" class="nav-link" data-scroll-target="#处理连续值特征">6.2.4 处理连续值特征</a></li>
  <li><a href="#回归树" id="toc-回归树" class="nav-link" data-scroll-target="#回归树">6.2.5 回归树</a></li>
  </ul></li>
  <li><a href="#决策树的剪枝-pruning" id="toc-决策树的剪枝-pruning" class="nav-link" data-scroll-target="#决策树的剪枝-pruning">6.3 决策树的剪枝 (Pruning)</a></li>
  <li><a href="#在-scikit-learn-中使用决策树" id="toc-在-scikit-learn-中使用决策树" class="nav-link" data-scroll-target="#在-scikit-learn-中使用决策树">6.4 在 Scikit-learn 中使用决策树</a>
  <ul class="collapse">
  <li><a href="#分类决策树-decisiontreeclassifier" id="toc-分类决策树-decisiontreeclassifier" class="nav-link" data-scroll-target="#分类决策树-decisiontreeclassifier">6.4.1 分类决策树 (<code>DecisionTreeClassifier</code>)</a></li>
  <li><a href="#回归决策树-decisiontreeregressor" id="toc-回归决策树-decisiontreeregressor" class="nav-link" data-scroll-target="#回归决策树-decisiontreeregressor">6.4.2 回归决策树 (<code>DecisionTreeRegressor</code>)</a></li>
  </ul></li>
  <li><a href="#集成学习简介-ensemble-learning" id="toc-集成学习简介-ensemble-learning" class="nav-link" data-scroll-target="#集成学习简介-ensemble-learning">6.5 集成学习简介 (Ensemble Learning)</a></li>
  <li><a href="#bagging" id="toc-bagging" class="nav-link" data-scroll-target="#bagging">6.6 Bagging</a>
  <ul class="collapse">
  <li><a href="#bagging-的工作原理" id="toc-bagging-的工作原理" class="nav-link" data-scroll-target="#bagging-的工作原理">6.6.1 Bagging 的工作原理</a></li>
  <li><a href="#随机森林-random-forest" id="toc-随机森林-random-forest" class="nav-link" data-scroll-target="#随机森林-random-forest">6.6.2 随机森林 (Random Forest)</a></li>
  <li><a href="#在-scikit-learn-中使用-bagging-和随机森林" id="toc-在-scikit-learn-中使用-bagging-和随机森林" class="nav-link" data-scroll-target="#在-scikit-learn-中使用-bagging-和随机森林">6.6.3 在 Scikit-learn 中使用 Bagging 和随机森林</a></li>
  <li><a href="#随机森林的特征重要性-feature-importances" id="toc-随机森林的特征重要性-feature-importances" class="nav-link" data-scroll-target="#随机森林的特征重要性-feature-importances">6.6.4 随机森林的特征重要性 (Feature Importances)</a></li>
  <li><a href="#随机森林的参数调整建议" id="toc-随机森林的参数调整建议" class="nav-link" data-scroll-target="#随机森林的参数调整建议">6.6.5 随机森林的参数调整建议</a></li>
  </ul></li>
  <li><a href="#boosting" id="toc-boosting" class="nav-link" data-scroll-target="#boosting">6.7 Boosting</a>
  <ul class="collapse">
  <li><a href="#adaboost-adaptive-boosting" id="toc-adaboost-adaptive-boosting" class="nav-link" data-scroll-target="#adaboost-adaptive-boosting">6.7.1 AdaBoost (Adaptive Boosting)</a></li>
  <li><a href="#梯度提升树-gradient-boosting-trees-gbtgbm" id="toc-梯度提升树-gradient-boosting-trees-gbtgbm" class="nav-link" data-scroll-target="#梯度提升树-gradient-boosting-trees-gbtgbm">6.7.2 梯度提升树 (Gradient Boosting Trees, GBT/GBM)</a></li>
  <li><a href="#xgboost-extreme-gradient-boosting-简介" id="toc-xgboost-extreme-gradient-boosting-简介" class="nav-link" data-scroll-target="#xgboost-extreme-gradient-boosting-简介">6.7.3 XGBoost (Extreme Gradient Boosting) 简介</a></li>
  </ul></li>
  <li><a href="#本章总结" id="toc-本章总结" class="nav-link" data-scroll-target="#本章总结">6.8 本章总结</a></li>
  <li><a href="#思考与练习" id="toc-思考与练习" class="nav-link" data-scroll-target="#思考与练习">6.9 思考与练习</a>
  <ul class="collapse">
  <li><a href="#基础练习" id="toc-基础练习" class="nav-link" data-scroll-target="#基础练习">6.9.1 基础练习</a></li>
  <li><a href="#编码与实践" id="toc-编码与实践" class="nav-link" data-scroll-target="#编码与实践">6.9.2 编码与实践</a></li>
  <li><a href="#推荐阅读" id="toc-推荐阅读" class="nav-link" data-scroll-target="#推荐阅读">6.9.3 推荐阅读</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03-regression.html">第二部分：监督学习</a></li><li class="breadcrumb-item"><a href="./06-decision-trees-ensemble-learning.html"><span class="chapter-title">决策树与集成学习</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">决策树与集成学习</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="学习目标" class="level2">
<h2 class="anchored" data-anchor-id="学习目标">学习目标</h2>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>学习目标：</strong></p>
<ul>
<li>理解决策树的基本原理，包括节点分裂标准（如信息熵、基尼指数）和剪枝策略。</li>
<li>能够使用Scikit-learn构建、训练和可视化决策树模型。</li>
<li>理解集成学习的基本思想及其优势。</li>
<li>掌握Bagging方法的原理，特别是随机森林 (Random Forest) 的构建过程和特点。</li>
<li>掌握Boosting方法的基本原理，理解AdaBoost和梯度提升树 (Gradient Boosting, GBDT) 的核心思想。</li>
<li>能够使用Scikit-learn实现随机森林、AdaBoost和梯度提升模型，并进行参数调整和评估。</li>
<li>了解XGBoost等先进Boosting算法的简介。</li>
</ul>
</div>
</div>
</div>
</section>
<section id="决策树简介" class="level2">
<h2 class="anchored" data-anchor-id="决策树简介">6.1 决策树简介</h2>
<p>决策树是一种直观的监督学习模型，它通过学习数据中的简单决策规则来预测目标变量的值。树的结构类似于流程图，其中每个内部节点表示对一个属性的测试，每个分支代表一个测试输出，每个叶节点（或终端节点）代表一个类别标签（分类树）或一个数值（回归树）。从根节点到叶节点的路径对应一个决策规则。</p>
<p><strong>决策树的优点：</strong></p>
<ul>
<li><strong>易于理解和解释：</strong> 决策树的结构可以被可视化，使得模型的决策过程非常直观。</li>
<li><strong>对数据预处理要求较低：</strong> 不需要进行特征归一化或标准化（尽管对于某些实现可能有益）。</li>
<li><strong>能够处理数值型和类别型数据：</strong> Scikit-learn的实现目前主要支持数值型特征，类别型特征需要预处理。</li>
<li><strong>能够处理多输出问题。</strong></li>
<li><strong>计算成本相对较低：</strong> 预测阶段非常快。</li>
</ul>
<p><strong>决策树的缺点：</strong></p>
<ul>
<li><strong>容易过拟合：</strong> 决策树倾向于生成过于复杂的树，完美拟合训练数据，但在未见过的数据上泛化能力差。剪枝是解决此问题的重要手段。</li>
<li><strong>不稳定性：</strong> 数据中的微小变动可能会导致生成完全不同的树。这个问题可以通过集成学习来缓解。</li>
<li><strong>对于某些复杂关系（如异或问题），决策树可能效率不高，需要较深的树。</strong></li>
<li><strong>贪心算法：</strong> 决策树的构建通常采用贪心算法，在每个节点选择局部最优的分裂，这不一定能保证全局最优。</li>
</ul>
<p>接下来，我们将深入探讨决策树如何构建，以及如何选择最佳的分裂点。</p>
</section>
<section id="决策树的构建与分裂标准" class="level2">
<h2 class="anchored" data-anchor-id="决策树的构建与分裂标准">6.2 决策树的构建与分裂标准</h2>
<p>决策树的构建是一个递归的过程，也称为<strong>递归划分 (Recursive Partitioning)</strong>。从包含所有训练样本的根节点开始，算法会尝试所有可能的特征和分裂点，选择一个能够最好地分离样本的特征和阈值（对于数值型特征）或子集（对于类别型特征），将当前节点划分为更纯的子节点。这个过程会持续进行，直到满足某个停止条件，例如：</p>
<ul>
<li>节点中的所有样本都属于同一类别（节点已经”纯净”）。</li>
<li>没有更多的特征可供分裂。</li>
<li>树达到预设的最大深度。</li>
<li>节点中的样本数量少于预设的最小阈值。</li>
<li>进一步分裂不能带来显著的”纯度”提升。</li>
</ul>
<section id="衡量标准的思想纯度提升" class="level3">
<h3 class="anchored" data-anchor-id="衡量标准的思想纯度提升">6.2.1 衡量标准的思想：纯度提升</h3>
<p>“最好地分离样本”通常意味着分裂后的子节点比父节点更”纯净”。纯度指的是节点中样本类别的一致性程度。如果一个节点中的所有样本都属于同一个类别，那么它是完全纯净的。相反，如果一个节点中的样本均匀地分布在所有类别中，那么它是最不纯净的。</p>
<p>决策树算法在选择分裂特征和分裂点时，会评估每个可能的分裂所带来的<strong>纯度提升 (Purity Gain)</strong> 或<strong>不纯度减少 (Impurity Reduction)</strong>。目标是找到那个能够最大化这种提升的分裂。</p>
<p>常用的衡量节点不纯度的指标主要有两种：<strong>信息熵 (Entropy)</strong> 和 <strong>基尼不纯度 (Gini Impurity)</strong>。</p>
</section>
<section id="信息熵-entropy-与信息增益-information-gain" class="level3">
<h3 class="anchored" data-anchor-id="信息熵-entropy-与信息增益-information-gain">6.2.2 信息熵 (Entropy) 与信息增益 (Information Gain)</h3>
<p>在信息论中，<strong>熵 (Entropy)</strong> 是对随机变量不确定性的度量。熵越大，表示数据的不确定性越高，纯度越低。</p>
<p>对于一个包含 <span class="math inline">\(K\)</span> 个类别的数据集 <span class="math inline">\(D\)</span>，其中第 <span class="math inline">\(k\)</span> 类样本所占的比例为 <span class="math inline">\(p_k\)</span>（<span class="math inline">\(k=1, 2, ..., K\)</span>），则数据集 <span class="math inline">\(D\)</span> 的信息熵定义为：</p>
<p><span class="math display">\[ \text{Ent}(D) = - \sum_{k=1}^{K} p_k \log_2(p_k) \]</span></p>
<p>其中，约定如果 <span class="math inline">\(p_k = 0\)</span>，则 <span class="math inline">\(p_k \log_2(p_k) = 0\)</span>。</p>
<ul>
<li>当节点完全纯净时（所有样本属于同一类别，<span class="math inline">\(p_k=1\)</span> for some <span class="math inline">\(k\)</span>, and <span class="math inline">\(p_j=0\)</span> for <span class="math inline">\(j \neq k\)</span>），熵为0。</li>
<li>当节点中各类样本均匀分布时（例如，二分类问题中，<span class="math inline">\(p_1=p_2=0.5\)</span>），熵达到最大值。</li>
</ul>
<p><strong>信息增益 (Information Gain)</strong></p>
<p>假设我们使用特征 <span class="math inline">\(A\)</span> 对数据集 <span class="math inline">\(D\)</span> 进行分裂，特征 <span class="math inline">\(A\)</span> 有 <span class="math inline">\(V\)</span> 个可能的取值 <span class="math inline">\(\{a^1, a^2, ..., a^V\}\)</span>。使用特征 <span class="math inline">\(A\)</span> 对 <span class="math inline">\(D\)</span> 进行分裂会产生 <span class="math inline">\(V\)</span> 个分支（子节点），其中第 <span class="math inline">\(v\)</span> 个分支 <span class="math inline">\(D^v\)</span> 包含了 <span class="math inline">\(D\)</span> 中在特征 <span class="math inline">\(A\)</span> 上取值为 <span class="math inline">\(a^v\)</span> 的样本。我们可以计算每个子节点 <span class="math inline">\(D^v\)</span> 的信息熵 <span class="math inline">\(\text{Ent}(D^v)\)</span>。</p>
<p>信息增益定义为父节点的信息熵与所有子节点信息熵的加权平均之差：</p>
<p><span class="math display">\[ \text{Gain}(D, A) = \text{Ent}(D) - \sum_{v=1}^{V} \frac{|D^v|}{|D|} \text{Ent}(D^v) \]</span></p>
<p>其中，<span class="math inline">\(\frac{|D^v|}{|D|}\)</span> 是子节点 <span class="math inline">\(D^v\)</span> 中样本数量占父节点 <span class="math inline">\(D\)</span> 中样本数量的比例，作为权重。</p>
<p>决策树算法（如ID3算法[Quinlan, 1986]）会选择具有<strong>最大信息增益</strong>的特征来进行分裂。</p>
<p><strong>信息增益的局限性</strong></p>
<p>信息增益在选择分裂特征时存在一个潜在问题：它倾向于偏好那些具有较多取值的特征。例如：</p>
<ul>
<li>如果使用样本ID作为特征，每个ID值都对应一个唯一的样本，分裂后会得到完全纯净的子节点</li>
<li>这种情况下信息增益会非常高，但这种特征实际上毫无泛化能力</li>
</ul>
<p><strong>改进方法：信息增益率</strong></p>
<p>C4.5算法(Quinlan, 1993)通过引入<strong>信息增益率(Information Gain Ratio)</strong>来解决这个问题：</p>
<p><span class="math display">\[ \text{GainRatio}(D,A) = \frac{\text{Gain}(D,A)}{\text{IV}(A)} \]</span></p>
<p>其中<span class="math inline">\(\text{IV}(A)\)</span>是特征<span class="math inline">\(A\)</span>的固有值(Intrinsic Value)，作为对多值特征的惩罚项：</p>
<p><span class="math display">\[ \text{IV}(A) = -\sum_{v=1}^{V} \frac{|D^v|}{|D|} \log_2 \frac{|D^v|}{|D|} \]</span></p>
<p>这种方法能有效避免选择那些取值过多但无实际意义的特征。</p>
</section>
<section id="基尼不纯度-gini-impurity-与基尼指数" class="level3">
<h3 class="anchored" data-anchor-id="基尼不纯度-gini-impurity-与基尼指数">6.2.3 基尼不纯度 (Gini Impurity) 与基尼指数</h3>
<p><strong>基尼不纯度 (Gini Impurity)</strong> 是另一种衡量数据不确定性或纯度的指标。它表示从数据集中随机抽取两个样本，其类别标签不一致的概率。基尼不纯度越小，数据集的纯度越高。</p>
<p>对于一个包含 <span class="math inline">\(K\)</span> 个类别的数据集 <span class="math inline">\(D\)</span>，其中第 <span class="math inline">\(k\)</span> 类样本所占的比例为 <span class="math inline">\(p_k\)</span>，则数据集 <span class="math inline">\(D\)</span> 的基尼不纯度定义为：</p>
<p><span class="math display">\[ \text{Gini}(D) = 1 - \sum_{k=1}^{K} p_k^2 = \sum_{k \neq j} p_k p_j \]</span></p>
<ul>
<li>当节点完全纯净时（<span class="math inline">\(p_k=1\)</span> for some <span class="math inline">\(k\)</span>），<span class="math inline">\(\text{Gini}(D) = 1 - 1^2 = 0\)</span>。</li>
<li>当节点中各类样本均匀分布时（例如，二分类问题中，<span class="math inline">\(p_1=p_2=0.5\)</span>），<span class="math inline">\(\text{Gini}(D) = 1 - (0.5^2 + 0.5^2) = 0.5\)</span>。</li>
</ul>
<p><strong>基尼指数 (Gini Index)</strong></p>
<p>当使用特征 <span class="math inline">\(A\)</span> 对数据集 <span class="math inline">\(D\)</span> 进行分裂时，我们会计算分裂后的子节点的基尼不纯度。如果特征 <span class="math inline">\(A\)</span> 将数据集 <span class="math inline">\(D\)</span> 分裂为两个子集 <span class="math inline">\(D_1\)</span> 和 <span class="math inline">\(D_2\)</span>（例如，对于二元分裂），则分裂后的基尼指数定义为子节点基尼不纯度的加权平均：</p>
<p><span class="math display">\[ \text{GiniIndex}(D, A) = \frac{|D_1|}{|D|} \text{Gini}(D_1) + \frac{|D_2|}{|D|} \text{Gini}(D_2) \]</span></p>
<p>决策树算法（如CART算法[Breiman et al., 1984]，Classification and Regression Trees）会选择那个使得分裂后<strong>基尼指数最小</strong>的特征和分裂点。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
基尼不纯度 vs.&nbsp;信息熵
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>计算效率</strong><br>
基尼不纯度的计算通常比信息熵略快，因为它不涉及对数运算。</p></li>
<li><p><strong>效果差异</strong><br>
在实践中，两者通常会产生非常相似的树。基尼不纯度倾向于将最大的类别孤立出来，而信息熵则更倾向于产生更平衡的树。</p></li>
<li><p><strong>Scikit-learn实现</strong><br>
<code>DecisionTreeClassifier</code> 默认使用基尼不纯度 (<code>criterion='gini'</code>)，但也可以设置为使用信息熵 (<code>criterion='entropy'</code>)。</p></li>
</ul>
</div>
</div>
<p>下图直观地比较了二分类问题中，类别1的概率 <span class="math inline">\(p_1\)</span> 从0到1变化时，信息熵和基尼不纯度的变化情况。</p>
<p><img src="./images/06-decision-trees-ensemble-learning/impurity_measures.svg" class="img-fluid"> <em>(图 6.1: 二分类问题中信息熵与基尼不纯度随类别概率p的变化曲线。横轴p表示类别1的概率，纵轴表示不纯度度量。)</em></p>
</section>
<section id="处理连续值特征" class="level3">
<h3 class="anchored" data-anchor-id="处理连续值特征">6.2.4 处理连续值特征</h3>
<p>对于连续值（数值型）特征，决策树算法通常采用二分法来寻找最佳分裂点。具体做法是：</p>
<ol type="1">
<li>将该特征的所有取值进行排序。</li>
<li>遍历所有可能的<strong>分裂点</strong>。一个常见策略是选择相邻两个排序后的特征值的中点作为潜在分裂点。</li>
<li>对每个潜在分裂点，计算分裂后的纯度提升（如信息增益或基尼指数减小量）。</li>
<li>选择那个能够带来最大纯度提升的分裂点作为该特征的最佳分裂点。</li>
</ol>
<p>例如，如果某连续特征的取值有 <code>[10, 20, 30, 40]</code>，则潜在的分裂点可以是 <code>15, 25, 35</code>。</p>
</section>
<section id="回归树" class="level3">
<h3 class="anchored" data-anchor-id="回归树">6.2.5 回归树</h3>
<p>决策树不仅可以用于分类任务，也可以用于回归任务（预测连续值）。对于回归树：</p>
<ul>
<li><strong>分裂标准：</strong> 不再使用信息熵或基尼不纯度，而是使用能够最小化子节点预测值与真实值之间<strong>均方误差 (Mean Squared Error, MSE)</strong> 或其他回归损失函数（如平均绝对误差 MAE）的分裂。 假设一个节点 <span class="math inline">\(R_m\)</span> 包含的样本的目标值为 <span class="math inline">\(y^{(i)}\)</span>，该节点的预测值为 <span class="math inline">\(\hat{y}_m\)</span>（通常是该节点所有样本目标值的均值）。分裂标准是选择特征和分裂点，使得分裂后的子节点 <span class="math inline">\(R_1, R_2\)</span> 的MSE之和最小： <span class="math display">\[ \text{argmin} \left( \sum_{i \in R_1} (y^{(i)} - \hat{y}_{R_1})^2 + \sum_{i \in R_2} (y^{(i)} - \hat{y}_{R_2})^2 \right) \]</span></li>
<li><strong>叶节点预测：</strong> 叶节点的预测值通常是该叶节点内所有训练样本目标值的平均值。</li>
</ul>
<p>Scikit-learn 提供了 <code>DecisionTreeRegressor</code> 类用于回归任务。</p>
</section>
</section>
<section id="决策树的剪枝-pruning" class="level2">
<h2 class="anchored" data-anchor-id="决策树的剪枝-pruning">6.3 决策树的剪枝 (Pruning)</h2>
<p>如前所述，决策树非常容易过拟合训练数据，生成一个庞大而复杂的树，其在训练集上表现完美，但在未见过的测试数据上性能不佳。为了解决这个问题，需要对决策树进行<strong>剪枝 (Pruning)</strong>。</p>
<p>剪枝的目的是简化决策树，牺牲一些在训练集上的准确性来换取在未知数据上更好的泛化能力。主要有两种剪枝策略：</p>
<ol type="1">
<li><p><strong>预剪枝 (Pre-pruning / Early Stopping):</strong> 在决策树完全构建<strong>之前</strong>就停止树的生长。具体做法是在每个节点进行分裂前，先评估当前分裂是否能够带来泛化性能的提升（例如，通过在验证集上测试，或者检查纯度提升是否达到某个阈值）。如果分裂不能带来预期的提升，则停止分裂，并将当前节点标记为叶节点。</p>
<ul>
<li><strong>优点：</strong> 预剪枝使得决策树的很多分支都没有展开，这不仅降低了过拟合的风险，而且显著减少了树的训练时间和存储开销。</li>
<li><strong>缺点：</strong> 预剪枝是一种”贪心”的策略。有些分裂可能在当前看起来不能带来提升，但基于其后续的分裂却可能导致性能的显著提高。预剪枝可能会过早地停止树的生长，导致”欠拟合”的风险。</li>
</ul>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>常见的预剪枝停止条件（Scikit-learn <code>DecisionTreeClassifier</code> 参数）</strong></p>
<ul>
<li><strong><code>max_depth</code></strong>: 树的最大深度，限制树的生长层数</li>
<li><strong><code>min_samples_split</code></strong>: 节点可分裂的最小样本数，小于此值则停止分裂</li>
<li><strong><code>min_samples_leaf</code></strong>: 叶节点必须包含的最小样本数，防止生成过小的叶节点<br>
</li>
<li><strong><code>min_impurity_decrease</code></strong>: 分裂需达到的最小不纯度减少量，过滤无效分裂</li>
<li><strong><code>max_leaf_nodes</code></strong>: 限制叶节点总数，控制模型复杂度</li>
</ul>
</div>
</div>
</div></li>
<li><p><strong>后剪枝 (Post-pruning):</strong> 先让决策树充分生长，构建一个完整的、可能过拟合的树。然后，自底向上地考察树中的非叶节点，尝试将其替换为叶节点（即剪掉其子树）。如果将该节点替换为叶节点后，在验证集上的性能有所提升或保持不变，则执行剪枝。</p>
<ul>
<li><strong>优点：</strong> 后剪枝通常能产生比预剪枝泛化性能更好的决策树，因为它是在树完全生长后，基于更全局的信息进行判断，不容易出现欠拟合。</li>
<li><strong>缺点：</strong> 后剪枝过程需要在完整树生成之后进行，因此训练时间开销比预剪枝要大。</li>
</ul>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Scikit-learn中的剪枝实现</strong></p>
<ul>
<li><strong>预剪枝</strong>：<code>DecisionTreeClassifier</code> 主要通过参数实现预剪枝</li>
<li><strong>后剪枝</strong>：通常需要用户自行在验证集上实现，或使用支持后剪枝的高级库</li>
<li><strong>代价复杂度剪枝(CCP)</strong>：从0.22版本开始支持
<ul>
<li>通过 <code>ccp_alpha</code> 参数控制</li>
<li>当 <code>ccp_alpha</code> &gt; 0 时，会剪除”最弱连接”（移除后不纯度增加最小的子树）</li>
<li>建议使用交叉验证选择最优的 <code>ccp_alpha</code> 值</li>
</ul></li>
</ul>
</div>
</div>
</div></li>
</ol>
</section>
<section id="在-scikit-learn-中使用决策树" class="level2">
<h2 class="anchored" data-anchor-id="在-scikit-learn-中使用决策树">6.4 在 Scikit-learn 中使用决策树</h2>
<p>Scikit-learn 提供了 <code>DecisionTreeClassifier</code> 用于分类任务，以及 <code>DecisionTreeRegressor</code> 用于回归任务。它们的使用方式与其他Scikit-learn模型类似。</p>
<section id="分类决策树-decisiontreeclassifier" class="level3">
<h3 class="anchored" data-anchor-id="分类决策树-decisiontreeclassifier">6.4.1 分类决策树 (<code>DecisionTreeClassifier</code>)</h3>
<p>我们将使用鸢尾花数据集来演示如何训练和可视化一个分类决策树。</p>
<div id="29a0d82d" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier, export_graphviz, plot_tree</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> graphviz <span class="co"># 需要安装graphviz库和对应的系统包</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载数据</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> load_iris()</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>X_iris <span class="op">=</span> iris.data</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>y_iris <span class="op">=</span> iris.target</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>feature_names_iris <span class="op">=</span> iris.feature_names</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>class_names_iris <span class="op">=</span> iris.target_names</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 划分训练集和测试集</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>X_train_iris, X_test_iris, y_train_iris, y_test_iris <span class="op">=</span> train_test_split(X_iris, y_iris, random_state<span class="op">=</span><span class="dv">42</span>, test_size<span class="op">=</span><span class="fl">0.3</span>, stratify<span class="op">=</span>y_iris)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建决策树分类器实例 (默认使用gini不纯度)</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 我们可以设置一些预剪枝参数，例如 max_depth</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>clf_dt_iris <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>, max_depth<span class="op">=</span><span class="dv">3</span>) <span class="co"># 限制最大深度为3层</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>clf_dt_iris.fit(X_train_iris, y_train_iris)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># 预测</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>y_pred_iris <span class="op">=</span> clf_dt_iris.predict(X_test_iris)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>accuracy_iris <span class="op">=</span> accuracy_score(y_test_iris, y_pred_iris)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"鸢尾花数据集决策树准确率 (max_depth=3): </span><span class="sc">{</span>accuracy_iris<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 可视化决策树</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># 方法一：使用 plot_tree (推荐，直接在matplotlib中绘制)</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plot_tree(clf_dt_iris, </span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>          filled<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>          rounded<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>          class_names<span class="op">=</span>class_names_iris, </span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>          feature_names<span class="op">=</span>feature_names_iris)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Decision Tree for Iris Dataset (max_depth=3) using plot_tree"</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># 保存图像到指定目录</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>image_path_plot_tree <span class="op">=</span> <span class="st">"images/06-decision-trees-ensemble-learning/iris_decision_tree_plot_tree.svg"</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>plt.savefig(image_path_plot_tree, <span class="bu">format</span><span class="op">=</span><span class="st">'svg'</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>plt.close() <span class="co"># 关闭图像，防止重复显示</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>鸢尾花数据集决策树准确率 (max_depth=3): 0.9778</code></pre>
</div>
</div>
<p><img src="./images/06-decision-trees-ensemble-learning/iris_decision_tree_plot_tree.svg" class="img-fluid"> <em>(图 6.2: 在鸢尾花数据集上训练的决策树 (max_depth=3)。颜色表示类别，gini表示该节点的基尼不纯度，samples表示落入该节点的样本数，value表示各类别样本数分布。)</em></p>
<p><strong>主要参数解读 (<code>DecisionTreeClassifier</code>)</strong>：</p>
<ul>
<li><code>criterion</code>: string, “gini” (默认) 或 “entropy”。衡量分裂质量的函数。</li>
<li><code>splitter</code>: string, “best” (默认) 或 “random”。选择分裂的策略。“best”会尝试所有特征的所有可能分裂点；“random”会随机选择一部分特征和分裂点进行尝试，可能在某些情况下加速训练或增加树的多样性（用于集成）。</li>
<li><code>max_depth</code>: int, 默认=None。树的最大深度。如果为None，则节点会一直扩展直到所有叶子都是纯的，或者直到所有叶子包含少于<code>min_samples_split</code>个样本。</li>
<li><code>min_samples_split</code>: int or float, 默认=2。分裂内部节点所需的最小样本数。</li>
<li><code>min_samples_leaf</code>: int or float, 默认=1。叶节点所需的最小样本数。</li>
<li><code>max_features</code>: int, float, string or None, 默认=None。寻找最佳分裂时要考虑的特征数量。
<ul>
<li>如果为int，则在每个分裂点考虑<code>max_features</code>个特征。</li>
<li>如果为float，则<code>max_features</code>是一个百分比，<code>int(max_features * n_features)</code>个特征在每个分裂点被考虑。</li>
<li>如果为”sqrt”，则<code>max_features=sqrt(n_features)</code>。</li>
<li>如果为”log2”，则<code>max_features=log2(n_features)</code>。</li>
<li>如果为None，则<code>max_features=n_features</code>。</li>
</ul></li>
<li><code>random_state</code>: int, RandomState instance or None, 默认=None。用于控制估计器的随机性。当<code>splitter="random"</code>时，或者在选择特征时（如果<code>max_features &lt; n_features</code>），会用到。</li>
<li><code>max_leaf_nodes</code>: int or None, 默认=None。以最佳优先的方式生长一棵具有<code>max_leaf_nodes</code>的树。最佳节点定义为不纯度相对减少最多的节点。如果为None则叶节点数量不受限制。</li>
<li><code>min_impurity_decrease</code>: float, 默认=0.0。如果一个节点的分裂导致不纯度的减少大于或等于此值，则该节点将被分裂。</li>
<li><code>class_weight</code>: dict, list of dicts, “balanced” or None, 默认=None。与类别关联的权重。用于处理类别不平衡问题。</li>
<li><code>ccp_alpha</code>: non-negative float, 默认=0.0。用于最小代价复杂度剪枝的复杂度参数。将选择代价复杂度最大且小于<code>ccp_alpha</code>的子树被剪枝。默认情况下，不执行剪枝。</li>
</ul>
</section>
<section id="回归决策树-decisiontreeregressor" class="level3">
<h3 class="anchored" data-anchor-id="回归决策树-decisiontreeregressor">6.4.2 回归决策树 (<code>DecisionTreeRegressor</code>)</h3>
<p>回归树的用法与分类树非常相似，只是它们预测的是连续值，并且使用不同的分裂标准（如MSE）。</p>
<div id="990ffe00" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 生成简单的一维回归数据</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>X_reg <span class="op">=</span> np.sort(<span class="dv">5</span> <span class="op">*</span> np.random.rand(<span class="dv">80</span>, <span class="dv">1</span>), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>y_reg <span class="op">=</span> np.sin(X_reg).ravel() <span class="op">+</span> np.random.randn(<span class="dv">80</span>) <span class="op">*</span> <span class="fl">0.1</span> <span class="co"># y = sin(x) + noise</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练两个不同深度的回归树</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>dt_reg1 <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>dt_reg2 <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>dt_reg1.fit(X_reg, y_reg)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>dt_reg2.fit(X_reg, y_reg)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 生成测试点用于绘图</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>X_test_reg <span class="op">=</span> np.arange(<span class="fl">0.0</span>, <span class="fl">5.0</span>, <span class="fl">0.01</span>)[:, np.newaxis]</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>y_pred_reg1 <span class="op">=</span> dt_reg1.predict(X_test_reg)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>y_pred_reg2 <span class="op">=</span> dt_reg2.predict(X_test_reg)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 绘图</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_reg, y_reg, s<span class="op">=</span><span class="dv">20</span>, edgecolor<span class="op">=</span><span class="st">"black"</span>, c<span class="op">=</span><span class="st">"darkorange"</span>, label<span class="op">=</span><span class="st">"data"</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>plt.plot(X_test_reg, y_pred_reg1, color<span class="op">=</span><span class="st">"cornflowerblue"</span>, label<span class="op">=</span><span class="st">"max_depth=2"</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>plt.plot(X_test_reg, y_pred_reg2, color<span class="op">=</span><span class="st">"yellowgreen"</span>, label<span class="op">=</span><span class="st">"max_depth=5"</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"data"</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"target"</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Decision Tree Regression"</span>)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 保存图像</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>image_path_reg_tree <span class="op">=</span> <span class="st">"images/06-decision-trees-ensemble-learning/regression_tree_example.svg"</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>plt.savefig(image_path_reg_tree, <span class="bu">format</span><span class="op">=</span><span class="st">'svg'</span>)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="./images/06-decision-trees-ensemble-learning/regression_tree_example.svg" class="img-fluid"> <em>(图 6.3: 一个简单的一维回归决策树示例。可以看出，<code>max_depth=2</code>的树对数据进行了粗略的划分，而<code>max_depth=5</code>的树则更细致地拟合了数据，但也可能开始过拟合噪声。)</em></p>
<p>回归树的叶节点预测的是该叶节点内所有训练样本目标值的均值。因此，其预测结果是分段常数函数。</p>
<p>主要的参数与 <code>DecisionTreeClassifier</code> 类似，但 <code>criterion</code> 参数对于 <code>DecisionTreeRegressor</code> 通常是：</p>
<ul>
<li><code>"mse"</code> (默认，均方误差，L2损失)</li>
<li><code>"friedman_mse"</code> (带Friedman改进的均方误差)</li>
<li><code>"mae"</code> (平均绝对误差，L1损失)</li>
<li>从 Scikit-learn 1.0 版本开始，<code>"poisson"</code> 也被支持。</li>
</ul>
</section>
</section>
<section id="集成学习简介-ensemble-learning" class="level2">
<h2 class="anchored" data-anchor-id="集成学习简介-ensemble-learning">6.5 集成学习简介 (Ensemble Learning)</h2>
<p>“三个臭皮匠，顶个诸葛亮。” 这句俗语形象地说明了集成学习的核心思想。<strong>集成学习 (Ensemble Learning)</strong> 不是指某一个特定的机器学习算法，而是一种元算法框架 (meta-algorithm)，它通过构建并结合多个学习器（通常称为<strong>基学习器 (base learners)</strong> 或<strong>弱学习器 (weak learners)</strong>）的预测来获得比单个学习器更好的泛化性能。</p>
<p>即使是表现一般的弱学习器，只要它们之间具有一定的<strong>多样性 (diversity)</strong>并且表现略好于随机猜测，通过有效的集成策略，就可以构建出一个强大的<strong>集成模型 (ensemble model)</strong>。</p>
<p><strong>为什么集成学习有效？</strong></p>
<ol type="1">
<li><strong>降低方差 (Variance Reduction)：</strong> 多个模型的预测进行平均（例如在Bagging中），可以平滑掉单个模型可能存在的随机波动和噪声，使得最终模型的预测更加稳定和鲁棒，从而降低方差。这对于那些容易过拟合的高方差模型（如未剪枝的决策树）尤其有效。</li>
<li><strong>降低偏差 (Bias Reduction)：</strong> 某些集成方法（例如Boosting）通过迭代地关注先前模型预测错误的样本，逐步提升模型的拟合能力，从而降低整体的偏差。这有助于解决模型欠拟合的问题。</li>
<li><strong>改善模型选择/避免陷入局部最优：</strong> 单个模型可能会陷入参数空间的局部最优解。集成多个从不同初始条件或数据子集训练出来的模型，相当于从更广阔的假设空间中进行搜索，有助于找到更好的或更接近全局最优的解。</li>
<li><strong>提高泛化能力：</strong> 通过结合多个具有不同”视角”或”偏好”的模型，集成模型能够捕捉到数据中更复杂、更全面的模式，从而在未见过的数据上表现更好。</li>
</ol>
<p><strong>集成学习的关键要素：</strong></p>
<ul>
<li><strong>基学习器的选择：</strong> 理论上任何学习算法都可以作为基学习器。决策树由于其易于实现、对参数不敏感（在某些集成方法中）以及能够处理不同类型数据等优点，是集成学习中最常用的基学习器之一。</li>
<li><strong>基学习器的多样性：</strong> 这是集成学习成功的关键。如果所有基学习器都完全相同或者高度相关，那么集成它们并不能带来性能上的提升。产生多样性的方法包括：
<ul>
<li>使用不同的训练数据子集（例如，Bagging中的自助采样）。</li>
<li>使用不同的特征子集（例如，随机森林中的特征随机选择）。</li>
<li>使用不同的算法作为基学习器。</li>
<li>使用不同的超参数配置。</li>
</ul></li>
<li><strong>集成策略/结合方法：</strong> 如何将多个基学习器的预测结果结合起来形成最终预测。常见的方法包括：
<ul>
<li><strong>投票法 (Voting)：</strong> 用于分类任务。包括硬投票（少数服从多数）和软投票（基于概率的加权平均）。</li>
<li><strong>平均法 (Averaging)：</strong> 用于回归任务。简单平均或加权平均。</li>
<li><strong>学习法 (Learning)：</strong> 例如Stacking，训练一个元学习器来学习如何最好地结合基学习器的预测。</li>
</ul></li>
</ul>
<p><strong>主要的集成学习方法类别：</strong></p>
<ol type="1">
<li><strong>Bagging (Bootstrap Aggregating)：</strong> 并行集成方法，旨在降低方差。代表算法是随机森林 (Random Forest)。</li>
<li><strong>Boosting：</strong> 串行集成方法，旨在降低偏差。代表算法包括AdaBoost, Gradient Boosting Machines (GBM), XGBoost, LightGBM, CatBoost等。</li>
<li><strong>Stacking (Stacked Generalization)：</strong> 通过训练一个元模型来结合多个不同类型基学习器的预测。</li>
</ol>
<p>接下来，我们将详细探讨Bagging和Boosting这两大类主流的集成学习方法。</p>
</section>
<section id="bagging" class="level2">
<h2 class="anchored" data-anchor-id="bagging">6.6 Bagging</h2>
<p><strong>Bagging</strong> (Bootstrap Aggregating) 是一种经典的并行集成学习方法，由 Breiman 于1996年提出。该方法通过自助采样构建多个训练子集，并独立训练基学习器，最后通过投票或平均方式集成预测结果。Bagging 能有效降低模型方差，提升泛化性能，特别适用于高方差模型如未剪枝决策树。</p>
<section id="bagging-的工作原理" class="level3">
<h3 class="anchored" data-anchor-id="bagging-的工作原理">6.6.1 Bagging 的工作原理</h3>
<p>Bagging的核心思想可以概括为以下几个步骤：</p>
<ol type="1">
<li><p><strong>自助采样 (Bootstrap Sampling)：</strong> 假设我们有一个包含 <span class="math inline">\(m\)</span> 个样本的原始训练数据集 <span class="math inline">\(D\)</span>。Bagging通过<strong>有放回的随机采样 (sampling with replacement)</strong> 从 <span class="math inline">\(D\)</span> 中抽取 <span class="math inline">\(m\)</span> 个样本，构成一个新的训练子集 <span class="math inline">\(D_i\)</span>。这个过程重复 <span class="math inline">\(B\)</span> 次（<span class="math inline">\(B\)</span> 是基学习器的数量），从而得到 <span class="math inline">\(B\)</span> 个不同的训练子集 <span class="math inline">\(D_1, D_2, ..., D_B\)</span>。 由于是有放回采样，每个 <span class="math inline">\(D_i\)</span> 中的样本可能包含重复的原始样本，也可能缺少某些原始样本。据统计，每个自助采样出的训练子集 <span class="math inline">\(D_i\)</span> 大约包含原始数据中 <span class="math inline">\(1 - (1 - 1/m)^m \approx 1 - 1/e \approx 63.2\%\)</span> 的独特样本。那些未被某个 <span class="math inline">\(D_i\)</span> 抽中的样本（大约占原始数据的 <span class="math inline">\(36.8\%\)</span>）被称为<strong>袋外样本 (Out-of-Bag, OOB) 样本</strong>，它们可以用于评估该基学习器的性能，而无需额外的验证集（称为OOB评估）。</p></li>
<li><p><strong>独立训练基学习器：</strong> 使用每个自助采样得到的训练子集 <span class="math inline">\(D_i\)</span> 独立地训练一个基学习器 <span class="math inline">\(h_i\)</span>。这些基学习器可以是同一种类型的算法（例如，都是决策树），也可以是不同类型的算法（尽管前者更常见）。由于每个基学习器是在略有不同的数据上训练的，它们之间会产生一定的多样性。</p></li>
<li><p><strong>结合预测结果：</strong> 当所有 <span class="math inline">\(B\)</span> 个基学习器都训练完成后，对于新的输入样本 <span class="math inline">\(\mathbf{x}\)</span>，每个基学习器 <span class="math inline">\(h_i\)</span> 都会给出一个预测结果 <span class="math inline">\(h_i(\mathbf{x})\)</span>。</p>
<ul>
<li><strong>分类任务：</strong> 通常采用<strong>简单投票法 (plurality voting 或 hard voting)</strong>，即选择得票最多的类别作为最终预测结果。如果基学习器能输出类别概率，也可以采用<strong>软投票法 (soft voting)</strong>，即对所有基学习器输出的类别概率进行平均，然后选择概率最大的类别。 <span class="math display">\[ H(\mathbf{x}) = \text{argmax}_k \sum_{i=1}^{B} I(h_i(\mathbf{x}) = k) \quad \text{(Hard Voting)} \]</span> <span class="math display">\[ H(\mathbf{x}) = \text{argmax}_k \frac{1}{B} \sum_{i=1}^{B} P(y=k | \mathbf{x}, h_i) \quad \text{(Soft Voting)} \]</span></li>
<li><strong>回归任务：</strong> 通常采用<strong>简单平均法 (averaging)</strong>，即将所有基学习器的预测值进行平均作为最终预测结果。 <span class="math display">\[ H(\mathbf{x}) = \frac{1}{B} \sum_{i=1}^{B} h_i(\mathbf{x}) \]</span></li>
</ul></li>
</ol>
<p>下图展示了Bagging的基本流程：</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<svg width="672" height="480" viewbox="0.00 0.00 655.84 263.00" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 259)">
<title>BaggingFlowchart</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-259 651.84,-259 651.84,4 -4,4"></polygon>
<g id="clust1" class="cluster">
<title>cluster_data</title>
<polygon fill="none" stroke="black" points="8,-116 8,-193 105.32,-193 105.32,-116 8,-116"></polygon>
<text text-anchor="middle" x="56.66" y="-176.4" font-family="Times,serif" font-size="14.00">原始训练集 D</text>
</g>
<g id="clust2" class="cluster">
<title>cluster_sampling</title>
<polygon fill="none" stroke="black" points="146.32,-8 146.32,-247 254.15,-247 254.15,-8 146.32,-8"></polygon>
<text text-anchor="middle" x="200.24" y="-230.4" font-family="Times,serif" font-size="14.00">自助采样</text>
</g>
<g id="clust3" class="cluster">
<title>cluster_training</title>
<polygon fill="none" stroke="black" points="275.15,-8 275.15,-247 375.15,-247 375.15,-8 275.15,-8"></polygon>
<text text-anchor="middle" x="325.15" y="-230.4" font-family="Times,serif" font-size="14.00">独立训练基学习器</text>
</g>
<g id="clust4" class="cluster">
<title>cluster_aggregation</title>
<polygon fill="none" stroke="black" points="396.15,-88 396.15,-165 497.53,-165 497.53,-88 396.15,-88"></polygon>
<text text-anchor="middle" x="446.84" y="-148.4" font-family="Times,serif" font-size="14.00">结合预测</text>
</g>
<!-- D -->
<g id="node1" class="node">
<title>D</title>
<path fill="none" stroke="black" d="M85.49,-160C85.49,-160 27.84,-160 27.84,-160 21.84,-160 15.84,-154 15.84,-148 15.84,-148 15.84,-136 15.84,-136 15.84,-130 21.84,-124 27.84,-124 27.84,-124 85.49,-124 85.49,-124 91.49,-124 97.49,-130 97.49,-136 97.49,-136 97.49,-148 97.49,-148 97.49,-154 91.49,-160 85.49,-160"></path>
<text text-anchor="middle" x="56.66" y="-137.8" font-family="Times,serif" font-size="14.00">D (m个样本)</text>
</g>
<!-- D1 -->
<g id="node2" class="node">
<title>D1</title>
<path fill="none" stroke="black" d="M234.06,-214C234.06,-214 166.41,-214 166.41,-214 160.41,-214 154.41,-208 154.41,-202 154.41,-202 154.41,-190 154.41,-190 154.41,-184 160.41,-178 166.41,-178 166.41,-178 234.06,-178 234.06,-178 240.06,-178 246.06,-184 246.06,-190 246.06,-190 246.06,-202 246.06,-202 246.06,-208 240.06,-214 234.06,-214"></path>
<text text-anchor="middle" x="200.24" y="-191.8" font-family="Times,serif" font-size="14.00">D₁ (m个样本)</text>
</g>
<!-- D&#45;&gt;D1 -->
<g id="edge1" class="edge">
<title>D-&gt;D1</title>
<path fill="none" stroke="black" stroke-dasharray="5,2" d="M97.48,-157.19C112.1,-162.77 128.93,-169.18 144.64,-175.17"></path>
<polygon fill="black" stroke="black" points="143.65,-178.54 154.24,-178.84 146.14,-172 143.65,-178.54"></polygon>
<text text-anchor="middle" x="125.82" y="-174.2" font-family="Times,serif" font-size="14.00">抽样</text>
</g>
<!-- D2 -->
<g id="node3" class="node">
<title>D2</title>
<path fill="none" stroke="black" d="M234.06,-160C234.06,-160 166.41,-160 166.41,-160 160.41,-160 154.41,-154 154.41,-148 154.41,-148 154.41,-136 154.41,-136 154.41,-130 160.41,-124 166.41,-124 166.41,-124 234.06,-124 234.06,-124 240.06,-124 246.06,-130 246.06,-136 246.06,-136 246.06,-148 246.06,-148 246.06,-154 240.06,-160 234.06,-160"></path>
<text text-anchor="middle" x="200.24" y="-137.8" font-family="Times,serif" font-size="14.00">D₂ (m个样本)</text>
</g>
<!-- D&#45;&gt;D2 -->
<g id="edge2" class="edge">
<title>D-&gt;D2</title>
<path fill="none" stroke="black" stroke-dasharray="5,2" d="M97.48,-142C111.97,-142 128.62,-142 144.19,-142"></path>
<polygon fill="black" stroke="black" points="144.24,-145.5 154.24,-142 144.24,-138.5 144.24,-145.5"></polygon>
</g>
<!-- DB -->
<g id="node5" class="node">
<title>DB</title>
<path fill="none" stroke="black" d="M232.56,-52C232.56,-52 167.91,-52 167.91,-52 161.91,-52 155.91,-46 155.91,-40 155.91,-40 155.91,-28 155.91,-28 155.91,-22 161.91,-16 167.91,-16 167.91,-16 232.56,-16 232.56,-16 238.56,-16 244.56,-22 244.56,-28 244.56,-28 244.56,-40 244.56,-40 244.56,-46 238.56,-52 232.56,-52"></path>
<text text-anchor="middle" x="200.24" y="-29.8" font-family="Times,serif" font-size="14.00">Dʙ (m个样本)</text>
</g>
<!-- D&#45;&gt;DB -->
<g id="edge3" class="edge">
<title>D-&gt;DB</title>
<path fill="none" stroke="black" stroke-dasharray="5,2" d="M74.79,-123.73C91.82,-106.22 119.25,-79.76 146.32,-61 148.15,-59.74 150.04,-58.49 151.98,-57.28"></path>
<polygon fill="black" stroke="black" points="153.84,-60.24 160.66,-52.13 150.28,-54.22 153.84,-60.24"></polygon>
</g>
<!-- h1 -->
<g id="node6" class="node">
<title>h1</title>
<path fill="none" stroke="black" d="M340.15,-214C340.15,-214 310.15,-214 310.15,-214 304.15,-214 298.15,-208 298.15,-202 298.15,-202 298.15,-190 298.15,-190 298.15,-184 304.15,-178 310.15,-178 310.15,-178 340.15,-178 340.15,-178 346.15,-178 352.15,-184 352.15,-190 352.15,-190 352.15,-202 352.15,-202 352.15,-208 346.15,-214 340.15,-214"></path>
<text text-anchor="middle" x="325.15" y="-191.8" font-family="Times,serif" font-size="14.00">h₁</text>
</g>
<!-- D1&#45;&gt;h1 -->
<g id="edge4" class="edge">
<title>D1-&gt;h1</title>
<path fill="none" stroke="black" d="M246.31,-196C260.04,-196 274.95,-196 288.01,-196"></path>
<polygon fill="black" stroke="black" points="288.03,-199.5 298.03,-196 288.03,-192.5 288.03,-199.5"></polygon>
</g>
<!-- h2 -->
<g id="node7" class="node">
<title>h2</title>
<path fill="none" stroke="black" d="M340.15,-160C340.15,-160 310.15,-160 310.15,-160 304.15,-160 298.15,-154 298.15,-148 298.15,-148 298.15,-136 298.15,-136 298.15,-130 304.15,-124 310.15,-124 310.15,-124 340.15,-124 340.15,-124 346.15,-124 352.15,-130 352.15,-136 352.15,-136 352.15,-148 352.15,-148 352.15,-154 346.15,-160 340.15,-160"></path>
<text text-anchor="middle" x="325.15" y="-137.8" font-family="Times,serif" font-size="14.00">h₂</text>
</g>
<!-- D2&#45;&gt;h2 -->
<g id="edge5" class="edge">
<title>D2-&gt;h2</title>
<path fill="none" stroke="black" d="M246.31,-142C260.04,-142 274.95,-142 288.01,-142"></path>
<polygon fill="black" stroke="black" points="288.03,-145.5 298.03,-142 288.03,-138.5 288.03,-145.5"></polygon>
</g>
<!-- Db -->
<g id="node4" class="node">
<title>Db</title>
<path fill="none" stroke="black" d="M215.24,-106C215.24,-106 185.24,-106 185.24,-106 179.24,-106 173.24,-100 173.24,-94 173.24,-94 173.24,-82 173.24,-82 173.24,-76 179.24,-70 185.24,-70 185.24,-70 215.24,-70 215.24,-70 221.24,-70 227.24,-76 227.24,-82 227.24,-82 227.24,-94 227.24,-94 227.24,-100 221.24,-106 215.24,-106"></path>
<text text-anchor="middle" x="200.24" y="-83.8" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- hB -->
<g id="node9" class="node">
<title>hB</title>
<path fill="none" stroke="black" d="M340.15,-52C340.15,-52 310.15,-52 310.15,-52 304.15,-52 298.15,-46 298.15,-40 298.15,-40 298.15,-28 298.15,-28 298.15,-22 304.15,-16 310.15,-16 310.15,-16 340.15,-16 340.15,-16 346.15,-16 352.15,-22 352.15,-28 352.15,-28 352.15,-40 352.15,-40 352.15,-46 346.15,-52 340.15,-52"></path>
<text text-anchor="middle" x="325.15" y="-29.8" font-family="Times,serif" font-size="14.00">hʙ</text>
</g>
<!-- DB&#45;&gt;hB -->
<g id="edge6" class="edge">
<title>DB-&gt;hB</title>
<path fill="none" stroke="black" d="M244.57,-34C258.64,-34 274.1,-34 287.63,-34"></path>
<polygon fill="black" stroke="black" points="287.99,-37.5 297.99,-34 287.99,-30.5 287.99,-37.5"></polygon>
</g>
<!-- Agg -->
<g id="node10" class="node">
<title>Agg</title>
<ellipse fill="none" stroke="black" cx="446.84" cy="-114" rx="42.88" ry="18"></ellipse>
<text text-anchor="middle" x="446.84" y="-109.8" font-family="Times,serif" font-size="14.00">投票/平均</text>
</g>
<!-- h1&#45;&gt;Agg -->
<g id="edge7" class="edge">
<title>h1-&gt;Agg</title>
<path fill="none" stroke="black" d="M352.28,-182.59C359.81,-178.47 367.93,-173.75 375.15,-169 390.17,-159.11 406.05,-146.93 418.94,-136.54"></path>
<polygon fill="black" stroke="black" points="421.4,-139.05 426.94,-130.02 416.98,-133.63 421.4,-139.05"></polygon>
</g>
<!-- h2&#45;&gt;Agg -->
<g id="edge8" class="edge">
<title>h2-&gt;Agg</title>
<path fill="none" stroke="black" d="M352.41,-135.86C366.08,-132.66 383.19,-128.66 399.02,-124.95"></path>
<polygon fill="black" stroke="black" points="400.25,-128.26 409.19,-122.57 398.66,-121.44 400.25,-128.26"></polygon>
</g>
<!-- hb -->
<g id="node8" class="node">
<title>hb</title>
<path fill="none" stroke="black" d="M340.15,-106C340.15,-106 310.15,-106 310.15,-106 304.15,-106 298.15,-100 298.15,-94 298.15,-94 298.15,-82 298.15,-82 298.15,-76 304.15,-70 310.15,-70 310.15,-70 340.15,-70 340.15,-70 346.15,-70 352.15,-76 352.15,-82 352.15,-82 352.15,-94 352.15,-94 352.15,-100 346.15,-106 340.15,-106"></path>
<text text-anchor="middle" x="325.15" y="-83.8" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- hb&#45;&gt;Agg -->
<!-- hB&#45;&gt;Agg -->
<g id="edge10" class="edge">
<title>hB-&gt;Agg</title>
<path fill="none" stroke="black" d="M352.23,-47.49C359.75,-51.62 367.89,-56.31 375.15,-61 389.84,-70.49 405.47,-82.04 418.28,-91.93"></path>
<polygon fill="black" stroke="black" points="416.22,-94.77 426.26,-98.16 420.53,-89.25 416.22,-94.77"></polygon>
</g>
<!-- Result -->
<g id="node11" class="node">
<title>Result</title>
<ellipse fill="lightblue" stroke="black" cx="587.18" cy="-114" rx="60.81" ry="18"></ellipse>
<text text-anchor="middle" x="587.18" y="-109.8" font-family="Times,serif" font-size="14.00">最终预测 H(x)</text>
</g>
<!-- Agg&#45;&gt;Result -->
<g id="edge11" class="edge">
<title>Agg-&gt;Result</title>
<path fill="none" stroke="black" d="M489.72,-114C498.1,-114 507.12,-114 516.15,-114"></path>
<polygon fill="black" stroke="black" points="516.33,-117.5 526.33,-114 516.33,-110.5 516.33,-117.5"></polygon>
</g>
</g>
</svg>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p><em>(图 6.4: Bagging算法流程示意图。)</em></p>
</section>
<section id="随机森林-random-forest" class="level3">
<h3 class="anchored" data-anchor-id="随机森林-random-forest">6.6.2 随机森林 (Random Forest)</h3>
<p><strong>随机森林 (Random Forest, RF)</strong> 是由Breiman在2001年提出的一种集成学习方法，它是Bagging的一个扩展变体，专门使用<strong>决策树</strong>作为基学习器。与标准Bagging相比，随机森林在两个方面引入了随机性：1) 样本随机性（通过自助采样）；2) 特征随机性（在决策树每个节点分裂时随机选择特征子集），这种双重随机性机制能有效增强基学习器之间的多样性。</p>
<p><strong>随机森林的构建过程：</strong></p>
<ol type="1">
<li><strong>自助采样：</strong> 与Bagging相同，从原始训练集中有放回地抽取 <span class="math inline">\(m\)</span> 个样本，形成一个训练子集。</li>
<li><strong>训练决策树（带特征随机性）：</strong> 对于每个训练子集，训练一个决策树。在每个节点进行分裂时，算法<strong>不是从所有 <span class="math inline">\(p\)</span> 个特征中选择最优分裂特征，而是从一个随机选择的特征子集（大小通常为 <span class="math inline">\(k &lt; p\)</span>）中选择最优分裂特征。</strong>
<ul>
<li>通常，<span class="math inline">\(k\)</span> 的取值对于分类问题是 <span class="math inline">\(\sqrt{p}\)</span>，对于回归问题是 <span class="math inline">\(p/3\)</span>（这些是Scikit-learn中的默认值）。</li>
<li>每棵树通常会完全生长，不进行或很少进行剪枝（因为Bagging本身有助于减少过拟合）。</li>
</ul></li>
<li><strong>集成预测：</strong> 对于新的输入样本，所有决策树分别进行预测，然后通过简单投票（分类）或平均（回归）得到最终的随机森林预测结果。</li>
</ol>
<p><strong>为什么引入特征随机性？</strong></p>
<p>如果数据中存在一些非常强的预测特征，那么在标准的Bagging过程中，很多基决策树的顶部分裂可能都会选择这些强特征，导致这些树的结构变得相似，从而降低了它们的多样性。通过在每个节点分裂时限制可选特征的范围，随机森林迫使一些树使用次优的特征进行分裂，这增加了树之间的差异性，通常能带来更好的整体性能（进一步降低方差）。</p>
<p><strong>随机森林的优点：</strong></p>
<ul>
<li><strong>高准确率：</strong> 通常具有非常好的预测性能，是许多机器学习竞赛和实际应用中的首选算法之一。</li>
<li><strong>鲁棒性好，不易过拟合：</strong> 由于Bagging和特征随机性的双重作用，随机森林对噪声不敏感，并且比单个决策树更不容易过拟合。</li>
<li><strong>能够处理高维数据：</strong> 即使特征数量远大于样本数量，也能表现良好。</li>
<li><strong>能够处理类别不平衡问题：</strong> 可以通过调整类别权重或进行下采样/过采样来改善。</li>
<li><strong>可以并行训练：</strong> 各个决策树的训练是独立的。</li>
<li><strong>内置特征重要性评估：</strong> 可以通过计算特征在所有树中对不纯度减少的平均贡献，或者通过置换特征后模型性能的下降程度，来评估特征的重要性。</li>
<li><strong>提供了OOB错误率估计：</strong> 可以作为模型泛化能力的一个无偏估计，而无需显式的交叉验证集。</li>
</ul>
<p><strong>随机森林的缺点：</strong></p>
<ul>
<li><strong>可解释性降低：</strong> 相对于单个决策树，随机森林是一个”黑盒”模型，其内部决策逻辑更难直观理解。</li>
<li><strong>训练和预测时间可能较长：</strong> 需要训练和查询大量的树，尤其是在树的数量很多或者树很深的情况下。</li>
<li><strong>对于某些非常稀疏的数据或某些特定结构的数据，可能不如线性模型或其他专门算法。</strong></li>
</ul>
</section>
<section id="在-scikit-learn-中使用-bagging-和随机森林" class="level3">
<h3 class="anchored" data-anchor-id="在-scikit-learn-中使用-bagging-和随机森林">6.6.3 在 Scikit-learn 中使用 Bagging 和随机森林</h3>
<p>Scikit-learn 提供了 <code>BaggingClassifier</code> 和 <code>BaggingRegressor</code> 作为通用的Bagging实现，它们可以接受任何基估计器。同时，也直接提供了专门的 <code>RandomForestClassifier</code> 和 <code>RandomForestRegressor</code>。</p>
<section id="baggingclassifier-示例" class="level4">
<h4 class="anchored" data-anchor-id="baggingclassifier-示例">6.6.3.1 <code>BaggingClassifier</code> 示例</h4>
<p>我们可以使用 <code>BaggingClassifier</code> 来集成多个决策树（或其他模型）。</p>
<div id="3996ec8b" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> BaggingClassifier</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 生成月亮数据</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>X_moons_bag, y_moons_bag <span class="op">=</span> make_moons(n_samples<span class="op">=</span><span class="dv">500</span>, noise<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>X_train_bag, X_test_bag, y_train_bag, y_test_bag <span class="op">=</span> train_test_split(X_moons_bag, y_moons_bag, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建一个Bagging分类器，基学习器是决策树</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># n_estimators: 基学习器的数量</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># max_samples: 每个基学习器训练时从X抽取的最大样本数 (可以是整数或浮点数比例)</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># bootstrap: 是否使用自助采样 (True代表Bagging)</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># oob_score: 是否使用袋外样本来估计泛化误差</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>bag_clf <span class="op">=</span> BaggingClassifier(</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>, max_depth<span class="op">=</span><span class="va">None</span>), <span class="co"># 基学习器，这里用未剪枝的决策树</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">500</span>,       <span class="co"># 500棵树</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    max_samples<span class="op">=</span><span class="fl">1.0</span>,        <span class="co"># 每棵树使用100%的自助采样样本 (等价于m个)</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    bootstrap<span class="op">=</span><span class="va">True</span>,         <span class="co"># 开启自助采样</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,              <span class="co"># 使用所有可用的CPU核心</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    oob_score<span class="op">=</span><span class="va">True</span>          <span class="co"># 计算OOB分数</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>bag_clf.fit(X_train_bag, y_train_bag)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>y_pred_bag <span class="op">=</span> bag_clf.predict(X_test_bag)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"BaggingClassifier (with Decision Trees) 准确率: </span><span class="sc">{</span>accuracy_score(y_test_bag, y_pred_bag)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> bag_clf.oob_score_:</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"BaggingClassifier OOB Score: </span><span class="sc">{</span>bag_clf<span class="sc">.</span>oob_score_<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="co"># 对比单个决策树的性能</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>single_dt_clf <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>single_dt_clf.fit(X_train_bag, y_train_bag)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>y_pred_single_dt <span class="op">=</span> single_dt_clf.predict(X_test_bag)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Single Decision Tree 准确率: </span><span class="sc">{</span>accuracy_score(y_test_bag, y_pred_single_dt)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>BaggingClassifier (with Decision Trees) 准确率: 0.9120
BaggingClassifier OOB Score: 0.8960
Single Decision Tree 准确率: 0.8560</code></pre>
</div>
</div>
<p>可以看到，Bagging集成通常能显著提高单个决策树的性能，特别是当单个决策树容易过拟合时。</p>
</section>
<section id="randomforestclassifier-与-randomforestregressor" class="level4">
<h4 class="anchored" data-anchor-id="randomforestclassifier-与-randomforestregressor">6.6.3.2 <code>RandomForestClassifier</code> 与 <code>RandomForestRegressor</code></h4>
<p><code>RandomForestClassifier</code> 和 <code>RandomForestRegressor</code> 是专门为随机森林算法优化过的类，使用起来更方便，并且通常比手动配置 <code>BaggingClassifier</code> 套用 <code>DecisionTreeClassifier</code> 效率更高一些，因为它们可以直接控制决策树构建过程中的特征随机性。</p>
<p><strong><code>RandomForestClassifier</code> 示例 (鸢尾花数据集):</strong></p>
<div id="c34aa95a" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>iris_rf <span class="op">=</span> load_iris()</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>X_iris_rf <span class="op">=</span> iris_rf.data</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>y_iris_rf <span class="op">=</span> iris_rf.target</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>X_train_rf, X_test_rf, y_train_rf, y_test_rf <span class="op">=</span> train_test_split(X_iris_rf, y_iris_rf, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>rf_clf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, oob_score<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>rf_clf.fit(X_train_rf, y_train_rf)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>y_pred_rf <span class="op">=</span> rf_clf.predict(X_test_rf)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">RandomForestClassifier 准确率 (鸢尾花): </span><span class="sc">{</span>accuracy_score(y_test_rf, y_pred_rf)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> rf_clf.oob_score_:</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"RandomForestClassifier OOB Score: </span><span class="sc">{</span>rf_clf<span class="sc">.</span>oob_score_<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
RandomForestClassifier 准确率 (鸢尾花): 1.0000
RandomForestClassifier OOB Score: 0.9464</code></pre>
</div>
</div>
<p><strong>主要参数 (<code>RandomForestClassifier</code> / <code>RandomForestRegressor</code>)</strong>：</p>
<p>许多参数与 <code>DecisionTreeClassifier</code>/<code>Regressor</code> 和 <code>BaggingClassifier</code>/<code>Regressor</code> 类似，例如：</p>
<ul>
<li><code>n_estimators</code>: int, 默认=100。森林中树的数量。</li>
<li><code>criterion</code>: {“gini”, “entropy”} for Classifier, {“mse”, “mae”, “friedman_mse”, “poisson”} for Regressor。分裂质量的衡量标准。</li>
<li><code>max_depth</code>: int, 默认=None。树的最大深度。</li>
<li><code>min_samples_split</code>: int or float, 默认=2。</li>
<li><code>min_samples_leaf</code>: int or float, 默认=1。</li>
<li><code>min_weight_fraction_leaf</code>: float, 默认=0.0。</li>
<li><code>max_features</code>: {“sqrt”, “log2”, None}, int or float, 默认=“sqrt” (RF Classifier) or 1.0 (RF Regressor, effectively <code>n_features</code> in older versions, but typically refers to all features for regressor, needs careful check for specific sklearn version behavior, usually <code>p</code> for regressors). 这是随机森林特有的特征随机性参数，控制每个分裂点随机选择的特征子集大小。
<ul>
<li>对于分类器，默认 <code>"sqrt"</code> (即 <span class="math inline">\(\\sqrt{p}\)</span>)。</li>
<li>对于回归器，旧版本默认是 <code>n_features</code> (所有特征)，但更现代的实现或文献推荐 <span class="math inline">\(p/3\)</span>。Scikit-learn的 <code>RandomForestRegressor</code> 的 <code>max_features</code> 默认值在不同版本中可能有所变化，通常是1.0（代表所有特征）或 <code>"auto"</code> (等同于<code>n_features</code>)，但用户应根据问题调整，例如设置为 <code>1/3</code> 或 <code>"sqrt"</code>。</li>
</ul></li>
<li><code>max_leaf_nodes</code>: int, 默认=None。</li>
<li><code>min_impurity_decrease</code>: float, 默认=0.0。</li>
<li><code>bootstrap</code>: bool, 默认=True。是否在构建树时使用自助采样。</li>
<li><code>oob_score</code>: bool, 默认=False。是否使用袋外样本来估计泛化准确率。</li>
<li><code>n_jobs</code>: int, 默认=None。并行运行的作业数。<code>-1</code> 表示使用所有处理器。</li>
<li><code>random_state</code>: int, RandomState instance or None。</li>
<li><code>class_weight</code>: dict, list of dicts, “balanced”, “balanced_subsample” or None (Classifier only).</li>
<li><code>ccp_alpha</code>: non-negative float, 默认=0.0 (用于剪枝)。</li>
</ul>
<p>与 <code>BaggingClassifier(DecisionTreeClassifier(...))</code> 相比，<code>RandomForestClassifier</code> 更直接地实现了决策树构建时特征子集的随机选择。</p>
</section>
</section>
<section id="随机森林的特征重要性-feature-importances" class="level3">
<h3 class="anchored" data-anchor-id="随机森林的特征重要性-feature-importances">6.6.4 随机森林的特征重要性 (Feature Importances)</h3>
<p>随机森林可以评估各个特征对于预测任务的重要性。这对于理解数据、进行特征选择非常有帮助。</p>
<p>计算特征重要性的主要方法有两种：</p>
<ol type="1">
<li><strong>基于不纯度减少的特征重要性 (Mean Decrease in Impurity, MDI):</strong> 当训练随机森林中的每棵树时，可以记录每个特征在分裂节点时平均带来的不纯度减少量（例如，基尼不纯度减少或信息增益）。将所有树的这个值进行平均，就可以得到该特征的重要性得分。得分越高的特征，在分裂决策中起到的作用越大。 这是Scikit-learn中 <code>RandomForestClassifier</code> 和 <code>RandomForestRegressor</code> 的 <code>feature_importances_</code> 属性计算的默认方法。
<ul>
<li><strong>优点：</strong> 计算速度快，直接从训练过程中获得。</li>
<li><strong>缺点：</strong> 可能会偏向于那些具有较多取值类别的高基数特征（high-cardinality features），并且如果特征之间存在相关性，可能会低估某些相关特征的重要性。</li>
</ul></li>
<li><strong>基于置换的特征重要性 (Permutation Importance / Mean Decrease in Accuracy, MDA):</strong> 在一个训练好的模型上，对于某个特征，将其在验证集或测试集（或OOB样本）中的值进行随机打乱（置换），然后观察模型性能（如准确率或R²)的下降程度。如果打乱一个特征的值导致模型性能显著下降，则说明该特征很重要。 这种方法更可靠，因为它直接衡量特征对模型预测性能的影响，并且不受高基数特征或特征相关性的偏见影响。 Scikit-learn 提供了 <code>sklearn.inspection.permutation_importance</code> 函数来实现。</li>
</ol>
<p><strong>示例：使用 <code>feature_importances_</code> 属性获取MDI特征重要性</strong></p>
<div id="b591b5a8" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 继续使用上面训练好的鸢尾花随机森林模型 rf_clf</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>importances_mdi_iris <span class="op">=</span> rf_clf.feature_importances_</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>std_mdi_iris <span class="op">=</span> np.std([tree.feature_importances_ <span class="cf">for</span> tree <span class="kw">in</span> rf_clf.estimators_], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>indices_mdi_iris <span class="op">=</span> np.argsort(importances_mdi_iris)[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 打印特征排序</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">鸢尾花数据集特征重要性 (MDI):"</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> f <span class="kw">in</span> <span class="bu">range</span>(X_iris_rf.shape[<span class="dv">1</span>]):</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>f <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">. 特征 </span><span class="sc">{</span>iris_rf<span class="sc">.</span>feature_names[indices_mdi_iris[f]]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>importances_mdi_iris[indices_mdi_iris[f]]<span class="sc">:.4f}</span><span class="ss">)"</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 绘制特征重要性条形图</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Feature Importances for Iris Dataset (MDI from RandomForest)"</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>plt.bar(<span class="bu">range</span>(X_iris_rf.shape[<span class="dv">1</span>]), importances_mdi_iris[indices_mdi_iris],</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span><span class="st">"skyblue"</span>, yerr<span class="op">=</span>std_mdi_iris[indices_mdi_iris], align<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(X_iris_rf.shape[<span class="dv">1</span>]), np.array(iris_rf.feature_names)[indices_mdi_iris], rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">"right"</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>plt.xlim([<span class="op">-</span><span class="dv">1</span>, X_iris_rf.shape[<span class="dv">1</span>]])</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Mean Decrease in Impurity"</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 保存图像</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>image_path_rf_feat_imp <span class="op">=</span> <span class="st">"images/06-decision-trees-ensemble-learning/iris_rf_feature_importance_mdi.svg"</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>plt.savefig(image_path_rf_feat_imp, <span class="bu">format</span><span class="op">=</span><span class="st">'svg'</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
鸢尾花数据集特征重要性 (MDI):
1. 特征 petal length (cm) (0.4376)
2. 特征 petal width (cm) (0.4231)
3. 特征 sepal length (cm) (0.1097)
4. 特征 sepal width (cm) (0.0295)</code></pre>
</div>
</div>
<p><img src="./images/06-decision-trees-ensemble-learning/iris_rf_feature_importance_mdi.svg" class="img-fluid"> <em>(图 6.5: 鸢尾花数据集上随机森林的特征重要性（基于平均不纯度减少MDI）。可以看出，花瓣长度和花瓣宽度是最重要的特征。)</em></p>
<p><strong>示例：使用 <code>permutation_importance</code> (MDA)</strong></p>
<div id="55af5c56" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> permutation_importance</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 在测试集上计算置换重要性</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>perm_importance_iris <span class="op">=</span> permutation_importance(</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    rf_clf, X_test_rf, y_test_rf, n_repeats<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>, n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>importances_mda_iris <span class="op">=</span> perm_importance_iris.importances_mean</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>std_mda_iris <span class="op">=</span> perm_importance_iris.importances_std</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>indices_mda_iris <span class="op">=</span> np.argsort(importances_mda_iris)[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">鸢尾花数据集特征重要性 (Permutation Importance - MDA on Test Set):"</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> f <span class="kw">in</span> <span class="bu">range</span>(X_iris_rf.shape[<span class="dv">1</span>]):</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>f <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">. 特征 </span><span class="sc">{</span>iris_rf<span class="sc">.</span>feature_names[indices_mda_iris[f]]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>importances_mda_iris[indices_mda_iris[f]]<span class="sc">:.4f}</span><span class="ss"> +/- </span><span class="sc">{</span>std_mda_iris[indices_mda_iris[f]]<span class="sc">:.4f}</span><span class="ss">)"</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 绘制特征重要性条形图</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Permutation Feature Importances for Iris Dataset (MDA on Test Set)"</span>)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>plt.bar(<span class="bu">range</span>(X_iris_rf.shape[<span class="dv">1</span>]), importances_mda_iris[indices_mda_iris],</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span><span class="st">"lightcoral"</span>, yerr<span class="op">=</span>std_mda_iris[indices_mda_iris], align<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(X_iris_rf.shape[<span class="dv">1</span>]), np.array(iris_rf.feature_names)[indices_mda_iris], rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">"right"</span>)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>plt.xlim([<span class="op">-</span><span class="dv">1</span>, X_iris_rf.shape[<span class="dv">1</span>]])</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Mean Accuracy Decrease"</span>)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 保存图像</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>image_path_rf_perm_imp <span class="op">=</span> <span class="st">"images/06-decision-trees-ensemble-learning/iris_rf_feature_importance_mda.svg"</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>plt.savefig(image_path_rf_perm_imp, <span class="bu">format</span><span class="op">=</span><span class="st">'svg'</span>)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
鸢尾花数据集特征重要性 (Permutation Importance - MDA on Test Set):
1. 特征 petal length (cm) (0.1737 +/- 0.0614)
2. 特征 petal width (cm) (0.1184 +/- 0.0294)
3. 特征 sepal width (cm) (0.0000 +/- 0.0000)
4. 特征 sepal length (cm) (0.0000 +/- 0.0000)</code></pre>
</div>
</div>
<p><img src="./images/06-decision-trees-ensemble-learning/iris_rf_feature_importance_mda.svg" class="img-fluid"> <em>(图 6.6: 鸢尾花数据集上随机森林的置换特征重要性（基于测试集上的平均准确率下降MDA）。结果与MDI方法相似，表明花瓣相关特征的重要性。)</em></p>
</section>
<section id="随机森林的参数调整建议" class="level3">
<h3 class="anchored" data-anchor-id="随机森林的参数调整建议">6.6.5 随机森林的参数调整建议</h3>
<ul>
<li><code>n_estimators</code>: 通常越大越好，但到一定程度后性能提升会饱和，并且会增加计算成本。一般从100开始，可以尝试几百到几千。可以通过观察OOB错误率随<code>n_estimators</code>的变化来选择一个合适的值。</li>
<li><code>max_features</code>: 这是影响随机森林性能和多样性的关键参数。默认值（分类<code>"sqrt"</code>，回归<code>1.0</code>或<code>"auto"</code>）通常是一个不错的起点，但值得通过交叉验证进行调优。</li>
<li><code>max_depth</code>, <code>min_samples_split</code>, <code>min_samples_leaf</code>: 这些参数控制单棵树的复杂度。对于随机森林，通常倾向于让树生长得比较深（即<code>max_depth=None</code>或较大），因为Bagging可以有效地减少过拟合。但如果担心计算成本或模型大小，可以适当限制它们。</li>
<li><code>bootstrap</code>: 几乎总是设置为<code>True</code>。</li>
<li><code>oob_score</code>: 设置为<code>True</code>可以方便地获得模型的泛化性能估计，而无需额外的验证集，尤其是在数据量较少时有用。</li>
</ul>
<p>通常使用 <code>GridSearchCV</code> 或 <code>RandomizedSearchCV</code> 结合交叉验证来调整这些参数。</p>
</section>
</section>
<section id="boosting" class="level2">
<h2 class="anchored" data-anchor-id="boosting">6.7 Boosting</h2>
<p><strong>Boosting</strong> 是一族可将弱学习器（通常指仅比随机猜测略好的学习器）提升为强学习器的集成学习算法。与Bagging中基学习器并行独立训练不同，Boosting采用<strong>串行方式</strong>训练基学习器，即每个新的基学习器都是在前一个基学习器的基础上进行优化的。</p>
<p>Boosting的核心思想是<strong>迭代地关注先前学习器预测错误的样本</strong>。在每一轮迭代中，Boosting算法会提高那些被前一轮弱学习器错误分类的样本的权重（或者说，让新的学习器更关注这些”难啃的骨头”），同时降低那些被正确分类的样本的权重。这样，后续的弱学习器就会更加专注于解决之前模型未能很好处理的那些困难样本，从而逐步提升整个集成模型的性能。</p>
<p>最终，所有弱学习器的预测结果会通过加权投票（分类）或加权平均（回归）的方式结合起来，形成最终的强学习器预测。</p>
<p><strong>Boosting的主要特点：</strong></p>
<ul>
<li><strong>降低偏差：</strong> Boosting主要致力于降低模型的偏差。通过迭代地修正错误，模型能够逐渐拟合训练数据中更复杂的模式。</li>
<li><strong>串行训练：</strong> 基学习器是依次生成的，后一个学习器的训练依赖于前一个学习器的表现。</li>
<li><strong>对基学习器要求较低：</strong> 理论上，只要基学习器比随机猜测好一点点，Boosting就能显著提升其性能。</li>
<li><strong>容易过拟合（如果迭代次数过多或基学习器过于复杂）：</strong> 虽然Boosting旨在提升性能，但如果迭代次数过多，或者基学习器本身过于复杂，也可能导致在训练数据上过拟合。因此，通常需要通过交叉验证来确定合适的迭代次数或使用早停策略。</li>
</ul>
<p>下面我们将介绍两种经典的Boosting算法：AdaBoost和梯度提升树 (Gradient Boosting Trees, GBT)。</p>
<section id="adaboost-adaptive-boosting" class="level3">
<h3 class="anchored" data-anchor-id="adaboost-adaptive-boosting">6.7.1 AdaBoost (Adaptive Boosting)</h3>
<p>AdaBoost (Adaptive Boosting，自适应提升) 是由Yoav Freund和Robert Schapire在1995年提出的，是最早也是最著名的Boosting算法之一。它通过改变训练数据的权重分布来使得后续的分类器更加关注之前分类错误的样本。</p>
<p><strong>AdaBoost算法流程 (以二分类为例，类别标签为 +1 和 -1)：</strong></p>
<ol type="1">
<li><p><strong>初始化样本权重：</strong> 给定一个包含 <span class="math inline">\(m\)</span> 个样本的训练集 <span class="math inline">\(D = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), ..., (\mathbf{x}_m, y_m)\}\)</span>。初始化每个样本的权重为 <span class="math inline">\(w_i^{(1)} = 1/m\)</span>，对于 <span class="math inline">\(i=1, ..., m\)</span>。</p></li>
<li><p><strong>迭代训练弱学习器 (共 <span class="math inline">\(T\)</span> 轮)：</strong> 对于 <span class="math inline">\(t = 1, ..., T\)</span>：</p>
<ol type="a">
<li><strong>训练弱分类器 <span class="math inline">\(h_t(\mathbf{x})\)</span>：</strong> 使用当前带有权重分布 <span class="math inline">\(W^{(t)} = (w_1^{(t)}, ..., w_m^{(t)})\)</span> 的训练数据训练一个弱分类器 <span class="math inline">\(h_t(\mathbf{x})\)</span>。该弱分类器旨在最小化加权错误率。</li>
<li><strong>计算弱分类器 <span class="math inline">\(h_t\)</span> 的加权错误率 <span class="math inline">\(\epsilon_t\)</span>：</strong> <span class="math display">\[ \epsilon_t = P(h_t(\mathbf{x}_i) \neq y_i) = \sum_{i=1}^{m} w_i^{(t)} I(h_t(\mathbf{x}_i) \neq y_i) \]</span> 其中 <span class="math inline">\(I(\cdot)\)</span> 是指示函数，当条件成立时为1，否则为0。</li>
<li><strong>计算弱分类器 <span class="math inline">\(h_t\)</span> 的权重 <span class="math inline">\(\alpha_t\)</span> (投票权重)：</strong> <span class="math display">\[ \alpha_t = \frac{1}{2} \ln \left( \frac{1 - \epsilon_t}{\epsilon_t} \right) \]</span> 可以看出，当弱分类器的错误率 <span class="math inline">\(\epsilon_t &lt; 0.5\)</span> 时（即比随机猜测好），<span class="math inline">\(\alpha_t &gt; 0\)</span>。错误率越低，该分类器的投票权重越大。如果 <span class="math inline">\(\epsilon_t \ge 0.5\)</span>，则该弱分类器效果很差，AdaBoost通常会停止或采取其他策略（例如，在Scikit-learn实现中，如果错误率为0或1，权重可能会被特殊处理）。</li>
<li><strong>更新样本权重 <span class="math inline">\(W^{(t+1)}\)</span>：</strong> <span class="math display">\[ w_i^{(t+1)} = \frac{w_i^{(t)}}{Z_t} \exp(-\alpha_t y_i h_t(\mathbf{x}_i)) \]</span> 其中 <span class="math inline">\(Z_t\)</span> 是归一化因子，确保 <span class="math inline">\(\sum_{i=1}^{m} w_i^{(t+1)} = 1\)</span>。 <span class="math inline">\(Z_t = \sum_{i=1}^{m} w_i^{(t)} \exp(-\alpha_t y_i h_t(\mathbf{x}_i))\)</span> 这个更新规则的含义是：
<ul>
<li>如果样本 <span class="math inline">\(i\)</span> 被 <span class="math inline">\(h_t\)</span> 正确分类 (<span class="math inline">\(y_i h_t(\mathbf{x}_i) = 1\)</span>)，则其权重 <span class="math inline">\(w_i\)</span> 会乘以 <span class="math inline">\(\exp(-\alpha_t)\)</span>，即权重减小。</li>
<li>如果样本 <span class="math inline">\(i\)</span> 被 <span class="math inline">\(h_t\)</span> 错误分类 (<span class="math inline">\(y_i h_t(\mathbf{x}_i) = -1\)</span>)，则其权重 <span class="math inline">\(w_i\)</span> 会乘以 <span class="math inline">\(\exp(\alpha_t)\)</span>，即权重增加。 权重调整的幅度由 <span class="math inline">\(\alpha_t\)</span> (即分类器的性能) 决定。</li>
</ul></li>
</ol></li>
<li><p><strong>组合强分类器：</strong> 最终的强分类器 <span class="math inline">\(H(\mathbf{x})\)</span> 是所有 <span class="math inline">\(T\)</span> 个弱分类器的加权组合： <span class="math display">\[ H(\mathbf{x}) = \text{sign} \left( \sum_{t=1}^{T} \alpha_t h_t(\mathbf{x}) \right) \]</span> 其中 <span class="math inline">\(\text{sign}(\cdot)\)</span> 是符号函数。</p></li>
</ol>
<p><strong>AdaBoost的解释：</strong> AdaBoost可以被看作是一种前向分步加法模型 (Forward Stagewise Additive Modeling)，它试图优化指数损失函数 <span class="math inline">\(L(y, f(\mathbf{x})) = \exp(-y f(\mathbf{x}))\)</span>。每一步都贪心地选择一个使得损失函数下降最大的弱分类器及其权重。</p>
<p><strong>AdaBoost的基学习器：</strong> 理论上可以是任何分类器，但最常用的是<strong>决策树桩 (decision stumps)</strong>，即只有一层分裂的决策树。因为AdaBoost算法本身会赋予分类器权重并调整样本权重，所以基学习器不需要太复杂。</p>
<p><strong>在Scikit-learn中使用 <code>AdaBoostClassifier</code>：</strong></p>
<p>Scikit-learn提供了 <code>AdaBoostClassifier</code> 和 <code>AdaBoostRegressor</code>。</p>
<div id="56e40728" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> AdaBoostClassifier</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier <span class="co"># AdaBoost通常使用决策树作为基学习器</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 生成一个二分类数据集</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>X_ada, y_ada <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">20</span>, n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>                                   n_redundant<span class="op">=</span><span class="dv">0</span>, random_state<span class="op">=</span><span class="dv">42</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>X_train_ada, X_test_ada, y_train_ada, y_test_ada <span class="op">=</span> train_test_split(X_ada, y_ada, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建AdaBoost分类器</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co"># base_estimator: 基学习器，默认为DecisionTreeClassifier(max_depth=1) 即决策树桩</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="co"># n_estimators: 弱学习器的最大数量（迭代次数），默认为50</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co"># learning_rate: 学习率，用于缩减每个弱学习器的贡献，取值在 (0, 1] 之间，默认为1.0。</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co">#                较小的学习率通常需要更多的n_estimators。</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="co"># algorithm: {'SAMME', 'SAMME.R'}, 默认='SAMME.R'。</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="co">#            SAMME.R 使用类别概率进行更新，通常收敛更快，性能也更好。</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="co">#            SAMME 使用类别标签进行更新。</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>ada_clf <span class="op">=</span> AdaBoostClassifier(</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span>), <span class="co"># 显式指定基学习器和其参数</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    algorithm<span class="op">=</span><span class="st">"SAMME"</span>, <span class="co"># SAMME.R 通常更好 -&gt; 更改为 SAMME 以修复错误</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.5</span>,   <span class="co"># 学习率</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>ada_clf.fit(X_train_ada, y_train_ada)</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>y_pred_ada <span class="op">=</span> ada_clf.predict(X_test_ada)</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">AdaBoostClassifier 准确率: </span><span class="sc">{</span>accuracy_score(y_test_ada, y_pred_ada)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a><span class="co"># 查看基学习器的错误率和权重 (如果需要，但这通常在内部处理)</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a><span class="co"># for i, base_estimator in enumerate(ada_clf.estimators_):</span></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a><span class="co">#     print(f"Estimator {i}: error = {ada_clf.estimator_errors_[i]:.4f}, weight = {ada_clf.estimator_weights_[i]:.4f}")</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/wangxq/.pyenv/versions/3.9.13/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning:

The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
AdaBoostClassifier 准确率: 0.8880</code></pre>
</div>
</div>
<p><strong>AdaBoost的优缺点：</strong></p>
<ul>
<li><strong>优点：</strong>
<ul>
<li>实现简单，分类精度较高。</li>
<li>可以使用各种基学习器。</li>
<li>不容易发生过拟合（相对于某些其他算法，但仍需注意迭代次数）。</li>
</ul></li>
<li><strong>缺点：</strong>
<ul>
<li>对异常值敏感，因为异常值在迭代中会获得较高的权重。</li>
<li>训练时间可能较长，因为是串行训练。</li>
</ul></li>
</ul>
<p>通常使用 <code>GridSearchCV</code> 或 <code>RandomizedSearchCV</code> 结合交叉验证来调整这些参数。</p>
</section>
<section id="梯度提升树-gradient-boosting-trees-gbtgbm" class="level3">
<h3 class="anchored" data-anchor-id="梯度提升树-gradient-boosting-trees-gbtgbm">6.7.2 梯度提升树 (Gradient Boosting Trees, GBT/GBM)</h3>
<p>梯度提升树 (Gradient Boosting Trees, GBT)，通常也称为梯度提升机 (Gradient Boosting Machines, GBM)，是另一种强大且广泛使用的Boosting集成算法，由Jerome Friedman在2001年提出。与AdaBoost通过调整样本权重来关注错误不同，梯度提升通过<strong>拟合先前学习器预测结果的残差（residuals）</strong>或者更一般地，<strong>损失函数的负梯度（negative gradient）</strong>来构建新的基学习器。</p>
<p><strong>梯度提升的工作原理：</strong></p>
<p>梯度提升将学习过程视为一个在函数空间中的优化问题。它试图找到一个函数（由多个基学习器，通常是决策树，相加而成）来最小化指定的损失函数。</p>
<ol type="1">
<li><p><strong>初始化模型：</strong> 首先，用一个简单的模型来初始化整体预测 <span class="math inline">\(F_0(\mathbf{x})\)</span>，通常是训练集目标值的均值（对于回归）或对数几率（对于分类）。 对于回归任务，如果损失函数是均方误差 (MSE)，初始模型 <span class="math inline">\(F_0(\mathbf{x}) = \bar{y}\)</span>。 对于分类任务，通常更复杂一些。</p></li>
<li><p><strong>迭代训练基学习器 (共 <span class="math inline">\(M\)</span> 轮)：</strong> 对于 <span class="math inline">\(m = 1, ..., M\)</span>：</p>
<ol type="a">
<li><p><strong>计算伪残差 (Pseudo-residuals)：</strong> 对于每个样本 <span class="math inline">\(i\)</span>，计算当前集成模型 <span class="math inline">\(F_{m-1}(\mathbf{x}_i)\)</span> 关于真实值 <span class="math inline">\(y_i\)</span> 的损失函数的负梯度。这个负梯度被称为”伪残差”，因为它指明了当前模型需要在哪个方向上改进以减少损失。 <span class="math display">\[ r_{im} = - \left[ \frac{\partial L(y_i, F(\mathbf{x}_i))}{\partial F(\mathbf{x}_i)} \right]_{F(\mathbf{x})=F_{m-1}(\mathbf{x})} \]</span></p>
<ul>
<li>如果损失函数是均方误差 <span class="math inline">\(L(y, F) = \frac{1}{2}(y - F)^2\)</span>，则负梯度就是普通残差 <span class="math inline">\(r_{im} = y_i - F_{m-1}(\mathbf{x}_i)\)</span>。</li>
<li>对于其他损失函数（如分类中的对数损失/deviance），伪残差的形式会不同。</li>
</ul></li>
<li><p><strong>训练新的基学习器 <span class="math inline">\(h_m(\mathbf{x})\)</span>：</strong> 使用伪残差 <span class="math inline">\(r_{im}\)</span> 作为目标，训练一个新的基学习器 <span class="math inline">\(h_m(\mathbf{x})\)</span> 来拟合这些伪残差。通常基学习器是决策树（CART）。 <span class="math display">\[ h_m(\mathbf{x}) \approx r_m \]</span></p></li>
<li><p><strong>确定最佳步长 (对于回归树，通常是叶节点的最优值)：</strong> 对于树的每个叶节点区域 <span class="math inline">\(R_{jm}\)</span>，计算一个最优的输出值 <span class="math inline">\(\gamma_{jm}\)</span>，使得将这个值加到当前集成预测上时，能最大程度地减少该叶节点内样本的损失。 <span class="math display">\[ \gamma_{jm} = \text{argmin}_{\gamma} \sum_{\mathbf{x}_i \in R_{jm}} L(y_i, F_{m-1}(\mathbf{x}_i) + \gamma) \]</span> 然后，新的树 <span class="math inline">\(h_m(\mathbf{x})\)</span> 的预测值就是这些 <span class="math inline">\(\gamma_{jm}\)</span>。</p></li>
<li><p><strong>更新集成模型 <span class="math inline">\(F_m(\mathbf{x})\)</span>：</strong> 将新训练的基学习器 <span class="math inline">\(h_m(\mathbf{x})\)</span>（其预测已是最优步长）以一定的<strong>学习率 (learning rate)</strong> <span class="math inline">\(\nu\)</span> (也称为shrinkage) 加入到集成模型中。 <span class="math display">\[ F_m(\mathbf{x}) = F_{m-1}(\mathbf{x}) + \nu \cdot h_m(\mathbf{x}) \]</span> 学习率 <span class="math inline">\(\nu\)</span> (通常是一个小的正数，如0.01到0.1) 用于缩减每一步的贡献，有助于防止过拟合，并允许更多的基学习器参与到集成中，从而可能找到更好的解。</p></li>
</ol></li>
<li><p><strong>最终预测：</strong> 最终的预测模型 <span class="math inline">\(F_M(\mathbf{x})\)</span> 是所有基学习器贡献的总和。对于分类问题，通常将 <span class="math inline">\(F_M(\mathbf{x})\)</span> 转换为概率（例如通过Sigmoid函数）。</p></li>
</ol>
<p><strong>关键概念：</strong></p>
<ul>
<li><strong>损失函数 (Loss Function)：</strong> GBT的灵活性在于可以选择不同的损失函数以适应不同的任务。
<ul>
<li>回归：常用均方误差 (MSE，<code>loss='ls'</code> 或 <code>loss='squared_error'</code> in Scikit-learn), 绝对误差 (MAE, <code>loss='lad'</code>), Huber损失 (<code>loss='huber'</code>)。</li>
<li>分类：常用对数损失 (Log Loss / Deviance，<code>loss='deviance'</code> for binary, or for multiclass <code>loss='log_loss'</code> in newer sklearn) 或指数损失 (<code>loss='exponential'</code>, 类似于AdaBoost)。</li>
</ul></li>
<li><strong>学习率 (Learning Rate / Shrinkage, <span class="math inline">\(\nu\)</span>)：</strong> 控制每个基学习器对总模型的贡献程度。较小的学习率通常需要更多的基学习器 (<code>n_estimators</code>) 才能达到相同的训练误差，但能提高模型的泛化能力。学习率和基学习器数量之间存在权衡。</li>
<li><strong>子采样 (Subsampling / Stochastic Gradient Boosting)：</strong> 类似于Bagging中的思想，在每轮训练基学习器时，不是使用全部训练样本，而是随机抽取一部分样本（例如，50%-80%）来训练。这引入了随机性，有助于减少方差，防止过拟合，并加快训练速度。通过 <code>subsample</code> 参数控制。</li>
<li><strong>特征子采样 (Feature Subsampling)：</strong> 类似于随机森林，在构建每棵树的每个分裂点时，只考虑一部分随机选择的特征。通过 <code>max_features</code> 参数控制。</li>
</ul>
<p><strong>在Scikit-learn中使用 <code>GradientBoostingClassifier</code> / <code>GradientBoostingRegressor</code>：</strong></p>
<div id="a98cfed1" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingClassifier</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 生成分类数据</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>X_gbt, y_gbt <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">20</span>, n_informative<span class="op">=</span><span class="dv">5</span>, </span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>                                   n_redundant<span class="op">=</span><span class="dv">0</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>X_train_gbt, X_test_gbt, y_train_gbt, y_test_gbt <span class="op">=</span> train_test_split(X_gbt, y_gbt, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建梯度提升分类器</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co"># n_estimators: 基学习器的数量 (树的数量)</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co"># learning_rate: 学习率</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co"># max_depth: 每棵决策树的最大深度</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="co"># subsample: 用于拟合单个基学习器的样本的比例 (随机梯度提升)</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="co"># loss: 要优化的损失函数 ('deviance' for classification, 'ls' for regression)</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>gbt_clf <span class="op">=</span> GradientBoostingClassifier(</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">100</span>,      <span class="co"># 100棵树</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,     <span class="co"># 学习率</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">3</span>,           <span class="co"># 每棵树最大深度为3</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    subsample<span class="op">=</span><span class="fl">0.8</span>,         <span class="co"># 每棵树使用80%的样本进行训练</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>gbt_clf.fit(X_train_gbt, y_train_gbt)</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>y_pred_gbt <span class="op">=</span> gbt_clf.predict(X_test_gbt)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">GradientBoostingClassifier 准确率: </span><span class="sc">{</span>accuracy_score(y_test_gbt, y_pred_gbt)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 特征重要性</span></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a><span class="co"># importances_gbt = gbt_clf.feature_importances_</span></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="co"># print("GBT Feature importances:", importances_gbt)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
GradientBoostingClassifier 准确率: 0.9080</code></pre>
</div>
</div>
<p><strong>梯度提升树的优缺点：</strong></p>
<ul>
<li><strong>优点：</strong>
<ul>
<li><strong>高预测精度：</strong> 通常能够达到非常高的预测性能，尤其是在结构化数据上。</li>
<li><strong>灵活性强：</strong> 可以使用多种损失函数，处理不同类型的数据（数值、类别特征需要预处理）。</li>
<li><strong>鲁棒性较好：</strong> 对异常值的鲁棒性强于AdaBoost（尤其是在使用如Huber损失时）。</li>
<li><strong>可解释性：</strong> 虽然是集成模型，但仍可以得到特征重要性。</li>
</ul></li>
<li><strong>缺点：</strong>
<ul>
<li><strong>训练时间较长：</strong> 由于是串行训练，计算成本较高，尤其是在大数据集上。</li>
<li><strong>调参相对复杂：</strong> 有多个重要参数需要仔细调整（如<code>n_estimators</code>, <code>learning_rate</code>, <code>max_depth</code>, <code>subsample</code>），它们之间有交互作用。</li>
<li><strong>容易过拟合：</strong> 如果参数设置不当（如学习率过大，树的数量过多而没有早停），模型也可能过拟合。早停 (Early Stopping) 是一个常用的防止过拟合的技巧，即在验证集上的性能不再提升时停止训练。</li>
</ul></li>
</ul>
<p>通常使用 <code>GridSearchCV</code> 或 <code>RandomizedSearchCV</code> 结合交叉验证来调整这些参数。</p>
</section>
<section id="xgboost-extreme-gradient-boosting-简介" class="level3">
<h3 class="anchored" data-anchor-id="xgboost-extreme-gradient-boosting-简介">6.7.3 XGBoost (Extreme Gradient Boosting) 简介</h3>
<p>XGBoost (Extreme Gradient Boosting) 是由陈天奇博士等人 (2016) 开发的一个开源机器学习项目，它高效地实现了梯度提升算法并进行了多方面的优化和扩展，被广泛应用于学术界和工业界，并在许多机器学习竞赛中取得了巨大成功。XGBoost 不仅性能卓越，而且具有高度的灵活性和可移植性。</p>
<p><strong>XGBoost 的主要特点和优势：</strong></p>
<ul>
<li><strong>正则化 (Regularization)：</strong> XGBoost 在目标函数中加入了L1 (Lasso Regression) 和 L2 (Ridge Regression) 正则化项，有助于控制模型的复杂度，防止过拟合。这是相对于传统GBM的一个重要改进。 <span class="math display">\[ \text{Obj}^{(t)} = \sum_{i=1}^{n} l(y_i, \hat{y}_i^{(t-1)} + f_t(\mathbf{x}_i)) + \Omega(f_t) + \text{constant} \]</span> 其中 <span class="math inline">\(\Omega(f_t) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T} w_j^2\)</span>，<span class="math inline">\(T\)</span> 是叶子节点的数量，<span class="math inline">\(w_j\)</span> 是叶子节点 <span class="math inline">\(j\)</span> 的分数（权重），<span class="math inline">\(\gamma\)</span> 和 <span class="math inline">\(\lambda\)</span> 是正则化参数。</li>
<li><strong>高度优化的树构建算法：</strong>
<ul>
<li><strong>近似贪心算法：</strong> 对于大规模数据集，XGBoost 使用一种近似算法来寻找最佳分裂点，提高了效率。</li>
<li><strong>稀疏感知 (Sparsity-aware) 分裂查找：</strong> 能够自动处理缺失值和零值，无需预先进行填充。</li>
<li><strong>并行化与缓存优化：</strong> 能够在特征级别进行并行计算，并利用缓存优化数据访问，大大加快了训练速度。</li>
</ul></li>
<li><strong>内置交叉验证 (Cross-Validation)：</strong> XGBoost 可以在训练过程中直接进行交叉验证，方便地获取最优的迭代次数。</li>
<li><strong>树剪枝 (Tree Pruning)：</strong> 与传统GBM在分裂前判断（预剪枝）不同，XGBoost 通常先生长一棵树到指定的最大深度，然后自底向上进行剪枝（基于 <code>gamma</code> 参数）。</li>
<li><strong>灵活性：</strong> 用户可以自定义优化目标和评估标准。</li>
<li><strong>可移植性：</strong> 支持多种编程语言（Python, R, Java, Scala, C++等）和计算环境（包括分布式环境如Hadoop, Spark）。</li>
</ul>
<p><strong>在Python中使用XGBoost：</strong></p>
<p>XGBoost 有其自己独立的Python包 (<code>xgboost</code>)，但它也提供了与Scikit-learn兼容的API (<code>XGBClassifier</code>, <code>XGBRegressor</code>)，使得它可以方便地集成到Scikit-learn的工作流中。</p>
<p>首先，你需要安装 <code>xgboost</code> 包：</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install xgboost</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>或者</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> install <span class="at">-c</span> conda-forge xgboost</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong><code>XGBClassifier</code> 示例：</strong></p>
<div id="bc98a99e" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> xgboost <span class="im">as</span> xgb</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris <span class="co"># 示例数据</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 加载数据</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>iris_xgb <span class="op">=</span> load_iris() <span class="co"># 使用新变量名避免与之前章节冲突</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>X_xgb, y_xgb <span class="op">=</span> iris_xgb.data, iris_xgb.target</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb <span class="op">=</span> train_test_split(X_xgb, y_xgb, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>, stratify<span class="op">=</span>y_xgb)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建 XGBoost 分类器实例</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 常用参数：</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co"># n_estimators: 树的数量 (迭代次数)</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co"># learning_rate: 学习率 (eta)</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="co"># max_depth: 每棵树的最大深度</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="co"># subsample: 训练每棵树时样本的采样比例</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="co"># colsample_bytree: 构建每棵树时特征的采样比例</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a><span class="co"># gamma: 节点分裂所需的最小损失降低。较大的gamma值会导致更保守的算法。</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="co"># reg_alpha: L1 正则化项的权重</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="co"># reg_lambda: L2 正则化项的权重</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="co"># objective: 学习任务及相应的学习目标，例如 'binary:logistic' (二分类), 'multi:softmax' (多分类)</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a><span class="co"># eval_metric: 验证数据的评估指标</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a><span class="co"># use_label_encoder=False: 推荐在新版XGBoost中设置，避免警告 (在新版XGBoost中此参数可能已弃用或行为改变，若报错可移除或改为True并处理相应警告)</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>xgb_clf_example <span class="op">=</span> xgb.XGBClassifier(</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>    subsample<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>    colsample_bytree<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>    objective<span class="op">=</span><span class="st">'multi:softmax'</span>, <span class="co"># 鸢尾花是多分类问题</span></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>    num_class<span class="op">=</span><span class="bu">len</span>(iris_xgb.target_names), <span class="co"># 类别数量</span></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>    use_label_encoder<span class="op">=</span><span class="va">False</span>, <span class="co"># 针对旧版本XGBoost的建议，新版本可能不需要</span></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>    eval_metric<span class="op">=</span><span class="st">'mlogloss'</span>     <span class="co"># 多分类对数损失</span></span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练模型 (可以加入早停)</span></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a><span class="co"># xgb_clf_example.fit(X_train_xgb, y_train_xgb,</span></span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a><span class="co">#             eval_set=[(X_test_xgb, y_test_xgb)],</span></span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a><span class="co">#             early_stopping_rounds=10, # 如果10轮内验证集性能没有提升则停止</span></span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a><span class="co">#             verbose=False) # verbose=True 会打印评估结果</span></span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>xgb_clf_example.fit(X_train_xgb, y_train_xgb) <span class="co"># 简单训练</span></span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a><span class="co"># 预测</span></span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>y_pred_xgb_example <span class="op">=</span> xgb_clf_example.predict(X_test_xgb)</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>accuracy_xgb_example <span class="op">=</span> accuracy_score(y_test_xgb, y_pred_xgb_example)</span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"XGBoost Classifier 准确率 (鸢尾花示例): </span><span class="sc">{</span>accuracy_xgb_example<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>XGBoost Classifier 准确率 (鸢尾花示例): 0.9000</code></pre>
</div>
</div>
<p>XGBoost 提供了丰富的参数用于精细控制模型的行为，其调参过程也相对复杂，但通常能带来显著的性能提升。</p>
<p>这些高级的Boosting库通常作为独立的Python包提供，并与Scikit-learn API兼容。在实际项目中，如果对性能有较高要求，它们往往是比标准<code>GradientBoostingClassifier/Regressor</code>更好的选择。</p>
</section>
</section>
<section id="本章总结" class="level2">
<h2 class="anchored" data-anchor-id="本章总结">6.8 本章总结</h2>
<p>本章深入探讨了监督学习中的两种重要模型类别：决策树和集成学习。</p>
<p><strong>决策树</strong>：</p>
<ul>
<li>是一种直观的、基于规则的分类与回归方法，其结构类似流程图。</li>
<li>构建过程涉及递归地选择最佳分裂特征和分裂点，常用标准有信息增益（基于信息熵）和基尼不纯度。</li>
<li>容易过拟合，需要通过剪枝（预剪枝或后剪枝，如CCP）来提升泛化能力。</li>
<li>Scikit-learn提供了 <code>DecisionTreeClassifier</code> 和 <code>DecisionTreeRegressor</code>。</li>
</ul>
<p><strong>集成学习</strong>：</p>
<ul>
<li>核心思想是结合多个学习器（基学习器）的预测以获得比单个学习器更好的性能。</li>
<li>关键在于基学习器的多样性和有效的结合策略。</li>
<li>主要分为两大类：
<ul>
<li><strong>Bagging (Bootstrap Aggregating)</strong>：
<ul>
<li>通过自助采样创建多样化的训练子集，并行训练基学习器。</li>
<li>主要目标是降低方差，对高方差模型（如决策树）效果显著。</li>
<li><strong>随机森林 (Random Forest)</strong> 是Bagging的成功应用，它使用决策树作为基学习器，并在树的构建中引入特征随机性，进一步增强多样性。</li>
<li>随机森林能够提供特征重要性评估，并具有良好的鲁棒性和高准确率。</li>
<li>Scikit-learn提供了 <code>BaggingClassifier</code>/<code>Regressor</code> 和 <code>RandomForestClassifier</code>/<code>Regressor</code>。</li>
</ul></li>
<li><strong>Boosting</strong>：
<ul>
<li>串行训练基学习器，每个新的学习器都侧重于修正先前学习器的错误。</li>
<li>主要目标是降低偏差。</li>
<li><strong>AdaBoost (Adaptive Boosting)</strong> 通过调整样本权重和学习器权重，迭代地训练弱学习器。</li>
<li><strong>梯度提升树 (Gradient Boosting Trees, GBT/GBM)</strong> 将学习过程视为在函数空间中优化损失函数，每轮迭代拟合损失函数的负梯度（伪残差）。</li>
<li><strong>XGBoost (Extreme Gradient Boosting)</strong> 作为梯度提升的高效实现，通过正则化、优化的树构建算法（如稀疏感知、并行处理）和内置功能（如交叉验证、剪枝）等特性，在性能和效率上都有显著提升。</li>
<li>XGBoost, LightGBM, CatBoost 是对梯度提升算法的高效实现和扩展，在实践中表现优异。</li>
<li>Scikit-learn提供了 <code>AdaBoostClassifier</code>/<code>Regressor</code> 和 <code>GradientBoostingClassifier</code>/<code>Regressor</code>。</li>
</ul></li>
</ul></li>
</ul>
<p>通过本章的学习，我们掌握了构建、理解和应用决策树及各种强大集成学习模型的基本原理和实践方法，为解决复杂的分类和回归问题提供了有力的工具。</p>
</section>
<section id="思考与练习" class="level2">
<h2 class="anchored" data-anchor-id="思考与练习">6.9 思考与练习</h2>
<section id="基础练习" class="level3">
<h3 class="anchored" data-anchor-id="基础练习">6.9.1 基础练习</h3>
<ol type="1">
<li><strong>理解不同分裂标准：</strong>
<ul>
<li>信息熵和基尼不纯度在计算上有什么主要区别？它们各自的取值范围是什么？</li>
<li>在什么情况下信息增益会倾向于选择取值较多的特征？信息增益率是如何缓解这个问题的？</li>
<li>对于回归树，为什么通常使用MSE作为分裂标准而不是分类树中的纯度指标？</li>
</ul></li>
<li><strong>剪枝的必要性：</strong>
<ul>
<li>为什么决策树容易过拟合？预剪枝和后剪枝分别是如何尝试解决这个问题的？</li>
<li>在Scikit-learn中，<code>max_depth</code>, <code>min_samples_split</code>, <code>min_samples_leaf</code> 这些参数是如何帮助预剪枝的？</li>
<li>代价复杂度剪枝 (CCP) 的基本思想是什么？<code>ccp_alpha</code>参数起什么作用？</li>
</ul></li>
<li><strong>Bagging 与 Boosting 的对比：</strong>
<ul>
<li>Bagging和Boosting在训练基学习器的方式上有什么根本区别（并行 vs.&nbsp;串行）？</li>
<li>它们各自主要致力于解决什么问题（降低方差 vs.&nbsp;降低偏差）？</li>
<li>随机森林与标准的Bagging（以决策树为基学习器）相比，引入了什么额外的随机性？这带来了什么好处？</li>
<li>AdaBoost和梯度提升树在迭代过程中是如何学习和修正错误的？它们关注的”错误”有何不同？</li>
</ul></li>
<li><strong>参数敏感性：</strong>
<ul>
<li>对于随机森林，<code>n_estimators</code> 和 <code>max_features</code> 是两个重要的参数。讨论它们如何影响模型的性能和训练时间。</li>
<li>对于梯度提升树，<code>n_estimators</code>, <code>learning_rate</code>, 和 <code>max_depth</code> 是关键参数。解释它们之间的相互作用，以及为什么通常需要一起调整它们。</li>
<li>什么是早停策略？它在梯度提升中为什么重要？</li>
</ul></li>
</ol>
</section>
<section id="编码与实践" class="level3">
<h3 class="anchored" data-anchor-id="编码与实践">6.9.2 编码与实践</h3>
<ol type="1">
<li><strong>数据集探索与预处理：</strong> 选择一个你感兴趣的分类或回归数据集（例如，UCI机器学习仓库中的数据集，或Kaggle上的入门级竞赛数据集）。进行初步的数据探索和必要的预处理（处理缺失值、编码类别特征等）。</li>
<li><strong>决策树调优：</strong> 在你选择的数据集上训练一个决策树模型。尝试调整不同的预剪枝参数（如<code>max_depth</code>, <code>min_samples_leaf</code>）以及<code>ccp_alpha</code> (如果适用)，使用交叉验证来评估不同参数组合的效果，并可视化最优的决策树结构。</li>
<li><strong>集成模型比较：</strong>
<ul>
<li>实现随机森林分类器/回归器。调整<code>n_estimators</code>和<code>max_features</code>，观察OOB错误率的变化。使用<code>feature_importances_</code>和<code>permutation_importance</code>来分析特征的重要性。</li>
<li>实现AdaBoost分类器/回归器。尝试不同的基学习器（例如，不同深度的决策树）和调整<code>n_estimators</code>与<code>learning_rate</code>。</li>
<li>实现梯度提升分类器/回归器。重点调整<code>n_estimators</code>, <code>learning_rate</code>, <code>max_depth</code>和<code>subsample</code>。如果可能，尝试实现早停策略。</li>
<li>比较这三种集成学习模型（随机森林、AdaBoost、梯度提升）以及单个优化后的决策树在你的数据集上的性能（使用合适的评估指标）。讨论它们的优缺点和适用场景。</li>
</ul></li>
<li><strong>尝试XGBoost：</strong> 安装并使用XGBoost库（<code>xgboost</code>）在你选择的数据集上进行建模。调整其关键参数（如<code>n_estimators</code>, <code>learning_rate</code>, <code>max_depth</code>, <code>gamma</code>, <code>subsample</code>, <code>colsample_bytree</code>等），并尝试使用其内置的交叉验证和早停功能。将其性能与Scikit-learn中的<code>GradientBoostingClassifier</code>/<code>Regressor</code>以及之前尝试的其他模型进行比较。如果时间和兴趣允许，也可以进一步探索LightGBM或CatBoost。</li>
</ol>
<p>通过理论思考和动手实践，你将能更深刻地理解决策树和集成学习的强大之处以及它们在实际问题中的应用方式。</p>
</section>
<section id="推荐阅读" class="level3">
<h3 class="anchored" data-anchor-id="推荐阅读">6.9.3 推荐阅读</h3>
<ol type="1">
<li><strong>《统计学习方法》李航著，第5章（决策树）和第8章（提升方法）。</strong> 这本书对决策树、AdaBoost和提升树有非常清晰和深入的数学推导。</li>
<li><strong>《Elements of Statistical Learning》 (ESL) by Hastie, Tibshirani, and Friedman, Chapters 9 (Trees) and 10 (Boosting and Additive Trees), 15 (Random Forests).</strong> 这是机器学习领域的经典教材，对相关算法有非常详尽的阐述。</li>
<li><strong>Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5-32.</strong> 随机森林的原始论文。</li>
<li><strong>Freund, Y., &amp; Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1), 119-139.</strong> AdaBoost的经典论文之一。</li>
<li><strong>Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine. Annals of statistics, 1189-1232.</strong> 梯度提升机的原始论文。</li>
<li><strong>Chen, T., &amp; Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining (pp.&nbsp;785-794).</strong> XGBoost的原始论文。</li>
<li><strong>Scikit-learn官方文档：</strong>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/tree.html">Decision Trees</a></li>
<li><a href="https://scikit-learn.org/stable/modules/ensemble.html">Ensemble methods</a> (包含Bagging, RandomForests, AdaBoost, GradientBoosting等)</li>
</ul></li>
<li><strong>XGBoost官方文档：</strong> <a href="https://xgboost.readthedocs.io/">https://xgboost.readthedocs.io/</a></li>
<li><strong>LightGBM官方文档：</strong> <a href="https://lightgbm.readthedocs.io/">https://lightgbm.readthedocs.io/</a></li>
<li><strong>CatBoost官方文档：</strong> <a href="https://catboost.ai/docs/">https://catboost.ai/docs/</a></li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./05-svm.html" class="pagination-link" aria-label="支持向量机 (SVM)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">支持向量机 (SVM)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./07-clustering.html" class="pagination-link" aria-label="聚类分析">
        <span class="nav-page-text"><span class="chapter-title">聚类分析</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>